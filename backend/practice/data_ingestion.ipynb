{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d71aef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple, Any\n",
    "from datetime import datetime, timedelta\n",
    "from dataclasses import dataclass, asdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "327de6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class M5DatasetConfig:\n",
    "    \"\"\"Configuration specific to M5 Walmart dataset.\"\"\"\n",
    "    # Data paths\n",
    "    sales_train_path: str = \"data/raw/sales_train_evaluation.csv\"\n",
    "    prices_path: str = \"data/raw/sell_prices.csv\"\n",
    "    calendar_path: str = \"data/raw/calendar.csv\"\n",
    "    \n",
    "    # Schema definitions\n",
    "    sales_schema: Dict = None\n",
    "    prices_schema: Dict = None\n",
    "    calendar_schema: Dict = None\n",
    "    \n",
    "    # Processing parameters\n",
    "    chunk_size: int = 10000\n",
    "    max_memory_gb: float = 8.0\n",
    "    \n",
    "    # Validation thresholds\n",
    "    min_sales_per_item: int = 100  \n",
    "    max_zero_days_pct: float = 0.95  \n",
    "    \n",
    "    # Output configuration\n",
    "    save_interim: bool = True\n",
    "    save_features: bool = True\n",
    "    compression: str = \"snappy\"\n",
    "    load_fraction: float =0.01\n",
    "    max_rows: int = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "from contextlib import contextmanager\n",
    "import time\n",
    "import gc\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class MemoryManager:\n",
    "    \"\"\"memory management for large dataset processing.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_memory_gb: float = 8.0):\n",
    "        self.max_memory_gb = max_memory_gb\n",
    "        self.max_memory_bytes = max_memory_gb * 1024**3\n",
    "        \n",
    "    def get_memory_usage(self) -> Dict[str, float]:\n",
    "        \"\"\"Get current memory usage statistics.\"\"\"\n",
    "        process = psutil.Process()\n",
    "        memory_info = process.memory_info()\n",
    "        \n",
    "        return {\n",
    "            'rss_gb': memory_info.rss / 1024**3,\n",
    "            'vms_gb': memory_info.vms / 1024**3,\n",
    "            'percent': process.memory_percent(),\n",
    "            'available_gb': psutil.virtual_memory().available / 1024**3\n",
    "        }\n",
    "    \n",
    "    def check_memory_usage(self):\n",
    "        \"\"\"Check if memory usage is within limits.\"\"\"\n",
    "        memory_stats = self.get_memory_usage()\n",
    "        \n",
    "        if memory_stats['rss_gb'] > self.max_memory_gb:\n",
    "            logger.warning(f\"Memory usage ({memory_stats['rss_gb']:.2f} GB) exceeds limit ({self.max_memory_gb} GB)\")\n",
    "            gc.collect()  # Force garbage collection\n",
    "            \n",
    "        logger.debug(f\"Memory usage: {memory_stats['rss_gb']:.2f} GB ({memory_stats['percent']:.1f}%)\")\n",
    "    \n",
    "    @contextmanager\n",
    "    def memory_monitor(self, operation_name: str):\n",
    "        \"\"\"Context manager to monitor memory usage during operations.\"\"\"\n",
    "        start_memory = self.get_memory_usage()\n",
    "        start_time = time.time()\n",
    "        \n",
    "        logger.info(f\"Starting {operation_name} - Memory: {start_memory['rss_gb']:.2f} GB\")\n",
    "        \n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            end_memory = self.get_memory_usage()\n",
    "            duration = time.time() - start_time\n",
    "            memory_delta = end_memory['rss_gb'] - start_memory['rss_gb']\n",
    "            \n",
    "            logger.info(f\"Completed {operation_name} in {duration:.2f}s - \"\n",
    "                       f\"Memory: {end_memory['rss_gb']:.2f} GB ({memory_delta:+.2f} GB)\")\n",
    "            \n",
    "            self.check_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3535bb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataQualityMetrics:\n",
    "    \"\"\"Comprehensive data quality metrics for M5 dataset.\"\"\"\n",
    "    total_time_series: int\n",
    "    valid_time_series: int\n",
    "    total_observations: int\n",
    "    missing_sales_pct: float\n",
    "    zero_sales_pct: float\n",
    "    negative_sales_count: int\n",
    "    price_coverage_pct: float\n",
    "    calendar_coverage_pct: float\n",
    "    data_completeness_score: float\n",
    "    temporal_consistency_score: float\n",
    "    hierarchical_consistency_score: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "130f193b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downcast(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if pd.api.types.is_integer_dtype(col_type):\n",
    "            df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "\n",
    "        elif pd.api.types.is_float_dtype(col_type):\n",
    "            df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "\n",
    "        elif pd.api.types.is_object_dtype(col_type):\n",
    "            if col == 'date':\n",
    "                df[col] = pd.to_datetime(df[col], format='%Y-%m-%d', errors='coerce')\n",
    "            else:\n",
    "                # Check if column is numeric-like\n",
    "                converted = pd.to_numeric(df[col], errors='coerce')\n",
    "                num_missing = converted.isna().sum()\n",
    "                total = len(df[col])\n",
    "\n",
    "                if num_missing / total < 0.05:  # less than 5% non-convertible treated as numeric\n",
    "                    df[col] = converted\n",
    "                else:\n",
    "                    df[col] = df[col].astype('category')\n",
    "                    print(f\"Column '{col}' converted to category\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def downcast_with_stats(df: pd.DataFrame, name: str = \"DataFrame\") -> pd.DataFrame:\n",
    "    start_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    df = downcast(df)\n",
    "    end_mem = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f\"ðŸ”§ {name} memory usage: {start_mem:.2f} MB â†’ {end_mem:.2f} MB ({100 * (start_mem - end_mem) / start_mem:.1f}% reduction)\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13abd445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super().default(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cf91c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import duckdb\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from contextlib import contextmanager\n",
    "import gc\n",
    "import time\n",
    "from functools import wraps\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class M5DatasetProcessor:\n",
    "    \"\"\"Processor for M5 Walmart dataset.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: M5DatasetConfig):\n",
    "        self.config = config\n",
    "        self.memory_manager = MemoryManager(config.max_memory_gb)\n",
    "        self.quality_metrics = None\n",
    "        \n",
    "        # Setup output directories\n",
    "        self.setup_directories()\n",
    "        \n",
    "        logger.info(\"M5DatasetProcessor initialized for production processing\")\n",
    "    \n",
    "    def setup_directories(self):\n",
    "        \"\"\"Create required directory structure.\"\"\"\n",
    "        dirs = [\n",
    "            \"data/interim/m5\",\n",
    "            \"data/processed/m5\", \n",
    "            \"data/features/m5\",\n",
    "            \"data/quality/m5\",\n",
    "            \"data/monitoring/m5\"\n",
    "        ]\n",
    "        \n",
    "        for dir_path in dirs:\n",
    "            Path(dir_path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def validate_file_integrity(self) -> bool:\n",
    "        \"\"\"Validate that all required M5 files exist and are readable.\"\"\"\n",
    "        logger.info(\"Validating M5 dataset file integrity...\")\n",
    "        \n",
    "        required_files = [\n",
    "            self.config.sales_train_path,\n",
    "            self.config.prices_path,\n",
    "            self.config.calendar_path\n",
    "        ]\n",
    "        \n",
    "        for file_path in required_files:\n",
    "            path = Path(file_path)\n",
    "            if not path.exists():\n",
    "                logger.error(f\"Required file missing: {file_path}\")\n",
    "                return False\n",
    "            \n",
    "            # Check file size (M5 files should be substantial)\n",
    "            file_size_mb = path.stat().st_size / (1024 * 1024)\n",
    "            logger.info(f\"File {path.name}: {file_size_mb:.1f} MB\")\n",
    "            \n",
    "            if file_size_mb < 1:  # Basic sanity check\n",
    "                logger.warning(f\"File {file_path} seems too small ({file_size_mb:.1f} MB)\")\n",
    "        \n",
    "        logger.info(\"File integrity validation passed\")\n",
    "        return True\n",
    "    \n",
    "    def load_calendar_data(self) -> pd.DataFrame:\n",
    "        \"\"\"Load and validate calendar data with comprehensive error handling.\"\"\"\n",
    "        logger.info(\"Loading M5 calendar data...\")\n",
    "        \n",
    "        with self.memory_manager.memory_monitor(\"calendar_loading\"):\n",
    "            try:\n",
    "                calendar_dtypes = {\n",
    "                    'date': 'str',\n",
    "                    'wm_yr_wk': 'int32',\n",
    "                    'weekday': 'str',  # keep as str for mapping\n",
    "                    'd': 'str',\n",
    "                    'event_name_1': 'str',\n",
    "                    'event_type_1': 'str', \n",
    "                    'event_name_2': 'str',\n",
    "                    'event_type_2': 'str',\n",
    "                    'snap_CA': 'int8',\n",
    "                    'snap_TX': 'int8',\n",
    "                    'snap_WI': 'int8'\n",
    "                }\n",
    "                \n",
    "                calendar = pd.read_csv(self.config.calendar_path, dtype=calendar_dtypes)\n",
    "                calendar = downcast_with_stats(calendar, name=\"Calendar\")\n",
    "\n",
    "                # Convert date column once\n",
    "                calendar['date'] = pd.to_datetime(calendar['date'], errors='coerce')\n",
    "\n",
    "                # Map weekday strings to numbers BEFORE any numeric conversion\n",
    "                weekday_map = {\n",
    "                    'Sunday': 7,\n",
    "                    'Monday': 1,\n",
    "                    'Tuesday': 2,\n",
    "                    'Wednesday': 3,\n",
    "                    'Thursday': 4,\n",
    "                    'Friday': 5,\n",
    "                    'Saturday': 6\n",
    "                }\n",
    "                calendar['weekday'] = calendar['weekday'].map(weekday_map).astype('int8')\n",
    "\n",
    "                # Validate calendar length\n",
    "                expected_days = (pd.to_datetime('2016-05-22') - pd.to_datetime('2011-01-29')).days + 1\n",
    "                if len(calendar) != expected_days:\n",
    "                    logger.warning(f\"Expected {expected_days} calendar days, found {len(calendar)}\")\n",
    "                \n",
    "                # Date features\n",
    "                calendar['year'] = calendar['date'].dt.year\n",
    "                calendar['month'] = calendar['date'].dt.month  \n",
    "                calendar['day'] = calendar['date'].dt.day\n",
    "                calendar['quarter'] = calendar['date'].dt.quarter\n",
    "                calendar['week_of_year'] = calendar['date'].dt.isocalendar().week\n",
    "                \n",
    "                # Weekend flag (assuming Sat=6, Sun=7 as weekend)\n",
    "                calendar['is_weekend'] = calendar['weekday'].isin([6,7]).astype('int8')\n",
    "                \n",
    "                # Event processing\n",
    "                calendar['has_event'] = (\n",
    "                    calendar['event_name_1'].notna() | calendar['event_name_2'].notna()\n",
    "                ).astype('int8')\n",
    "                \n",
    "                # SNAP benefits\n",
    "                calendar['snap_any'] = (\n",
    "                    (calendar['snap_CA'] == 1) |\n",
    "                    (calendar['snap_TX'] == 1) |\n",
    "                    (calendar['snap_WI'] == 1)\n",
    "                ).astype('int8')\n",
    "                \n",
    "                logger.info(f\"Calendar loaded: {len(calendar)} days, {calendar['date'].min()} to {calendar['date'].max()}\")\n",
    "                \n",
    "                return calendar\n",
    "            \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to load calendar data: {e}\")\n",
    "                raise\n",
    "\n",
    "        \n",
    "    def load_prices_data(self) -> pd.DataFrame:\n",
    "        \"\"\"Load and validate pricing data.\"\"\"\n",
    "        logger.info(\"Loading M5 pricing data...\")\n",
    "        \n",
    "        with self.memory_manager.memory_monitor(\"prices_loading\"):\n",
    "            try:\n",
    "                # Load with memory-efficient dtypes\n",
    "                prices_dtypes = {\n",
    "                    'store_id': 'str',\n",
    "                    'item_id': 'str', \n",
    "                    'wm_yr_wk': 'int32',\n",
    "                    'sell_price': 'float32'\n",
    "                }\n",
    "                \n",
    "                prices = pd.read_csv(self.config.prices_path, dtype=prices_dtypes)\n",
    "                prices = downcast_with_stats(prices, name=\"Prices\")\n",
    "\n",
    "                if self.config.load_fraction:\n",
    "                    n_rows = int(len(prices) * self.config.load_fraction)\n",
    "                    prices = prices.iloc[:n_rows]  # Top rows\n",
    "                    logger.info(f\"Using only {n_rows} rows ({self.config.load_fraction:.0%}) of prices data.\")\n",
    "                    \n",
    "                # Data quality checks\n",
    "                null_prices = prices['sell_price'].isna().sum()\n",
    "                if null_prices > 0:\n",
    "                    logger.warning(f\"Found {null_prices} null prices ({null_prices/len(prices)*100:.2f}%)\")\n",
    "                \n",
    "                negative_prices = (prices['sell_price'] < 0).sum()\n",
    "                if negative_prices > 0:\n",
    "                    logger.error(f\"Found {negative_prices} negative prices - this is critical!\")\n",
    "                    # In production, you might want to raise an exception or alert\n",
    "                \n",
    "                zero_prices = (prices['sell_price'] == 0).sum()\n",
    "                if zero_prices > 0:\n",
    "                    logger.warning(f\"Found {zero_prices} zero prices ({zero_prices/len(prices)*100:.2f}%)\")\n",
    "                \n",
    "                # Price statistics\n",
    "                price_stats = prices['sell_price'].describe()\n",
    "                logger.info(f\"Price statistics: min=${price_stats['min']:.2f}, \"\n",
    "                        f\"max=${price_stats['max']:.2f}, mean=${price_stats['mean']:.2f}\")\n",
    "                \n",
    "                logger.info(f\"Prices loaded: {len(prices)} price points for {prices['item_id'].nunique()} items\")\n",
    "                \n",
    "                return prices\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to load prices data: {e}\")\n",
    "                raise\n",
    "\n",
    "    def load_sales_data_chunked(self) -> pd.DataFrame:\n",
    "        \"\"\"Load sales data in chunks to handle memory efficiently.\"\"\"\n",
    "        logger.info(\"Loading M5 sales data (chunked processing)...\")\n",
    "        \n",
    "        with self.memory_manager.memory_monitor(\"sales_loading\"):\n",
    "            try:\n",
    "                # First, get the column structure\n",
    "                sample_df = pd.read_csv(self.config.sales_train_path, nrows=1)\n",
    "                \n",
    "                # Identify day columns (d_1, d_2, etc.)\n",
    "                day_columns = [col for col in sample_df.columns if col.startswith('d_')]\n",
    "                id_columns = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n",
    "\n",
    "                total_rows = sum(1 for _ in open(self.config.sales_train_path)) - 1  # exclude header\n",
    "                \n",
    "                # Determine target rows based on load_fraction and optional max_rows\n",
    "                if hasattr(self.config, 'max_rows') and self.config.max_rows is not None:\n",
    "                    target_rows = min(int(total_rows * self.config.load_fraction), self.config.max_rows)\n",
    "                else:\n",
    "                    target_rows = int(total_rows * self.config.load_fraction)\n",
    "\n",
    "                chunk_size = self.config.chunk_size\n",
    "\n",
    "                total_chunks = (target_rows + chunk_size - 1) // chunk_size\n",
    "                logger.info(f\"Total rows in sales: {total_rows}\")\n",
    "                logger.info(f\"Loading {self.config.load_fraction:.0%} of dataset â†’ {target_rows} rows in ~{total_chunks} chunks (chunk size={chunk_size})\")                \n",
    "                # Load in chunks for memory efficiency\n",
    "                chunks = []\n",
    "                rows_loaded = 0\n",
    "                chunk_iterator = pd.read_csv(\n",
    "                    self.config.sales_train_path,\n",
    "                    chunksize=self.config.chunk_size,\n",
    "                    dtype={col: 'str' for col in id_columns}\n",
    "                )\n",
    "                \n",
    "                for i, chunk in enumerate(chunk_iterator, start=1):\n",
    "                    if rows_loaded >= target_rows:\n",
    "                        logger.info(f\"Reached target rows ({target_rows}), stopping further loading\")\n",
    "                        break\n",
    "\n",
    "                    if rows_loaded + len(chunk) > target_rows:\n",
    "                        chunk = chunk.iloc[:target_rows - rows_loaded]\n",
    "\n",
    "                    for col in day_columns:\n",
    "                        chunk[col] = pd.to_numeric(chunk[col], downcast='integer')\n",
    "\n",
    "                    chunks.append(chunk)\n",
    "                    rows_loaded += len(chunk)\n",
    "\n",
    "                    logger.info(f\"Loaded chunk {i}/{total_chunks}, rows loaded so far: {rows_loaded}\")\n",
    "\n",
    "                    self.memory_manager.check_memory_usage()\n",
    "\n",
    "                sales_wide = pd.concat(chunks, ignore_index=True)\n",
    "                del chunks\n",
    "                gc.collect()\n",
    "\n",
    "                logger.info(f\"Completed loading sales data: {len(sales_wide)} items Ã— {len(day_columns)} days\")\n",
    "\n",
    "                return sales_wide, day_columns\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to load sales data: {e}\")\n",
    "                raise\n",
    "\n",
    "    \n",
    "    def reshape_sales_data(self, sales_wide: pd.DataFrame, day_columns: List[str]) -> pd.DataFrame:\n",
    "        logger.info(\"Reshaping sales data from wide to long format...\")\n",
    "        with self.memory_manager.memory_monitor(\"sales_reshaping\"):\n",
    "            try:\n",
    "                id_columns = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n",
    "\n",
    "                # If a 'sales' column exists, rename it to avoid conflict\n",
    "                if 'sold' in sales_wide.columns:\n",
    "                    logger.warning(\"'sales' column exists in sales_wide, renaming to 'sales_old'\")\n",
    "                    sales_wide = sales_wide.rename(columns={'sold': 'sold_old'})\n",
    "\n",
    "\n",
    "\n",
    "                logger.info(f\"Columns before melt: {sales_wide.columns.tolist()}\")\n",
    "\n",
    "                sales_long = sales_wide.melt(\n",
    "                    id_vars=id_columns,\n",
    "                    value_vars=day_columns,\n",
    "                    var_name='d',\n",
    "                    value_name='sold'  \n",
    "                )\n",
    "\n",
    "                sales_long['sold'] = pd.to_numeric(sales_long['sold'], downcast='integer')\n",
    "                sales_long = sales_long.sort_values(['item_id', 'store_id', 'd'])\n",
    "\n",
    "                logger.info(f\"Reshaped to long format: {len(sales_long)} observations\")\n",
    "\n",
    "                return sales_long\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to reshape sales data: {e}\")\n",
    "                logger.error(f\"Columns at error: {sales_wide.columns.tolist()}\")\n",
    "                raise\n",
    "              \n",
    "    def create_master_dataset(\n",
    "        self,\n",
    "        sales_wide: pd.DataFrame,\n",
    "        calendar: pd.DataFrame,\n",
    "        prices: pd.DataFrame,\n",
    "        chunk_size: int = 1000000\n",
    "    ) -> Path:\n",
    "        \"\"\"\n",
    "        Chunked version of master dataset creation with debug and fix for merge key mismatches.\n",
    "        \"\"\"\n",
    "        import gc\n",
    "\n",
    "        logger.info(\"Creating master dataset in chunks with enhanced key alignment and debug...\")\n",
    "\n",
    "        output_dir = Path(\"data/processed/m5/master_chunks\")\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            max_chunks = 3\n",
    "            total_rows = len(sales_wide)\n",
    "            total_chunks = (total_rows + chunk_size - 1) // chunk_size\n",
    "            logger.info(f\"Total rows: {total_rows}, Chunk size: {chunk_size}, Total chunks: {total_chunks}\")\n",
    "\n",
    "            chunk_paths = []\n",
    "\n",
    "            for i, start in enumerate(range(0, total_rows, chunk_size), 1):\n",
    "                if i > max_chunks:\n",
    "                    logger.info(f\"Reached max chunk limit ({max_chunks}), stopping further processing\")\n",
    "                    break\n",
    "\n",
    "                end = min(start + chunk_size, total_rows)\n",
    "                logger.info(f\"Processing chunk {i}/{total_chunks}: rows {start} to {end}\")\n",
    "                chunk = sales_wide.iloc[start:end].copy()\n",
    "\n",
    "                # Melt the chunk\n",
    "                id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n",
    "\n",
    "                sales_long = chunk.melt(\n",
    "                    id_vars=id_vars,\n",
    "                    var_name='d',\n",
    "                    value_name='sales'\n",
    "                )\n",
    "\n",
    "                # Step 1: Align calendar 'd' categories with sales_long 'd'\n",
    "                sales_long['d'] = sales_long['d'].astype('category')\n",
    "                calendar['d'] = calendar['d'].astype('category')\n",
    "                calendar['d'] = calendar['d'].cat.set_categories(sales_long['d'].cat.categories)\n",
    "\n",
    "\n",
    "                # Step 2: Merge with calendar\n",
    "                df = sales_long.merge(calendar, on='d', how='left')\n",
    "\n",
    "                # Step 5: Merge with prices\n",
    "                df = df.merge(\n",
    "                    prices[['store_id', 'item_id', 'wm_yr_wk', 'sell_price']],\n",
    "                    on=['store_id', 'item_id', 'wm_yr_wk'],\n",
    "                    how='left'\n",
    "                )\n",
    "\n",
    "\n",
    "                # Compute revenue\n",
    "                df['sales'] = pd.to_numeric(df['sales'], errors='coerce').fillna(0).astype('float32')\n",
    "                # df['sell_price'] = df['sell_price_y']\n",
    "                # df.drop(['sell_price_x'], axis=1, inplace=True)\n",
    "                df['revenue'] = (df['sales'] * df['sell_price']).astype('float32')\n",
    "\n",
    "                # Optional: downcast to reduce memory footprint (assuming you have this function)\n",
    "                df = downcast_with_stats(df, name=f\"Master Chunk [{start}-{end}]\")\n",
    "\n",
    "                # Save chunk to disk\n",
    "                out_path = output_dir / f\"master_chunk_{start}_{end}.parquet\"\n",
    "                df.to_parquet(out_path, index=False)\n",
    "                chunk_paths.append(out_path)\n",
    "\n",
    "                logger.info(f\"Saved chunk: {out_path} ({df.shape[0]} rows)\")\n",
    "\n",
    "                del chunk, sales_long, df\n",
    "                gc.collect()\n",
    "\n",
    "            logger.info(f\"All master chunks saved to {output_dir}\")\n",
    "            return output_dir\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed during chunked master dataset creation: {e}\")\n",
    "            raise\n",
    "\n",
    "    def load_master_dataset(self, master_data_dir: Path) -> pd.DataFrame:\n",
    "       import glob\n",
    "       parquet_files = glob.glob(str(master_data_dir / \"*.parquet\"))\n",
    "       dfs = [pd.read_parquet(f) for f in parquet_files]\n",
    "       return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    def validate_data_quality(self, df: pd.DataFrame) -> DataQualityMetrics:\n",
    "        \"\"\"Comprehensive data quality validation for M5 dataset.\"\"\"\n",
    "        logger.info(\"Performing comprehensive data quality validation...\")\n",
    "        \n",
    "        with self.memory_manager.memory_monitor(\"data_quality_validation\"):\n",
    "            try:\n",
    "                # Basic counts\n",
    "                total_obs = len(df)\n",
    "                unique_series = df.groupby(['store_id', 'item_id'],observed=False).ngroups\n",
    "                \n",
    "                # Sales data quality\n",
    "                missing_sales = df['sales'].isna().sum()\n",
    "                missing_sales_pct = (missing_sales / total_obs) * 100\n",
    "                \n",
    "                zero_sales = (df['sales'] == 0).sum()\n",
    "                zero_sales_pct = (zero_sales / total_obs) * 100\n",
    "                \n",
    "                negative_sales = (df['sales'] < 0).sum()\n",
    "                \n",
    "                # Price coverage\n",
    "                missing_prices = df['sell_price'].isna().sum()\n",
    "                price_coverage_pct = ((total_obs - missing_prices) / total_obs) * 100\n",
    "                \n",
    "                # Calendar coverage (should be 100% after join)\n",
    "                missing_dates = df['date'].isna().sum()\n",
    "                calendar_coverage_pct = ((total_obs - missing_dates) / total_obs) * 100\n",
    "                \n",
    "                # Data completeness score (weighted average of key metrics)\n",
    "                completeness_score = (\n",
    "                    (100 - missing_sales_pct) * 0.4 +  # Sales most important\n",
    "                    price_coverage_pct * 0.3 +         # Prices important for revenue\n",
    "                    calendar_coverage_pct * 0.3        # Calendar features important\n",
    "                ) / 100\n",
    "                \n",
    "                # Temporal consistency (check for gaps in time series)\n",
    "                temporal_gaps = 0\n",
    "                sample_series = df.groupby(['store_id', 'item_id'],observed=False).head(1000)  # Sample for efficiency\n",
    "                for (store, item), group in sample_series.groupby(['store_id', 'item_id'],observed=False):\n",
    "                    date_diff = group['date'].diff().dt.days\n",
    "                    gaps = (date_diff > 1).sum()\n",
    "                    temporal_gaps += gaps\n",
    "                \n",
    "                temporal_consistency_score = max(0, 1 - (temporal_gaps / unique_series))\n",
    "                \n",
    "                # Hierarchical consistency (validate item-dept-cat relationships)\n",
    "                hierarchy_issues = 0\n",
    "                item_dept = df.groupby('item_id')['dept_id'].nunique()\n",
    "                hierarchy_issues += (item_dept > 1).sum()  # Items should have single dept\n",
    "                \n",
    "                dept_cat = df.groupby('dept_id')['cat_id'].nunique()\n",
    "                hierarchy_issues += (dept_cat > 1).sum()  # Depts should have single category\n",
    "                \n",
    "                hierarchical_consistency_score = max(0, 1 - (hierarchy_issues / (df['item_id'].nunique() + df['dept_id'].nunique())))\n",
    "                \n",
    "                # Filter valid time series (meet minimum requirements)\n",
    "                series_sales_counts = df.groupby(['store_id', 'item_id'],observed=False)['sales'].count()\n",
    "                valid_series = (series_sales_counts >= self.config.min_sales_per_item).sum()\n",
    "                \n",
    "                series_zero_pcts = df.groupby(['store_id', 'item_id'],observed=False)['sales'].apply(lambda x: (x == 0).mean())\n",
    "                valid_series_zero = (series_zero_pcts <= self.config.max_zero_days_pct).sum()\n",
    "                \n",
    "                valid_time_series = min(valid_series, valid_series_zero)\n",
    "                \n",
    "                # Create quality metrics\n",
    "                quality_metrics = DataQualityMetrics(\n",
    "                    total_time_series=unique_series,\n",
    "                    valid_time_series=valid_time_series,\n",
    "                    total_observations=total_obs,\n",
    "                    missing_sales_pct=missing_sales_pct,\n",
    "                    zero_sales_pct=zero_sales_pct,\n",
    "                    negative_sales_count=negative_sales,\n",
    "                    price_coverage_pct=price_coverage_pct,\n",
    "                    calendar_coverage_pct=calendar_coverage_pct,\n",
    "                    data_completeness_score=completeness_score,\n",
    "                    temporal_consistency_score=temporal_consistency_score,\n",
    "                    hierarchical_consistency_score=hierarchical_consistency_score\n",
    "                )\n",
    "                \n",
    "                self.quality_metrics = quality_metrics\n",
    "                \n",
    "                # Log quality summary\n",
    "                logger.info(\"=\" * 60)\n",
    "                logger.info(\"DATA QUALITY SUMMARY\")\n",
    "                logger.info(\"=\" * 60)\n",
    "                logger.info(f\"Total time series: {unique_series:,}\")\n",
    "                logger.info(f\"Valid time series: {valid_time_series:,} ({valid_time_series/unique_series*100:.1f}%)\")\n",
    "                logger.info(f\"Total observations: {total_obs:,}\")\n",
    "                logger.info(f\"Missing sales: {missing_sales_pct:.2f}%\")\n",
    "                logger.info(f\"Zero sales: {zero_sales_pct:.1f}%\")\n",
    "                logger.info(f\"Negative sales: {negative_sales:,}\")\n",
    "                logger.info(f\"Price coverage: {price_coverage_pct:.1f}%\")\n",
    "\n",
    "                return quality_metrics\n",
    "            \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Data quality validation failed: {e}\")\n",
    "                raise\n",
    "\n",
    "\n",
    "    def save_quality_report(self, metrics: DataQualityMetrics):\n",
    "        \"\"\"Save comprehensive data quality report.\"\"\"\n",
    "        report_path = Path(\"data/quality/m5/quality_report.json\")\n",
    "        report_path.parent.mkdir(parents=True, exist_ok=True)  # Ensure the folder exists\n",
    "\n",
    "        report = {\n",
    "            'timestamp': datetime.utcnow().isoformat(),\n",
    "            'dataset': 'M5_Walmart',\n",
    "            'quality_metrics': asdict(metrics),\n",
    "            'validation_config': {\n",
    "                'min_sales_per_item': self.config.min_sales_per_item,\n",
    "                'max_zero_days_pct': self.config.max_zero_days_pct\n",
    "            }\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            with open(report_path, 'w') as f:\n",
    "                json.dump(report, f, indent=2, cls=NumpyEncoder)\n",
    "\n",
    "            logger.info(f\"âœ“ Quality report saved to {report_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save quality report: {e}\", exc_info=True)\n",
    "\n",
    "    \n",
    "    def save_processed_data(self, df: pd.DataFrame, filename: str = \"processed\"):\n",
    "        \"\"\"Save processed data with optimal compression and metadata.\"\"\"\n",
    "        logger.info(f\"Saving processed M5 data: {filename}\")\n",
    "        \n",
    "        with self.memory_manager.memory_monitor(\"data_saving\"):\n",
    "            try:\n",
    "                output_path = Path(f\"data/processed/m5/{filename}.parquet\")\n",
    "                \n",
    "                # Add processing metadata\n",
    "                metadata = {\n",
    "                    'processed_at': datetime.utcnow().isoformat(),\n",
    "                    'shape': f\"{df.shape[0]}x{df.shape[1]}\",\n",
    "                    'date_range_start': df['date'].min().isoformat(),\n",
    "                    'date_range_end': df['date'].max().isoformat(),\n",
    "                    'unique_time_series': int(df.groupby(['store_id', 'item_id'],observed=False).ngroups),\n",
    "                    'data_quality_score': float(self.quality_metrics.data_completeness_score) if self.quality_metrics else None\n",
    "                }\n",
    "                \n",
    "                # Convert to PyArrow table with metadata\n",
    "                table = pa.Table.from_pandas(df)\n",
    "                table = table.replace_schema_metadata({\n",
    "                    key: str(value) for key, value in metadata.items()\n",
    "                })\n",
    "                \n",
    "                # Save with optimal compression\n",
    "                pq.write_table(\n",
    "                    table, \n",
    "                    output_path,\n",
    "                    compression=self.config.compression,\n",
    "                    row_group_size=50000,  # Optimize for read performance\n",
    "                    use_dictionary=True     # Compress categorical columns\n",
    "                )\n",
    "                \n",
    "                file_size_mb = output_path.stat().st_size / (1024 * 1024)\n",
    "                logger.info(f\"Saved processed data: {output_path} ({file_size_mb:.1f} MB)\")\n",
    "                \n",
    "                \n",
    "                return output_path\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to save processed data: {e}\")\n",
    "                raise\n",
    "    \n",
    "    def run_full_pipeline(self) -> Tuple[pd.DataFrame, DataQualityMetrics]:\n",
    "        \"\"\"Run the complete M5 dataset processing pipeline.\"\"\"\n",
    "        logger.info(\"Starting FAANG-level M5 dataset processing pipeline...\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Validate file integrity\n",
    "            if not self.validate_file_integrity():\n",
    "                raise ValueError(\"File integrity validation failed\")\n",
    "            \n",
    "            # Step 2: Load calendar data\n",
    "            calendar = self.load_calendar_data()\n",
    "            \n",
    "            # Step 3: Load pricing data\n",
    "            prices = self.load_prices_data()\n",
    "            \n",
    "            # Step 4: Load and reshape sales data\n",
    "            sales_wide, day_columns = self.load_sales_data_chunked()\n",
    "            \n",
    "            # Step 5: Create master dataset\n",
    "            master_data_dir = self.create_master_dataset(sales_wide, calendar, prices)\n",
    "\n",
    "            master_df = self.load_master_dataset(master_data_dir)\n",
    "\n",
    "            # Free memory\n",
    "            del sales_wide, calendar, prices\n",
    "            gc.collect()\n",
    "            \n",
    "            # Step 6: Validate data quality\n",
    "            quality_metrics = self.validate_data_quality(master_df)\n",
    "\n",
    "            self.save_quality_report(quality_metrics)\n",
    "            \n",
    "            # Step 7: Save processed data\n",
    "            if self.config.save_interim:\n",
    "                self.save_processed_data(master_df, \"master\")\n",
    "            \n",
    "            duration = time.time() - start_time\n",
    "            \n",
    "            logger.info(\"=\" * 80)\n",
    "            logger.info(\"ðŸŽ‰ M5 DATASET PROCESSING COMPLETED SUCCESSFULLY\")\n",
    "            logger.info(\"=\" * 80)\n",
    "            logger.info(f\"Processing time: {duration/60:.1f} minutes\")\n",
    "            logger.info(f\"Final dataset shape: {master_df.shape}\")\n",
    "            logger.info(f\"Memory usage: {self.memory_manager.get_memory_usage()['rss_gb']:.2f} GB\")\n",
    "            logger.info(f\"Data quality score: {quality_metrics.data_completeness_score:.3f}\")\n",
    "            logger.info(\"=\" * 80)\n",
    "            \n",
    "            return master_df, quality_metrics\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Pipeline failed: {e}\")\n",
    "            raise\n",
    "        finally:\n",
    "            # Cleanup\n",
    "            self.db.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = M5DatasetConfig()\n",
    "processor = M5DatasetProcessor(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "1ac728f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expected 1941 calendar days, found 1969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'weekday' converted to category\n",
      "Column 'd' converted to category\n",
      "Column 'event_name_1' converted to category\n",
      "Column 'event_type_1' converted to category\n",
      "Column 'event_name_2' converted to category\n",
      "Column 'event_type_2' converted to category\n",
      "ðŸ”§ Calendar memory usage: 0.67 MB â†’ 0.23 MB (65.8% reduction)\n"
     ]
    }
   ],
   "source": [
    "calendar = processor.load_calendar_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "3db8c1d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>weekday</th>\n",
       "      <th>wday</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>d</th>\n",
       "      <th>event_name_1</th>\n",
       "      <th>event_type_1</th>\n",
       "      <th>event_name_2</th>\n",
       "      <th>event_type_2</th>\n",
       "      <th>snap_CA</th>\n",
       "      <th>snap_TX</th>\n",
       "      <th>snap_WI</th>\n",
       "      <th>day</th>\n",
       "      <th>quarter</th>\n",
       "      <th>week_of_year</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>has_event</th>\n",
       "      <th>snap_any</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-30</td>\n",
       "      <td>11101</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-31</td>\n",
       "      <td>11101</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-02-01</td>\n",
       "      <td>11101</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-02-02</td>\n",
       "      <td>11101</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2011</td>\n",
       "      <td>d_5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964</th>\n",
       "      <td>2016-06-15</td>\n",
       "      <td>11620</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>2016</td>\n",
       "      <td>d_1965</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1965</th>\n",
       "      <td>2016-06-16</td>\n",
       "      <td>11620</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2016</td>\n",
       "      <td>d_1966</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1966</th>\n",
       "      <td>2016-06-17</td>\n",
       "      <td>11620</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>2016</td>\n",
       "      <td>d_1967</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>2016-06-18</td>\n",
       "      <td>11621</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2016</td>\n",
       "      <td>d_1968</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1968</th>\n",
       "      <td>2016-06-19</td>\n",
       "      <td>11621</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2016</td>\n",
       "      <td>d_1969</td>\n",
       "      <td>NBAFinalsEnd</td>\n",
       "      <td>Sporting</td>\n",
       "      <td>Father's day</td>\n",
       "      <td>Cultural</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1969 rows Ã— 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  wm_yr_wk  weekday  wday  month  year       d  event_name_1  \\\n",
       "0    2011-01-29     11101        6     1      1  2011     d_1           NaN   \n",
       "1    2011-01-30     11101        7     2      1  2011     d_2           NaN   \n",
       "2    2011-01-31     11101        1     3      1  2011     d_3           NaN   \n",
       "3    2011-02-01     11101        2     4      2  2011     d_4           NaN   \n",
       "4    2011-02-02     11101        3     5      2  2011     d_5           NaN   \n",
       "...         ...       ...      ...   ...    ...   ...     ...           ...   \n",
       "1964 2016-06-15     11620        3     5      6  2016  d_1965           NaN   \n",
       "1965 2016-06-16     11620        4     6      6  2016  d_1966           NaN   \n",
       "1966 2016-06-17     11620        5     7      6  2016  d_1967           NaN   \n",
       "1967 2016-06-18     11621        6     1      6  2016  d_1968           NaN   \n",
       "1968 2016-06-19     11621        7     2      6  2016  d_1969  NBAFinalsEnd   \n",
       "\n",
       "     event_type_1  event_name_2 event_type_2  snap_CA  snap_TX  snap_WI  day  \\\n",
       "0             NaN           NaN          NaN        0        0        0   29   \n",
       "1             NaN           NaN          NaN        0        0        0   30   \n",
       "2             NaN           NaN          NaN        0        0        0   31   \n",
       "3             NaN           NaN          NaN        1        1        0    1   \n",
       "4             NaN           NaN          NaN        1        0        1    2   \n",
       "...           ...           ...          ...      ...      ...      ...  ...   \n",
       "1964          NaN           NaN          NaN        0        1        1   15   \n",
       "1965          NaN           NaN          NaN        0        0        0   16   \n",
       "1966          NaN           NaN          NaN        0        0        0   17   \n",
       "1967          NaN           NaN          NaN        0        0        0   18   \n",
       "1968     Sporting  Father's day     Cultural        0        0        0   19   \n",
       "\n",
       "      quarter  week_of_year  is_weekend  has_event  snap_any  \n",
       "0           1             4           1          0         0  \n",
       "1           1             4           1          0         0  \n",
       "2           1             5           0          0         0  \n",
       "3           1             5           0          0         1  \n",
       "4           1             5           0          0         1  \n",
       "...       ...           ...         ...        ...       ...  \n",
       "1964        2            24           0          0         1  \n",
       "1965        2            24           0          0         0  \n",
       "1966        2            24           0          0         0  \n",
       "1967        2            24           1          0         0  \n",
       "1968        2            24           1          1         0  \n",
       "\n",
       "[1969 rows x 20 columns]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "12ddcceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\Administrator\\OneDrive\\Desktop\\SalesAI\\backend\\data\\raw\\calendar.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "1bb11f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1969 entries, 0 to 1968\n",
      "Data columns (total 14 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   date          1969 non-null   object\n",
      " 1   wm_yr_wk      1969 non-null   int64 \n",
      " 2   weekday       1969 non-null   object\n",
      " 3   wday          1969 non-null   int64 \n",
      " 4   month         1969 non-null   int64 \n",
      " 5   year          1969 non-null   int64 \n",
      " 6   d             1969 non-null   object\n",
      " 7   event_name_1  162 non-null    object\n",
      " 8   event_type_1  162 non-null    object\n",
      " 9   event_name_2  5 non-null      object\n",
      " 10  event_type_2  5 non-null      object\n",
      " 11  snap_CA       1969 non-null   int64 \n",
      " 12  snap_TX       1969 non-null   int64 \n",
      " 13  snap_WI       1969 non-null   int64 \n",
      "dtypes: int64(7), object(7)\n",
      "memory usage: 215.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "608f1afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1969 entries, 0 to 1968\n",
      "Data columns (total 20 columns):\n",
      " #   Column        Non-Null Count  Dtype         \n",
      "---  ------        --------------  -----         \n",
      " 0   date          1969 non-null   datetime64[ns]\n",
      " 1   wm_yr_wk      1969 non-null   int16         \n",
      " 2   weekday       1969 non-null   int8          \n",
      " 3   wday          1969 non-null   int8          \n",
      " 4   month         1969 non-null   int32         \n",
      " 5   year          1969 non-null   int32         \n",
      " 6   d             1969 non-null   category      \n",
      " 7   event_name_1  162 non-null    category      \n",
      " 8   event_type_1  162 non-null    category      \n",
      " 9   event_name_2  5 non-null      category      \n",
      " 10  event_type_2  5 non-null      category      \n",
      " 11  snap_CA       1969 non-null   int8          \n",
      " 12  snap_TX       1969 non-null   int8          \n",
      " 13  snap_WI       1969 non-null   int8          \n",
      " 14  day           1969 non-null   int32         \n",
      " 15  quarter       1969 non-null   int32         \n",
      " 16  week_of_year  1969 non-null   UInt32        \n",
      " 17  is_weekend    1969 non-null   int8          \n",
      " 18  has_event     1969 non-null   int8          \n",
      " 19  snap_any      1969 non-null   int8          \n",
      "dtypes: UInt32(1), category(5), datetime64[ns](1), int16(1), int32(4), int8(8)\n",
      "memory usage: 168.4 KB\n"
     ]
    }
   ],
   "source": [
    "calendar.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "3038de43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date             0.000000\n",
       "wm_yr_wk         0.000000\n",
       "weekday          0.000000\n",
       "wday             0.000000\n",
       "month            0.000000\n",
       "year             0.000000\n",
       "d                0.000000\n",
       "event_name_1    91.772473\n",
       "event_type_1    91.772473\n",
       "event_name_2    99.746064\n",
       "event_type_2    99.746064\n",
       "snap_CA          0.000000\n",
       "snap_TX          0.000000\n",
       "snap_WI          0.000000\n",
       "day              0.000000\n",
       "quarter          0.000000\n",
       "week_of_year     0.000000\n",
       "is_weekend       0.000000\n",
       "has_event        0.000000\n",
       "snap_any         0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calendar.isnull().mean()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "d5dd9d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date             0.000000\n",
       "wm_yr_wk         0.000000\n",
       "weekday          0.000000\n",
       "wday             0.000000\n",
       "month            0.000000\n",
       "year             0.000000\n",
       "d                0.000000\n",
       "event_name_1    91.772473\n",
       "event_type_1    91.772473\n",
       "event_name_2    99.746064\n",
       "event_type_2    99.746064\n",
       "snap_CA          0.000000\n",
       "snap_TX          0.000000\n",
       "snap_WI          0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().mean()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "d845d6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r\"C:\\Users\\Administrator\\OneDrive\\Desktop\\SalesAI\\backend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "214fdcb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'store_id' converted to category\n",
      "Column 'item_id' converted to category\n",
      "ðŸ”§ Prices memory usage: 905.33 MB â†’ 58.98 MB (93.5% reduction)\n"
     ]
    }
   ],
   "source": [
    "prices = processor.load_prices_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "1a2a82d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 68411 entries, 0 to 68410\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype   \n",
      "---  ------      --------------  -----   \n",
      " 0   store_id    68411 non-null  category\n",
      " 1   item_id     68411 non-null  category\n",
      " 2   wm_yr_wk    68411 non-null  int16   \n",
      " 3   sell_price  68411 non-null  float32 \n",
      "dtypes: category(2), float32(1), int16(1)\n",
      "memory usage: 690.1 KB\n"
     ]
    }
   ],
   "source": [
    "prices.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "0f078349",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_df = pd.read_csv(r\"C:\\Users\\Administrator\\OneDrive\\Desktop\\SalesAI\\backend\\data\\raw\\sell_prices.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "a33f7ef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((68411, 4), (6841121, 4))"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices.shape, prices_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "5a7e5cef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "store_id      0.0\n",
       "item_id       0.0\n",
       "wm_yr_wk      0.0\n",
       "sell_price    0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices.isnull().mean()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "c80cda94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "store_id      0.0\n",
       "item_id       0.0\n",
       "wm_yr_wk      0.0\n",
       "sell_price    0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices_df.isnull().mean()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "885aa1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_13332\\213799540.py:231: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  chunk[col] = pd.to_numeric(chunk[col], downcast='integer')\n"
     ]
    }
   ],
   "source": [
    "sales_wide, day_columns = processor.load_sales_data_chunked()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "c71934f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'd_1',\n",
       "       'd_2', 'd_3', 'd_4',\n",
       "       ...\n",
       "       'd_1932', 'd_1933', 'd_1934', 'd_1935', 'd_1936', 'd_1937', 'd_1938',\n",
       "       'd_1939', 'd_1940', 'd_1941'],\n",
       "      dtype='object', length=1947)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_wide.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "7f968291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(304, 1947)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_wide.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "32e50086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "item_id     0\n",
       "dept_id     0\n",
       "cat_id      0\n",
       "store_id    0\n",
       "           ..\n",
       "d_1937      0\n",
       "d_1938      0\n",
       "d_1939      0\n",
       "d_1940      0\n",
       "d_1941      0\n",
       "Length: 1947, dtype: int64"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_wide.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "fa40ecad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1941"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(day_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "7ba9e45e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "9d376f6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calendar['wm_yr_wk'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "fb312de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating master dataset in chunks...\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating master dataset in chunks...\")\n",
    "output_dir = Path(\"data/processed/m5/master_chunks\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "1b87f078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 304, Chunk size: 1000000, Total chunks to process: 1\n"
     ]
    }
   ],
   "source": [
    "chunk_size: int = 1000000\n",
    "max_chunks = 3\n",
    "total_rows = len(sales_wide)\n",
    "total_chunks = (total_rows + chunk_size - 1) // chunk_size  # ceil division\n",
    "print(f\"Total rows: {total_rows}, Chunk size: {chunk_size}, Total chunks to process: {total_chunks}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "71387b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_paths = []\n",
    "\n",
    "for i, start in enumerate(range(0, total_rows, chunk_size), 1):\n",
    "    if i > max_chunks:\n",
    "        logger.info(f\"Reached max chunk limit ({max_chunks}), stopping further processing\")\n",
    "        break\n",
    "\n",
    "    end = min(start + chunk_size, total_rows)\n",
    "    logger.info(f\"Processing chunk {i}/{total_chunks}: rows {start} to {end}\")\n",
    "    chunk = sales_wide.iloc[start:end].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "aa2cd401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "item_id     0\n",
       "dept_id     0\n",
       "cat_id      0\n",
       "store_id    0\n",
       "           ..\n",
       "d_1937      0\n",
       "d_1938      0\n",
       "d_1939      0\n",
       "d_1940      0\n",
       "d_1941      0\n",
       "Length: 1947, dtype: int64"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ab7cf995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(304, 1947)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "22ff2e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt the chunk\n",
    "sales_long = chunk.melt(\n",
    "                        id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'],\n",
    "                        var_name='d',\n",
    "                        value_name='sales'\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a4cc6d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(590064, 8)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_long.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3f266ec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "item_id     0\n",
       "dept_id     0\n",
       "cat_id      0\n",
       "store_id    0\n",
       "state_id    0\n",
       "d           0\n",
       "sales       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_long.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "214f7695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(sales_long['sales'].isna().sum())          # Count NaNs\n",
    "print(np.isinf(sales_long['sales']).sum())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a04b4ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_long['d'] = sales_long['d'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "439a813f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CategoricalDtype(categories=['d_1', 'd_10', 'd_100', 'd_1000', 'd_1001', 'd_1002',\n",
       "                  'd_1003', 'd_1004', 'd_1005', 'd_1006',\n",
       "                  ...\n",
       "                  'd_990', 'd_991', 'd_992', 'd_993', 'd_994', 'd_995',\n",
       "                  'd_996', 'd_997', 'd_998', 'd_999'],\n",
       ", ordered=False, categories_dtype=object)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_long['d'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "49123d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CategoricalDtype(categories=['d_1', 'd_10', 'd_100', 'd_1000', 'd_1001', 'd_1002',\n",
       "                  'd_1003', 'd_1004', 'd_1005', 'd_1006',\n",
       "                  ...\n",
       "                  'd_990', 'd_991', 'd_992', 'd_993', 'd_994', 'd_995',\n",
       "                  'd_996', 'd_997', 'd_998', 'd_999'],\n",
       ", ordered=False, categories_dtype=object)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calendar['d'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e2f5abca",
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar['d'] = calendar['d'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "96e74b05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CategoricalDtype(categories=['d_1', 'd_10', 'd_100', 'd_1000', 'd_1001', 'd_1002',\n",
       "                  'd_1003', 'd_1004', 'd_1005', 'd_1006',\n",
       "                  ...\n",
       "                  'd_990', 'd_991', 'd_992', 'd_993', 'd_994', 'd_995',\n",
       "                  'd_996', 'd_997', 'd_998', 'd_999'],\n",
       ", ordered=False, categories_dtype=object)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calendar['d'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "efbfa924",
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar['d'] = calendar['d'].cat.set_categories(sales_long['d'].cat.categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ccd42c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calendar['d'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ec9f1111",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sales_long.merge(calendar, on='d',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a6cdd539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1969, 14), (1969, 20), (590064, 8))"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape, calendar.shape,sales_long.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9a4beae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                   0\n",
       "item_id              0\n",
       "dept_id              0\n",
       "cat_id               0\n",
       "store_id             0\n",
       "state_id             0\n",
       "d                    0\n",
       "sales                0\n",
       "date                 0\n",
       "wm_yr_wk             0\n",
       "weekday              0\n",
       "wday                 0\n",
       "month                0\n",
       "year                 0\n",
       "event_name_1    542032\n",
       "event_type_1    542032\n",
       "event_name_2    588848\n",
       "event_type_2    588848\n",
       "snap_CA              0\n",
       "snap_TX              0\n",
       "snap_WI              0\n",
       "day                  0\n",
       "quarter              0\n",
       "week_of_year         0\n",
       "is_weekend           0\n",
       "has_event            0\n",
       "snap_any             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4a8d2fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique 'd' in sales_long: 1941\n",
      "Unique 'd' in calendar: 19\n",
      "Number of 'd' values in sales_long not in calendar: 1922\n",
      "Some missing 'd' values: ['d_1365', 'd_485', 'd_1741', 'd_68', 'd_1421', 'd_144', 'd_1860', 'd_1495', 'd_1787', 'd_1908']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique 'd' in sales_long: {sales_long['d'].nunique()}\")\n",
    "print(f\"Unique 'd' in calendar: {calendar['d'].nunique()}\")\n",
    "\n",
    "missing_days = set(sales_long['d'].unique()) - set(calendar['d'].unique())\n",
    "print(f\"Number of 'd' values in sales_long not in calendar: {len(missing_days)}\")\n",
    "print(f\"Some missing 'd' values: {list(missing_days)[:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "48d21091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category\n",
      "category\n"
     ]
    }
   ],
   "source": [
    "print(sales_long['d'].dtype)\n",
    "print(calendar['d'].dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "846b6984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>sell_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CA_1</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>11325</td>\n",
       "      <td>9.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CA_1</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>11326</td>\n",
       "      <td>9.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CA_1</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>11327</td>\n",
       "      <td>8.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CA_1</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>11328</td>\n",
       "      <td>8.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CA_1</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>11329</td>\n",
       "      <td>8.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68406</th>\n",
       "      <td>CA_1</td>\n",
       "      <td>HOBBIES_1_304</td>\n",
       "      <td>11317</td>\n",
       "      <td>3.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68407</th>\n",
       "      <td>CA_1</td>\n",
       "      <td>HOBBIES_1_304</td>\n",
       "      <td>11318</td>\n",
       "      <td>3.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68408</th>\n",
       "      <td>CA_1</td>\n",
       "      <td>HOBBIES_1_304</td>\n",
       "      <td>11319</td>\n",
       "      <td>3.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68409</th>\n",
       "      <td>CA_1</td>\n",
       "      <td>HOBBIES_1_304</td>\n",
       "      <td>11320</td>\n",
       "      <td>3.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68410</th>\n",
       "      <td>CA_1</td>\n",
       "      <td>HOBBIES_1_304</td>\n",
       "      <td>11321</td>\n",
       "      <td>3.98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68411 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      store_id        item_id  wm_yr_wk  sell_price\n",
       "0         CA_1  HOBBIES_1_001     11325        9.58\n",
       "1         CA_1  HOBBIES_1_001     11326        9.58\n",
       "2         CA_1  HOBBIES_1_001     11327        8.26\n",
       "3         CA_1  HOBBIES_1_001     11328        8.26\n",
       "4         CA_1  HOBBIES_1_001     11329        8.26\n",
       "...        ...            ...       ...         ...\n",
       "68406     CA_1  HOBBIES_1_304     11317        3.98\n",
       "68407     CA_1  HOBBIES_1_304     11318        3.98\n",
       "68408     CA_1  HOBBIES_1_304     11319        3.98\n",
       "68409     CA_1  HOBBIES_1_304     11320        3.98\n",
       "68410     CA_1  HOBBIES_1_304     11321        3.98\n",
       "\n",
       "[68411 rows x 4 columns]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "7a28bbd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((590064, 27), (68411, 4))"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape,prices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "ff83f1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with prices\n",
    "df = df.merge(\n",
    "            prices[['store_id', 'item_id', 'wm_yr_wk', 'sell_price']],\n",
    "            on=['store_id', 'item_id', 'wm_yr_wk'],\n",
    "            how='left'\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "58112ba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(469142, 29)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "0de463c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                   0\n",
       "item_id              0\n",
       "dept_id              0\n",
       "cat_id               0\n",
       "store_id             0\n",
       "state_id             0\n",
       "d                    0\n",
       "sales                0\n",
       "date                 0\n",
       "wm_yr_wk             0\n",
       "weekday              0\n",
       "wday                 0\n",
       "month                0\n",
       "year                 0\n",
       "event_name_1    431106\n",
       "event_type_1    431106\n",
       "event_name_2    468193\n",
       "event_type_2    468193\n",
       "snap_CA              0\n",
       "snap_TX              0\n",
       "snap_WI              0\n",
       "day                  0\n",
       "quarter              0\n",
       "week_of_year         0\n",
       "is_weekend           0\n",
       "has_event            0\n",
       "snap_any             0\n",
       "sell_price_x         0\n",
       "sell_price_y         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "0b9eabc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sales'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "21e2dbdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print('sell_price' in df.columns)  # Should be True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "76b1708a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['revenue'] = (df['sales'] * df['sell_price']).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "212d3057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['revenue'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "f1eabc9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'id' converted to category\n",
      "Column 'item_id' converted to category\n",
      "Column 'dept_id' converted to category\n",
      "Column 'cat_id' converted to category\n",
      "Column 'store_id' converted to category\n",
      "Column 'state_id' converted to category\n",
      "ðŸ”§ Master Chunk [0-304] memory usage: 207.33 MB â†’ 23.06 MB (88.9% reduction)\n"
     ]
    }
   ],
   "source": [
    "df = downcast_with_stats(df, name=f\"Master Chunk [{start}-{end}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "f6ea5af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = output_dir / f\"master_chunk_{start}_{end}.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "9cc07b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(out_path, index=False)\n",
    "chunk_paths.append(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "346422a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk: data\\processed\\m5\\master_chunks\\master_chunk_0_304.parquet (469142 rows)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Saved chunk: {out_path} ({df.shape[0]} rows)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "203663f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2908"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del chunk, sales_long, df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "e93b7ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('data/processed/m5/master_chunks')"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "b4ad0bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'id' converted to category\n",
      "Column 'item_id' converted to category\n",
      "Column 'dept_id' converted to category\n",
      "Column 'cat_id' converted to category\n",
      "Column 'store_id' converted to category\n",
      "Column 'state_id' converted to category\n",
      "ðŸ”§ Master Chunk [0-304] memory usage: 260.73 MB â†’ 28.94 MB (88.9% reduction)\n"
     ]
    }
   ],
   "source": [
    "master_data = processor.create_master_dataset(sales_wide,calendar,prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "6355c2f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('data/processed/m5/master_chunks')"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "2be0636f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.read_parquet(r\"C:\\Users\\Administrator\\OneDrive\\Desktop\\SalesAI\\backend\\data\\processed\\m5\\master_chunks\\master_chunk_0_304.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "7d651cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = processor.load_master_dataset(master_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "b9c2a669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                   0\n",
       "item_id              0\n",
       "dept_id              0\n",
       "cat_id               0\n",
       "store_id             0\n",
       "state_id             0\n",
       "d                    0\n",
       "sales                0\n",
       "date                 0\n",
       "wm_yr_wk             0\n",
       "weekday              0\n",
       "wday                 0\n",
       "month                0\n",
       "year                 0\n",
       "event_name_1    542032\n",
       "event_type_1    542032\n",
       "event_name_2    588848\n",
       "event_type_2    588848\n",
       "snap_CA              0\n",
       "snap_TX              0\n",
       "snap_WI              0\n",
       "day                  0\n",
       "quarter              0\n",
       "week_of_year         0\n",
       "is_weekend           0\n",
       "has_event            0\n",
       "snap_any             0\n",
       "sell_price      120922\n",
       "revenue         120922\n",
       "dtype: int64"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "c119b1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_13332\\1108722504.py:387: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  unique_series = df.groupby(['store_id', 'item_id']).ngroups\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_13332\\1108722504.py:415: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  sample_series = df.groupby(['store_id', 'item_id']).head(1000)  # Sample for efficiency\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_13332\\1108722504.py:416: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  for (store, item), group in sample_series.groupby(['store_id', 'item_id']):\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_13332\\1108722504.py:425: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  item_dept = df.groupby('item_id')['dept_id'].nunique()\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_13332\\1108722504.py:428: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  dept_cat = df.groupby('dept_id')['cat_id'].nunique()\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_13332\\1108722504.py:434: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  series_sales_counts = df.groupby(['store_id', 'item_id'])['sales'].count()\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_13332\\1108722504.py:437: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  series_zero_pcts = df.groupby(['store_id', 'item_id'])['sales'].apply(lambda x: (x == 0).mean())\n"
     ]
    }
   ],
   "source": [
    " quality_metrics = processor.validate_data_quality(master_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "dc2bf067",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.save_quality_report(quality_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "5742b245",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_13332\\1108722504.py:516: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  'unique_time_series': int(df.groupby(['store_id', 'item_id']).ngroups),\n"
     ]
    }
   ],
   "source": [
    "if config.save_interim:\n",
    "   processor.save_processed_data(master_df, \"master\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "9b6c366f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.read_parquet(r\"C:\\Users\\Administrator\\OneDrive\\Desktop\\SalesAI\\backend\\data\\processed\\m5\\master.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "da29b0d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                   0\n",
       "item_id              0\n",
       "dept_id              0\n",
       "cat_id               0\n",
       "store_id             0\n",
       "state_id             0\n",
       "d                    0\n",
       "sales                0\n",
       "date                 0\n",
       "wm_yr_wk             0\n",
       "weekday              0\n",
       "wday                 0\n",
       "month                0\n",
       "year                 0\n",
       "event_name_1    542032\n",
       "event_type_1    542032\n",
       "event_name_2    588848\n",
       "event_type_2    588848\n",
       "snap_CA              0\n",
       "snap_TX              0\n",
       "snap_WI              0\n",
       "day                  0\n",
       "quarter              0\n",
       "week_of_year         0\n",
       "is_weekend           0\n",
       "has_event            0\n",
       "snap_any             0\n",
       "sell_price      120922\n",
       "revenue         120922\n",
       "dtype: int64"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e7a8ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "salesenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
