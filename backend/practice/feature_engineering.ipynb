{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "cf3bdab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import holidays\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import logging\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import gc\n",
    "from src.logger import logger\n",
    "from src.utils.config_loader import load_features_config, config_loader\n",
    "from src.exception import CustomException\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a251426a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class M5WalmartFeaturePipeline:\n",
    "    \"\"\"M5 Walmart-specific feature engineering pipeline with configuration support.\"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame, config_path: Optional[str] = None, \n",
    "                 memory_efficient: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize M5 feature pipeline.\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame with M5 data\n",
    "            config_path: Path to features configuration file\n",
    "            memory_efficient: Enable memory optimization for large datasets\n",
    "        \"\"\"\n",
    "        self.df = df.copy()\n",
    "        self.memory_efficient = memory_efficient\n",
    "        \n",
    "        # Load configuration\n",
    "        try:\n",
    "            if config_path:\n",
    "               self.config = load_features_config(config_path)\n",
    "               logger.info(f\"Loaded configuration: {config_path}\")\n",
    "            else:\n",
    "                self.config = self._get_default_config()\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to load configuration: {e}. Using defaults.\")\n",
    "            self.config = self._get_fallback_config()\n",
    "        \n",
    "        # Extract configuration values\n",
    "        dataset_config = self.config.get('dataset', {})\n",
    "        self.target_col = dataset_config.get('target_column', 'sales')\n",
    "        self.group_cols = dataset_config.get('group_columns', ['store_id', 'item_id'])\n",
    "        self.hierarchy_cols = dataset_config.get('hierarchy_columns', \n",
    "                                               ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])\n",
    "        self.date_col = dataset_config.get('date_column', 'date')\n",
    "        \n",
    "        # Processing configuration\n",
    "        processing_config = self.config.get('processing', {})\n",
    "        self.chunk_size = processing_config.get('chunk_size', 100000)\n",
    "        self.n_jobs = processing_config.get('n_jobs', -1)\n",
    "        \n",
    "        # Validate input data\n",
    "        self._validate_data()\n",
    "        \n",
    "        logger.info(f\"Initialized M5WalmartFeaturePipeline with {len(df)} rows\")\n",
    "        logger.info(f\"Target: {self.target_col}, Memory efficient: {memory_efficient}\")\n",
    "        \n",
    "\n",
    "    def _get_fallback_config(self) -> Dict:\n",
    "        \"\"\"Get fallback configuration if loading fails.\"\"\"\n",
    "        return {\n",
    "            'dataset': {\n",
    "                'target_column': 'date'\n",
    "            },\n",
    "            'date_features': {'enabled': True},\n",
    "            'lag_features': {\n",
    "                'enabled': True,\n",
    "                'sales_lags': {'windows': [1, 7, 14, 28]}\n",
    "            },\n",
    "            'rolling_features': {\n",
    "                'enabled': True,\n",
    "                'windows': [7, 14, 28],\n",
    "                'statistics': [{'name': 'mean'}, {'name': 'std'}]\n",
    "            },\n",
    "            'walmart_features': {\n",
    "                'snap_features': {'enabled': True},\n",
    "                'price_features': {'enabled': True},\n",
    "                'event_featu': 'sales',\n",
    "                'group_columns': ['store_id', 'item_id'],\n",
    "                'date_column': 'date',\n",
    "                'event_features': {'enabled': True}\n",
    "            }\n",
    "        }\n",
    "    \n",
    "\n",
    "    def _get_default_config(self) -> Dict:\n",
    "        \"\"\"Get default configuration optimized for M5 dataset.\"\"\"\n",
    "        return {\n",
    "            'date_features': {\n",
    "                'enabled': True,\n",
    "                'cols': ['year', 'month', 'day', 'dayofweek', 'quarter', \n",
    "                        'weekofyear', 'is_weekend', 'is_month_start', 'is_month_end']\n",
    "            },\n",
    "            'lag_features': {\n",
    "                'enabled': True,\n",
    "                'sales_lags': [1, 2, 3, 7, 14, 21, 28],  # Critical for M5\n",
    "                'price_lags': [1, 7, 14, 28],\n",
    "                'revenue_lags': [7, 14, 28]\n",
    "            },\n",
    "            'rolling_features': {\n",
    "                'enabled': True,\n",
    "                'windows': [7, 14, 28, 56],\n",
    "                'functions': ['mean', 'std', 'min', 'max']\n",
    "            },\n",
    "            'walmart_features': {\n",
    "                'enabled': True,\n",
    "                'snap_features': True,\n",
    "                'price_features': True,\n",
    "                'event_features': True,\n",
    "                'hierarchical_features': True\n",
    "            },\n",
    "            'advanced_features': {\n",
    "                'enabled': True,\n",
    "                'ewm_spans': [7, 14, 28],\n",
    "                'trend_features': True,\n",
    "                'ratio_features': True\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def _validate_data(self):\n",
    "\n",
    "        \"\"\"Validate that DataFrame has required M5 columns.\"\"\"\n",
    "\n",
    "        required_cols = [self.date_col] + self.group_cols + [self.target_col]\n",
    "        missing_cols = [col for col in required_cols if col not in self.df.columns]\n",
    "        \n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing required M5 columns: {missing_cols}\")\n",
    "        \n",
    "        # Check for M5-specific columns\n",
    "        m5_cols = ['dept_id', 'cat_id', 'state_id', 'sell_price']\n",
    "        available_m5_cols = [col for col in m5_cols if col in self.df.columns]\n",
    "        logger.info(f\"Available M5 columns: {available_m5_cols}\")\n",
    "        \n",
    "        # Ensure date is datetime\n",
    "        if not pd.api.types.is_datetime64_any_dtype(self.df[self.date_col]):\n",
    "            self.df[self.date_col] = pd.to_datetime(self.df[self.date_col])\n",
    "        \n",
    "        # Sort data for efficient processing\n",
    "        self.df = self.df.sort_values(self.group_cols + [self.date_col]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    def add_date_features(self) -> pd.DataFrame:\n",
    "\n",
    "        \"\"\"Add date features based on configuration.\"\"\"\n",
    "\n",
    "        date_config = self.config.get('date_features', {})\n",
    "        if not date_config.get('enabled', True):\n",
    "            logger.info(\"⏸️ Date features disabled in configuration\")\n",
    "            return self.df\n",
    "\n",
    "        logger.info(\"🗓️ Adding M5-specific date features...\")\n",
    "\n",
    "        # Ensure datetime format and handle invalid/missing values\n",
    "        self.df[self.date_col] = pd.to_datetime(self.df[self.date_col], errors='coerce')\n",
    "        print(self.date_col)\n",
    "\n",
    "        if self.df[self.date_col].isna().any():\n",
    "            logger.warning(\"⚠️ Date column contains missing values. Filling using forward-fill.\")\n",
    "            self.df[self.date_col] = self.df[self.date_col].fillna(method='ffill')\n",
    "\n",
    "        # Basic date features from config\n",
    "        basic_features = date_config.get('basic_features', [])\n",
    "        for feature in basic_features:\n",
    "            logger.info(f\"🔹 Adding basic date feature: {feature}\")\n",
    "\n",
    "            if feature == 'year':\n",
    "                self.df['year'] = self.df[self.date_col].dt.year.astype('Int16')\n",
    "            elif feature == 'month':\n",
    "                self.df['month'] = self.df[self.date_col].dt.month.astype('Int8')\n",
    "            elif feature == 'day':\n",
    "                self.df['day'] = self.df[self.date_col].dt.day.astype('Int8')\n",
    "            elif feature == 'dayofweek':\n",
    "                self.df['dayofweek'] = self.df[self.date_col].dt.dayofweek.astype('Int8')\n",
    "            elif feature == 'quarter':\n",
    "                self.df['quarter'] = self.df[self.date_col].dt.quarter.astype('Int8')\n",
    "            elif feature == 'weekofyear':\n",
    "                self.df['weekofyear'] = self.df[self.date_col].dt.isocalendar().week.astype('Int8')\n",
    "\n",
    "        # Derived features from config\n",
    "        derived_features = date_config.get('derived_features', [])\n",
    "        for feature in derived_features:\n",
    "            if feature == 'is_weekend' and 'dayofweek' in self.df.columns:\n",
    "                self.df['is_weekend'] = (self.df['dayofweek'] >= 5).astype('Int8')\n",
    "            elif feature == 'is_month_start' and 'day' in self.df.columns:\n",
    "                self.df['is_month_start'] = (self.df['day'] <= 5).astype('Int8')\n",
    "            elif feature == 'is_month_end' and 'day' in self.df.columns:\n",
    "                self.df['is_month_end'] = (self.df['day'] >= 25).astype('Int8')\n",
    "            elif feature == 'week_of_month' and 'day' in self.df.columns:\n",
    "                self.df['week_of_month'] = ((self.df['day'] - 1) // 7 + 1).astype('Int8')\n",
    "            elif feature == 'is_payday_week' and 'day' in self.df.columns:\n",
    "                self.df['is_payday_week'] = ((self.df['day'] <= 7) |\n",
    "                                            ((self.df['day'] >= 14) & (self.df['day'] <= 21))).astype('Int8')\n",
    "                \n",
    "\n",
    "        # Holiday features from config\n",
    "        holiday_config = date_config.get('holiday_features', {})\n",
    "        if holiday_config.get('enabled', True):\n",
    "            try:\n",
    "                import holidays\n",
    "                country = holiday_config.get('country', 'US')\n",
    "                years = holiday_config.get('years', list(range(2011, 2017)))\n",
    "                country_holidays = holidays.country_holidays(country, years=years)\n",
    "\n",
    "                self.df['is_holiday'] = self.df[self.date_col].dt.date.isin(country_holidays).astype('Int8')\n",
    "\n",
    "                # Custom holidays from config\n",
    "                custom_holidays = holiday_config.get('custom_holidays', [])\n",
    "                for holiday in custom_holidays:\n",
    "                    if holiday == 'christmas_period':\n",
    "                        self.df['christmas_period'] = ((self.df['month'] == 12) &\n",
    "                                                    (self.df['day'] >= 15)).astype('Int8')\n",
    "                    elif holiday == 'thanksgiving_week':\n",
    "                        self.df['thanksgiving_week'] = ((self.df['month'] == 11) &\n",
    "                                                        (self.df['day'] >= 22)).astype('Int8')\n",
    "                    elif holiday == 'back_to_school':\n",
    "                        self.df['back_to_school'] = ((self.df['month'] == 8) |\n",
    "                                                    ((self.df['month'] == 9) & (self.df['day'] <= 15))).astype('Int8')\n",
    "\n",
    "                logger.info(\"🎉 Holiday features added successfully\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"⚠️ Failed to add holiday features: {e}\")\n",
    "                self.df['is_holiday'] = 0\n",
    "\n",
    "        # Cyclical encoding\n",
    "        cyclical_config = date_config.get('cyclical_encoding', {})\n",
    "        if cyclical_config.get('enabled', True):\n",
    "            for feature_name, feature_config in cyclical_config.get('features', {}).items():\n",
    "                if feature_name in self.df.columns:\n",
    "                    period = feature_config['period']\n",
    "                    prefix = feature_config['prefix']\n",
    "                    self.df[f'{prefix}_sin'] = np.sin(2 * np.pi * self.df[feature_name] / period).astype('float32')\n",
    "                    self.df[f'{prefix}_cos'] = np.cos(2 * np.pi * self.df[feature_name] / period).astype('float32')\n",
    "\n",
    "            logger.info(\"🌀 Cyclical encoding applied successfully\")\n",
    "\n",
    "        logger.info(\"✅ Date features added successfully\")\n",
    "        logger.info(f\"Columns after adding date features: {list(self.df.columns)}\")\n",
    "\n",
    "        return self.df\n",
    "    \n",
    "    \n",
    "    def add_snap_features(self) -> pd.DataFrame:\n",
    "\n",
    "        \"\"\"Add SNAP (food assistance) benefit features - critical for Walmart.\"\"\"\n",
    "\n",
    "        logger.info(\"🛠️ Adding SNAP benefit features...\")\n",
    "\n",
    "        snap_cols = ['snap_CA', 'snap_TX', 'snap_WI']\n",
    "        available_snap = [col for col in snap_cols if col in self.df.columns]\n",
    "\n",
    "        if not available_snap:\n",
    "            logger.warning(\"⚠️ No SNAP columns found - skipping SNAP features\")\n",
    "            return self.df\n",
    "\n",
    "        # Initialize snap_any to 0\n",
    "        self.df['snap_any'] = 0\n",
    "\n",
    "        for col in available_snap:\n",
    "            self.df['snap_any'] = (self.df['snap_any'] | self.df[col]).astype('int8')\n",
    "\n",
    "        # SNAP interaction with grocery categories (if available)\n",
    "        if 'cat_id' in self.df.columns:\n",
    "            # Food categories typically benefit most from SNAP\n",
    "            food_categories = ['FOODS_1', 'FOODS_2', 'FOODS_3']  # Common M5 food categories\n",
    "            for cat in food_categories:\n",
    "                col_name = f'snap_{cat.lower()}_interaction'\n",
    "                if (self.df['cat_id'] == cat).any():\n",
    "                    self.df[col_name] = (self.df['snap_any'] * (self.df['cat_id'] == cat).astype('int8')).astype('int8')\n",
    "                    logger.info(f\"✅ Added SNAP interaction feature: {col_name}\")\n",
    "\n",
    "        # SNAP benefit timing features (benefits mostly distributed early in month)\n",
    "        self.df['snap_benefit_period'] = ((self.df['snap_any'] == 1) & (self.df['day'] <= 10)).astype('int8')\n",
    "        logger.info(\"✅ Added 'snap_benefit_period' feature\")\n",
    "\n",
    "        logger.info(f\"🎉 SNAP features added using columns: {available_snap}\")\n",
    "        return self.df\n",
    "    \n",
    "    \n",
    "    def add_price_features(self) -> pd.DataFrame:\n",
    "\n",
    "        logger.info(\"🛠️ Adding price features...\")\n",
    "\n",
    "        if 'sell_price' not in self.df.columns:\n",
    "            logger.warning(\"⚠️ No sell_price column found - skipping price features\")\n",
    "            return self.df\n",
    "\n",
    "        # Fill missing prices with forward/backward fill within item-store groups\n",
    "        self.df['sell_price'] = (\n",
    "            self.df.groupby(['store_id', 'item_id'], observed=False)['sell_price']\n",
    "            .transform(lambda x: x.ffill().bfill())\n",
    "        )\n",
    "        logger.info(\"✅ Filled missing 'sell_price' with group forward/backward fill\")\n",
    "\n",
    "        # Extract windows and ensure they are ints\n",
    "        price_lags = self.config.get('price_features', {}).get('change_features', {}).get('windows', [1, 7, 14, 28])\n",
    "        price_lags = [int(w) for w in price_lags]  # <<< make sure they are ints\n",
    "\n",
    "        increase_threshold = self.config.get('price_features', {}).get('change_features', {}).get('thresholds', {}).get('increase', 0.05)\n",
    "        decrease_threshold = self.config.get('price_features', {}).get('change_features', {}).get('thresholds', {}).get('decrease', -0.05)\n",
    "\n",
    "        for lag in price_lags:\n",
    "            lag_col = f'price_lag_{lag}'\n",
    "            self.df[lag_col] = (\n",
    "                self.df.groupby(['store_id', 'item_id'], observed=False)['sell_price']\n",
    "                .shift(lag)\n",
    "                .astype('float32')\n",
    "            )\n",
    "            logger.info(f\"✅ Added lag feature: {lag_col}\")\n",
    "\n",
    "            # Price change indicators\n",
    "            change_col = f'price_change_{lag}d'\n",
    "            self.df[change_col] = (\n",
    "                (self.df['sell_price'] - self.df[lag_col]) /\n",
    "                (self.df[lag_col] + 0.01)  # avoid division by zero\n",
    "            ).astype('float32')\n",
    "            logger.info(f\"✅ Added price change feature: {change_col}\")\n",
    "\n",
    "            # Price increase/decrease flags\n",
    "            self.df[f'price_increased_{lag}d'] = (self.df[change_col] > increase_threshold).astype('int8')\n",
    "            self.df[f'price_decreased_{lag}d'] = (self.df[change_col] < decrease_threshold).astype('int8')\n",
    "            logger.info(f\"✅ Added price increase/decrease flags for lag {lag}d\")\n",
    "\n",
    "        # Price volatility (rolling std dev)\n",
    "        volatility_windows = self.config.get('price_features', {}).get('volatility_features', {}).get('windows', [7, 28])\n",
    "        volatility_windows = [int(w) for w in volatility_windows]  # <<< cast here too\n",
    "\n",
    "        for window in volatility_windows:\n",
    "            vol_col = f'price_volatility_{window}d'\n",
    "            self.df[vol_col] = (\n",
    "                self.df.groupby(['store_id', 'item_id'], observed=False)['sell_price']\n",
    "                .rolling(window, min_periods=1)\n",
    "                .std()\n",
    "                .reset_index(level=[0, 1], drop=True)\n",
    "                .astype('float32')\n",
    "            )\n",
    "            logger.info(f\"✅ Added price volatility feature: {vol_col}\")\n",
    "\n",
    "        logger.info(\"🎉Price features added successfully\")\n",
    "        return self.df\n",
    "    \n",
    "    \n",
    "    def add_event_features(self) -> pd.DataFrame:\n",
    "        \"\"\"Add event-related features from M5 calendar.\"\"\"\n",
    "        logger.info(\"📅 Adding event features...\")\n",
    "\n",
    "        event_cols = ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
    "        available_events = [col for col in event_cols if col in self.df.columns]\n",
    "\n",
    "        if not available_events:\n",
    "            logger.warning(\"⚠️ No event columns found - skipping event features\")\n",
    "            return self.df\n",
    "\n",
    "        # Has any event flag\n",
    "        self.df['has_event'] = 0\n",
    "        for col in ['event_name_1', 'event_name_2']:\n",
    "            if col in self.df.columns:\n",
    "                self.df['has_event'] = (self.df['has_event'] | self.df[col].notna()).astype('int8')\n",
    "        logger.info(\"✅ Created 'has_event' flag\")\n",
    "\n",
    "        # Event type flags\n",
    "        for col in ['event_type_1', 'event_type_2']:\n",
    "            if col in self.df.columns:\n",
    "                event_types = self.df[col].dropna().unique()\n",
    "                for event_type in event_types:\n",
    "                    if pd.notna(event_type):\n",
    "                        flag_col = f'event_{event_type.lower().replace(\" \", \"_\")}'\n",
    "                        self.df[flag_col] = (self.df[col] == event_type).astype('int8')\n",
    "                logger.info(f\"✅ Created event type flags for column: {col}\")\n",
    "\n",
    "        # Days since/until major events\n",
    "        major_events = ['Christmas', 'Thanksgiving', 'Easter', 'SuperBowl']\n",
    "        for event in major_events:\n",
    "            event_col = f'event_{event.lower()}'\n",
    "            if event_col in self.df.columns:\n",
    "                event_dates = self.df.loc[self.df[event_col] == 1, 'date'].dropna().unique()\n",
    "                if len(event_dates) > 0:\n",
    "                    # Calculate days difference to each event date\n",
    "                    diffs = np.array([(self.df['date'] - pd.Timestamp(ed)).dt.days for ed in event_dates])\n",
    "                    # Shape: (num_events, num_rows)\n",
    "\n",
    "                    # Find closest event (min absolute difference)\n",
    "                    abs_diffs = np.abs(diffs)\n",
    "                    min_indices = np.argmin(abs_diffs, axis=0)\n",
    "                    min_abs_diff = abs_diffs[min_indices, range(len(self.df))]\n",
    "                    min_sign = np.sign(diffs[min_indices, range(len(self.df))])\n",
    "\n",
    "                    # Cap difference at 30 days and apply sign\n",
    "                    days_to_event = min_abs_diff.clip(max=30) * min_sign\n",
    "\n",
    "                    # Handle NaNs before casting to integer\n",
    "                    days_to_event_filled = pd.Series(days_to_event).fillna(0).astype('int16')\n",
    "                    self.df[f'days_to_{event.lower()}'] = days_to_event_filled.values\n",
    "                logger.info(f\"✅ Added days to/from event feature: days_to_{event.lower()}\")\n",
    "\n",
    "        logger.info(f\"🎉 Event features added using columns: {available_events}\")\n",
    "        return self.df\n",
    "\n",
    "    \n",
    "    def add_lag_features(self) -> pd.DataFrame:\n",
    "        \"\"\"Add lag features based on configuration.\"\"\"\n",
    "\n",
    "        lag_config = self.config.get('lag_features', {})\n",
    "        if not lag_config.get('enabled', True):\n",
    "            logger.info(\"⏸️ Lag features disabled in configuration\")\n",
    "            return self.df\n",
    "\n",
    "        logger.info(\"🔄 Adding lag features...\")\n",
    "\n",
    "        # Handle sales_lags which can be a list or a dict\n",
    "        sales_config = lag_config.get('sales_lags', [])\n",
    "        if isinstance(sales_config, list):\n",
    "            sales_lags = sales_config\n",
    "            sales_dtype = 'int16'  # default dtype if only list provided\n",
    "\n",
    "        else:\n",
    "            sales_lags = sales_config.get('windows', [])\n",
    "            sales_dtype = sales_config.get('dtype', 'int16')\n",
    "\n",
    "            \n",
    "        for lag in sales_lags:\n",
    "            col_name = f'{self.target_col}_lag_{lag}'\n",
    "            lagged = self.df.groupby(self.group_cols, observed=False)[self.target_col].shift(lag)\n",
    "\n",
    "            if 'int' in sales_dtype:\n",
    "                # Fill NaNs with -1 (or 0 if preferred) before converting to int\n",
    "                lagged = lagged.fillna(-1).astype(sales_dtype)\n",
    "            else:\n",
    "                lagged = lagged.astype(sales_dtype)\n",
    "\n",
    "            self.df[col_name] = lagged\n",
    "            logger.debug(f\"🛠️ Created lag feature: {col_name}\")\n",
    "        logger.info(f\"✅ Added {len(sales_lags)} sales lag features\")\n",
    "\n",
    "\n",
    "        # Price lags\n",
    "        price_config = lag_config.get('price_lags', [])\n",
    "        if isinstance(price_config, list):\n",
    "            price_lags = price_config\n",
    "            price_dtype = 'float32'\n",
    "        else:\n",
    "            price_lags = price_config.get('windows', [1, 7, 14, 28])\n",
    "            price_dtype = price_config.get('dtype', 'float32')\n",
    "\n",
    "        if 'sell_price' in self.df.columns:\n",
    "            for lag in price_lags:\n",
    "                col_name = f'price_lag_{lag}'\n",
    "                lagged = self.df.groupby(self.group_cols, observed=False)['sell_price'].shift(lag)\n",
    "                self.df[col_name] = lagged.astype(price_dtype)\n",
    "                logger.debug(f\"🛠️ Created price lag feature: {col_name}\")\n",
    "        \n",
    "        logger.info(f\"✅ Added {len(price_lags)} sales lag features\")\n",
    "\n",
    "        # Revenue lags\n",
    "        revenue_config = lag_config.get('revenue_lags', [])\n",
    "        if isinstance(revenue_config, list):\n",
    "            revenue_lags = revenue_config\n",
    "            revenue_dtype = 'float32'\n",
    "        else:\n",
    "            revenue_lags = revenue_config.get('windows', [])\n",
    "            revenue_dtype = revenue_config.get('dtype', 'float32')\n",
    "\n",
    "        if 'sell_price' in self.df.columns:\n",
    "            if 'revenue' not in self.df.columns:\n",
    "                self.df['revenue'] = self.df[self.target_col] * self.df['sell_price']\n",
    "\n",
    "            for lag in revenue_lags:\n",
    "                col_name = f'revenue_lag_{lag}'\n",
    "                lagged = self.df.groupby(self.group_cols,observed=False)['revenue'].shift(lag)\n",
    "                self.df[col_name] = lagged.astype(revenue_dtype)\n",
    "                logger.debug(f\"🛠️ Created revenue lag feature: {col_name}\")\n",
    "\n",
    "        logger.info(f\"✅ Added {len(revenue_lags)} sales lag features\")\n",
    "        return self.df\n",
    "\n",
    "\n",
    "    def add_rolling_features(self) -> pd.DataFrame:\n",
    "\n",
    "        \"\"\"Add rolling statistical features based on configuration.\"\"\"\n",
    "\n",
    "        rolling_config = self.config.get('rolling_features', {})\n",
    "        if not rolling_config.get('enabled', True):\n",
    "            logger.info(\"ℹ️ Rolling features disabled in configuration\")\n",
    "            return self.df\n",
    "\n",
    "        logger.info(\"🌀 Adding rolling features...\")\n",
    "\n",
    "        windows = rolling_config.get('windows', [7, 14, 28])\n",
    "        windows = [int(w) for w in windows]  # Ensure they are ints\n",
    "\n",
    "        min_periods = rolling_config.get('min_periods', 1)\n",
    "        columns = rolling_config.get('columns', [self.target_col])\n",
    "        statistics = rolling_config.get('statistics', [{'name': 'mean'}, {'name': 'std'}])\n",
    "\n",
    "        for col in columns:\n",
    "            if col not in self.df.columns:\n",
    "                logger.warning(f\"⚠️ Column '{col}' not found in dataframe, skipping rolling features for this column.\")\n",
    "                continue\n",
    "\n",
    "            for window in windows:\n",
    "                logger.debug(f\"🔁 Using rolling window: {window} (type: {type(window)}) for column: {col}\")\n",
    "\n",
    "                for stat_config in statistics:\n",
    "                    stat_name = stat_config['name']\n",
    "                    stat_dtype = stat_config.get('dtype', 'float32')\n",
    "                    fill_na = stat_config.get('fill_na', None)\n",
    "\n",
    "                    col_name = f'{col}_roll_{window}_{stat_name}'\n",
    "\n",
    "                    try:\n",
    "                        grouped = self.df.groupby(self.group_cols, observed=False)[col]\n",
    "                        rolling_obj = grouped.rolling(window, min_periods=min_periods)\n",
    "\n",
    "                        if stat_name == 'mean':\n",
    "                            result = rolling_obj.mean().reset_index(level=list(range(len(self.group_cols))), drop=True)\n",
    "                        elif stat_name == 'std':\n",
    "                            result = rolling_obj.std().reset_index(level=list(range(len(self.group_cols))), drop=True)\n",
    "                            if fill_na is not None:\n",
    "                                result = result.fillna(fill_na)\n",
    "                        elif stat_name == 'min':\n",
    "                            result = rolling_obj.min().reset_index(level=list(range(len(self.group_cols))), drop=True)\n",
    "                        elif stat_name == 'max':\n",
    "                            result = rolling_obj.max().reset_index(level=list(range(len(self.group_cols))), drop=True)\n",
    "                        else:\n",
    "                            logger.warning(f\"⚠️ Unknown statistic: {stat_name}, skipping...\")\n",
    "                            continue\n",
    "\n",
    "                        self.df[col_name] = result.astype(stat_dtype)\n",
    "                        logger.debug(f\"✅ Created rolling feature: {col_name}\")\n",
    "\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"❌ Failed to create rolling feature {col_name}: {e}\")\n",
    "\n",
    "        logger.info(f\"✅ Finished adding rolling features for {len(windows)} windows and {len(columns)} columns\")\n",
    "        return self.df    \n",
    "\n",
    "\n",
    "    def add_advanced_features(self) -> pd.DataFrame:\n",
    "        \"\"\"Add advanced M5-specific features like EWM, trend, ratio, and zero-sale patterns.\"\"\"\n",
    "        logger.info(\"🚀 Adding M5 advanced features...\")\n",
    "        \n",
    "        adv_cfg = self.config.get('advanced_features', {})\n",
    "        if not adv_cfg.get('enabled', True):\n",
    "            logger.info(\"⛔ Advanced features disabled in config\")\n",
    "            return self.df\n",
    "\n",
    "        # --- Exponentially Weighted Moving Average (EWM) ---\n",
    "        ewm_spans = adv_cfg.get('ewm_spans', [7, 14, 28])\n",
    "        for span in ewm_spans:\n",
    "            col_name = f'sales_ewm_{span}'\n",
    "            try:\n",
    "                self.df[col_name] = (\n",
    "                    self.df.groupby(['store_id', 'item_id'], observed=False)['sales']\n",
    "                        .transform(lambda x: x.ewm(span=span, adjust=False).mean())\n",
    "                        .astype('float32')\n",
    "                )\n",
    "                logger.debug(f\"📈 Created EWM feature: {col_name}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"❌ Failed to create {col_name}: {e}\")\n",
    "\n",
    "        # --- Trend Features ---\n",
    "        if adv_cfg.get('trend_features', True):\n",
    "            logger.info(\"📊 Adding trend features...\")\n",
    "            try:\n",
    "                self.df['time_index'] = (\n",
    "                    self.df.groupby(['store_id', 'item_id'], observed=False)\n",
    "                        .cumcount()\n",
    "                        .fillna(0)\n",
    "                        .astype('int16')\n",
    "                )\n",
    "\n",
    "                self.df['sales_velocity'] = (\n",
    "                    self.df.groupby(['store_id', 'item_id'], observed=False)['sales']\n",
    "                        .diff()\n",
    "                        .fillna(0)\n",
    "                        .astype('float32')\n",
    "                )\n",
    "\n",
    "                self.df['sales_acceleration'] = (\n",
    "                    self.df.groupby(['store_id', 'item_id'], observed=False)['sales_velocity']\n",
    "                        .diff()\n",
    "                        .fillna(0)\n",
    "                        .astype('float32')\n",
    "                )\n",
    "            except Exception as e:\n",
    "                logger.error(f\"❌ Failed to compute trend features: {e}\")\n",
    "\n",
    "        # --- Ratio Features ---\n",
    "        if adv_cfg.get('ratio_features', True):\n",
    "            logger.info(\"🔢 Adding ratio features...\")\n",
    "            for window in [7, 28]:\n",
    "                avg_col = f'sales_roll_{window}_mean'\n",
    "                if avg_col in self.df.columns:\n",
    "                    ratio_col = f'sales_ratio_to_{window}d_avg'\n",
    "                    try:\n",
    "                        self.df[ratio_col] = (\n",
    "                            (self.df['sales'] / (self.df[avg_col] + 1))\n",
    "                            .fillna(0)\n",
    "                            .astype('float32')\n",
    "                        )\n",
    "                        logger.debug(f\"✅ Created {ratio_col}\")\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"⚠️ Could not create ratio feature {ratio_col}: {e}\")\n",
    "\n",
    "        # --- Item Lifecycle ---\n",
    "        try:\n",
    "            self.df['days_since_first_sale'] = (\n",
    "                self.df.groupby(['store_id', 'item_id'], observed=False)['date']\n",
    "                    .rank(method='min')\n",
    "                    .fillna(0)\n",
    "                    .astype('int16')\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"⚠️ Could not compute 'days_since_first_sale': {e}\")\n",
    "\n",
    "        # --- Zero Sales Patterns ---\n",
    "        try:\n",
    "            self.df['zero_sales_flag'] = (self.df['sales'] == 0).astype('int8')\n",
    "\n",
    "            zero_groups = (\n",
    "                self.df.groupby(['store_id', 'item_id'],observed=False)['zero_sales_flag']\n",
    "                    .transform(lambda x: (x != x.shift()).cumsum())\n",
    "                    .fillna(0)\n",
    "                    .astype('int16')\n",
    "            )\n",
    "\n",
    "            self.df['consecutive_zero_days'] = (\n",
    "                self.df.groupby(['store_id', 'item_id', zero_groups], observed=False)\n",
    "                    .cumcount()\n",
    "                    .where(self.df['zero_sales_flag'] == 1, 0)\n",
    "                    .fillna(0)\n",
    "                    .astype('int8')\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"⚠️ Could not compute zero-sales features: {e}\")\n",
    "\n",
    "        logger.info(\"✅ Advanced features added successfully\")\n",
    "        return self.df\n",
    "\n",
    "\n",
    "\n",
    "    def handle_missing_values(self) -> pd.DataFrame:\n",
    "        \"\"\"Handle missing values based on column-specific strategies with fallback.\"\"\"\n",
    "        logger.info(\"Handling missing values...\")\n",
    "\n",
    "        # Define strategies\n",
    "        fill_strategies = {\n",
    "            'time_series_features': 'forward_fill',    # e.g., lag, rolling, ewm\n",
    "            'price_features': 'group_forward_fill',    # price-related columns\n",
    "            'flag_features': 'zero_fill',              # binary flags or booleans\n",
    "            'other_features': 'mean_fill'              # other numeric\n",
    "        }\n",
    "\n",
    "        # Validate group_cols\n",
    "        if not hasattr(self, 'group_cols') or not self.group_cols:\n",
    "            logger.warning(\"group_cols is undefined or empty; defaulting to no grouping\")\n",
    "            self.group_cols = []\n",
    "\n",
    "        numeric_cols = self.df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "        for col in numeric_cols:\n",
    "            if self.df[col].isnull().any():\n",
    "                missing_count = self.df[col].isnull().sum()\n",
    "                logger.debug(f\"Processing '{col}' with {missing_count} missing values\")\n",
    "\n",
    "                # Assign strategy based on column name patterns\n",
    "                if any(key in col for key in ['lag', 'roll', 'ewm']):\n",
    "                    strategy = fill_strategies['time_series_features']\n",
    "                elif 'price' in col:\n",
    "                    strategy = fill_strategies['price_features']\n",
    "                elif col.endswith('_flag') or 'is_' in col:\n",
    "                    strategy = fill_strategies['flag_features']\n",
    "                else:\n",
    "                    strategy = fill_strategies['other_features']\n",
    "\n",
    "                # Apply the strategy\n",
    "                if strategy == 'forward_fill':\n",
    "                    if self.group_cols:\n",
    "                        self.df[col] = self.df.groupby(self.group_cols,observed=False)[col].transform(\n",
    "                            lambda x: x.ffill().bfill()\n",
    "                        )\n",
    "                    else:\n",
    "                        self.df[col] = self.df[col].ffill().bfill()\n",
    "                elif strategy == 'group_forward_fill':\n",
    "                    if self.group_cols:\n",
    "                        self.df[col] = self.df.groupby(self.group_cols,observed=False)[col].transform(\n",
    "                            lambda x: x.ffill().bfill().fillna(x.mean())\n",
    "                        )\n",
    "                    else:\n",
    "                        self.df[col] = self.df[col].ffill().bfill().fillna(self.df[col].mean())\n",
    "                elif strategy == 'zero_fill':\n",
    "                    self.df[col] = self.df[col].fillna(0)\n",
    "                elif strategy == 'mean_fill':\n",
    "                    self.df[col] = self.df[col].fillna(self.df[col].mean())\n",
    "\n",
    "                # Fallback for remaining NaNs (e.g., fully missing groups)\n",
    "                if self.df[col].isnull().any():\n",
    "                    remaining_missing = self.df[col].isnull().sum()\n",
    "                    overall_mean = self.df[col].mean()\n",
    "                    if not np.isnan(overall_mean):\n",
    "                        self.df[col] = self.df[col].fillna(overall_mean)\n",
    "                        logger.debug(f\"Filled {remaining_missing} remaining NaNs in '{col}' with overall mean ({overall_mean})\")\n",
    "                    else:\n",
    "                        self.df[col] = self.df[col].fillna(0)\n",
    "                        logger.debug(f\"Filled {remaining_missing} remaining NaNs in '{col}' with 0 (no valid mean)\")\n",
    "\n",
    "                logger.debug(f\"Filled {missing_count} missing values in '{col}' using {strategy} with fallback\")\n",
    "\n",
    "        # Handle categorical columns\n",
    "        cat_cols = self.df.select_dtypes(include=['object', 'category']).columns\n",
    "        for col in cat_cols:\n",
    "            if self.df[col].isnull().any():\n",
    "                missing_count = self.df[col].isnull().sum()\n",
    "                mode_val = self.df[col].mode()[0] if not self.df[col].mode().empty else 'missing'\n",
    "                self.df[col] = self.df[col].fillna(mode_val)\n",
    "                logger.debug(f\"Filled {missing_count} missing values in categorical '{col}' with mode '{mode_val}'\")\n",
    "\n",
    "        # Final check for any remaining NaNs\n",
    "        if self.df.isnull().any().any():\n",
    "            logger.warning(f\"Remaining NaNs after handling: {self.df.isnull().sum()[self.df.isnull().sum() > 0]}\")\n",
    "        else:\n",
    "            logger.info(\"All missing values handled successfully\")\n",
    "\n",
    "        return self.df\n",
    "\n",
    "\n",
    "    def optimize_dtypes(self) -> pd.DataFrame:\n",
    "        \"\"\"🔧 Optimize data types for memory efficiency.\"\"\"\n",
    "        import gc\n",
    "\n",
    "        logger.info(\"🧪 Optimizing data types for memory efficiency...\")\n",
    "\n",
    "        # --- Integer downcasting ---\n",
    "        int_cols = self.df.select_dtypes(include=['int']).columns\n",
    "        for col in int_cols:\n",
    "            try:\n",
    "                if col.endswith('_flag') or 'is_' in col:\n",
    "                    self.df[col] = self.df[col].astype('int8')\n",
    "                    logger.debug(f\"✅ Downcasted '{col}' to int8\")\n",
    "                elif 'lag' in col and 'sales' in col:\n",
    "                    self.df[col] = pd.to_numeric(self.df[col], downcast='integer')\n",
    "                    logger.debug(f\"✅ Downcasted '{col}' using pd.to_numeric\")\n",
    "                elif col in ['year', 'month', 'day', 'dayofweek']:\n",
    "                    self.df[col] = self.df[col].astype('int16')\n",
    "                    logger.debug(f\"✅ Downcasted '{col}' to int16\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"⚠️ Could not downcast integer column '{col}': {e}\")\n",
    "            finally:\n",
    "                gc.collect()\n",
    "\n",
    "        # --- Float downcasting ---\n",
    "        float_cols = self.df.select_dtypes(include=['float']).columns\n",
    "        for col in float_cols:\n",
    "            try:\n",
    "                self.df[col] = self.df[col].astype('float32')\n",
    "                logger.debug(f\"✅ Downcasted float column '{col}' to float32\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"⚠️ Could not downcast float column '{col}': {e}\")\n",
    "            finally:\n",
    "                gc.collect()\n",
    "\n",
    "        # --- Categorical conversion ---\n",
    "        str_cols = self.df.select_dtypes(include=['object']).columns\n",
    "        for col in str_cols:\n",
    "            try:\n",
    "                unique_ratio = self.df[col].nunique(dropna=False) / len(self.df)\n",
    "                if unique_ratio < 0.5:\n",
    "                    self.df[col] = self.df[col].astype('category')\n",
    "                    logger.debug(f\"✅ Converted '{col}' to category\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"⚠️ Could not convert '{col}' to category: {e}\")\n",
    "            finally:\n",
    "                gc.collect()\n",
    "\n",
    "        logger.info(\"🎯 Data type optimization completed successfully\")\n",
    "        return self.df\n",
    "\n",
    "        \n",
    "    def run(self) -> pd.DataFrame:\n",
    "        \"\"\"Run the complete M5 feature engineering pipeline.\"\"\"\n",
    "        logger.info(\"🚀 Starting M5 Walmart Feature Pipeline...\")\n",
    "\n",
    "        start_time = datetime.now()\n",
    "\n",
    "        try:\n",
    "            # Step 1: M5-specific date features\n",
    "            self.df = self.add_date_features()\n",
    "\n",
    "            # Step 2: SNAP benefit features\n",
    "            if self.config.get('walmart_features', {}).get('snap_features', True):\n",
    "                self.df = self.add_snap_features()\n",
    "\n",
    "            # Step 3: Price features\n",
    "            if self.config.get('walmart_features', {}).get('price_features', True):\n",
    "                self.df = self.add_price_features()\n",
    "\n",
    "            # Step 4: Event features\n",
    "            if self.config.get('walmart_features', {}).get('event_features', True):\n",
    "                self.df = self.add_event_features()\n",
    "\n",
    "            # Step 5: Lag features\n",
    "            self.df = self.add_lag_features()\n",
    "\n",
    "            # Step 6: Rolling statistical features\n",
    "            self.df = self.add_rolling_features()\n",
    "\n",
    "            # Step 7: Hierarchical aggregation features\n",
    "            self.df = self.add_hierarchical_features()\n",
    "\n",
    "            # Step 8: Advanced features\n",
    "            self.df = self.add_advanced_features()\n",
    "\n",
    "            # Step 9: Handle missing values\n",
    "            self.df = self.handle_missing_values()\n",
    "\n",
    "            # Step 10: Optimize data types for memory efficiency\n",
    "            self.df = self.optimize_dtypes()\n",
    "\n",
    "            duration = datetime.now() - start_time\n",
    "            logger.info(f\"✅ M5 Feature Pipeline completed in {duration.total_seconds():.1f} seconds\")\n",
    "            logger.info(f\"📊 Final dataset shape: {self.df.shape}\")\n",
    "            logger.info(f\"🔧 Feature columns count: {len([col for col in self.df.columns if col not in ['date', 'store_id', 'item_id', 'sales']])}\")\n",
    "\n",
    "            return self.df\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ M5 Feature Pipeline failed: {e}\")\n",
    "            raise\n",
    "\n",
    "    def get_feature_importance(self, target_col: str = None, sample_size: int = 100000) -> pd.DataFrame:\n",
    "        \"\"\"Get feature importance using RandomForest on a sample.\"\"\"\n",
    "        logger.info(\"🔍 Calculating feature importance...\")\n",
    "\n",
    "        target_col = target_col or self.target_col\n",
    "\n",
    "        # Sample data for faster computation\n",
    "        if len(self.df) > sample_size:\n",
    "            sample_df = self.df.sample(n=sample_size, random_state=42)\n",
    "        else:\n",
    "            sample_df = self.df.copy()\n",
    "\n",
    "        # Prepare features, excluding columns that are identifiers or target\n",
    "        exclude_cols = {'date', 'store_id', 'item_id', target_col, 'd'}\n",
    "        feature_cols = [col for col in sample_df.columns if col not in exclude_cols]\n",
    "\n",
    "        X = sample_df[feature_cols].copy()\n",
    "        y = sample_df[target_col]\n",
    "\n",
    "        # Encode categorical features using LabelEncoder\n",
    "        for col in X.select_dtypes(include=['category', 'object']).columns:\n",
    "            le = LabelEncoder()\n",
    "            X[col] = le.fit_transform(X[col].astype(str))\n",
    "\n",
    "        # Drop rows with missing target\n",
    "        mask = y.notna()\n",
    "        X, y = X.loc[mask], y.loc[mask]\n",
    "\n",
    "        # Fit RandomForest\n",
    "        rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "        rf.fit(X, y)\n",
    "\n",
    "        # Create importance DataFrame\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': X.columns,\n",
    "            'importance': rf.feature_importances_\n",
    "        }).sort_values('importance', ascending=False).reset_index(drop=True)\n",
    "\n",
    "        logger.info(f\"✅ Feature importance calculated for {len(importance_df)} features\")\n",
    "        return importance_df\n",
    "\n",
    "\n",
    "    def save_features(self, output_path: str, include_metadata: bool = True):\n",
    "        \"\"\"Save engineered features to parquet file.\"\"\"\n",
    "        logger.info(f\"💾 Saving M5 features to {output_path}\")\n",
    "\n",
    "        # Ensure output directory exists\n",
    "        Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        if include_metadata:\n",
    "            # Add metadata info\n",
    "            metadata = {\n",
    "                'created_at': datetime.now().isoformat(),\n",
    "                'pipeline_version': 'M5_v1.0',\n",
    "                'original_shape': f\"{self.df.shape[0]}x{self.df.shape[1]}\",\n",
    "                'target_column': self.target_col,\n",
    "                'feature_count': len([col for col in self.df.columns if col not in ['date', 'store_id', 'item_id', 'sales']]),\n",
    "                'memory_efficient': self.memory_efficient\n",
    "            }\n",
    "\n",
    "            import pyarrow as pa\n",
    "            import pyarrow.parquet as pq\n",
    "\n",
    "            table = pa.Table.from_pandas(self.df)\n",
    "            # PyArrow expects metadata keys and values as bytes\n",
    "            table = table.replace_schema_metadata({k: str(v).encode() for k, v in metadata.items()})\n",
    "\n",
    "            pq.write_table(table, output_path, compression='snappy')\n",
    "        else:\n",
    "            self.df.to_parquet(output_path, compression='snappy', index=False)\n",
    "\n",
    "        file_size_mb = Path(output_path).stat().st_size / (1024 * 1024)\n",
    "        logger.info(f\"📦 Features saved: {output_path} ({file_size_mb:.1f} MB)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "1f56f42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('data/processed/m5/master.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "2b60222a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'd',\n",
       "       'sales', 'date', 'wm_yr_wk', 'weekday', 'wday', 'month', 'year',\n",
       "       'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2',\n",
       "       'snap_CA', 'snap_TX', 'snap_WI', 'day', 'quarter', 'week_of_year',\n",
       "       'is_weekend', 'has_event', 'snap_any', 'sell_price', 'revenue'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "d60d433f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                    0\n",
       "item_id               0\n",
       "dept_id               0\n",
       "cat_id                0\n",
       "store_id              0\n",
       "state_id              0\n",
       "d                     0\n",
       "sales                 0\n",
       "date                  0\n",
       "wm_yr_wk              0\n",
       "weekday               0\n",
       "wday                  0\n",
       "month                 0\n",
       "year                  0\n",
       "event_name_1    2717292\n",
       "event_type_1    2717292\n",
       "event_name_2    2951988\n",
       "event_type_2    2951988\n",
       "snap_CA               0\n",
       "snap_TX               0\n",
       "snap_WI               0\n",
       "day                   0\n",
       "quarter               0\n",
       "week_of_year          0\n",
       "is_weekend            0\n",
       "has_event             0\n",
       "snap_any              0\n",
       "sell_price       613258\n",
       "revenue          613258\n",
       "dtype: int64"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "29d2c725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-09-29 05:59:40,361 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32mAvailable M5 columns: ['dept_id', 'cat_id', 'state_id', 'sell_price']\u001b[0m\n",
      "[ 2025-09-29 05:59:41,973 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32mInitialized M5WalmartFeaturePipeline with 2958084 rows\u001b[0m\n",
      "[ 2025-09-29 05:59:41,989 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32mTarget: sales, Memory efficient: True\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "pipeline = M5WalmartFeaturePipeline(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "7bfbbc31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2958084, 29)"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "7f267717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-09-29 05:40:35,061 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m🗓️ Adding M5-specific date features...\u001b[0m\n",
      "date\n",
      "[ 2025-09-29 05:40:36,236 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m🎉 Holiday features added successfully\u001b[0m\n",
      "[ 2025-09-29 05:40:36,236 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m🌀 Cyclical encoding applied successfully\u001b[0m\n",
      "[ 2025-09-29 05:40:36,236 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m✅ Date features added successfully\u001b[0m\n",
      "[ 2025-09-29 05:40:36,242 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32mColumns after adding date features: ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'd', 'sales', 'date', 'wm_yr_wk', 'weekday', 'wday', 'month', 'year', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'day', 'quarter', 'week_of_year', 'is_weekend', 'has_event', 'snap_any', 'sell_price', 'revenue', 'is_holiday']\u001b[0m\n",
      "[ 2025-09-29 05:40:36,242 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m🛠️ Adding SNAP benefit features...\u001b[0m\n",
      "[ 2025-09-29 05:40:36,368 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m✅ Added 'snap_benefit_period' feature\u001b[0m\n",
      "[ 2025-09-29 05:40:36,368 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m🎉 SNAP features added using columns: ['snap_CA', 'snap_TX', 'snap_WI']\u001b[0m\n",
      "[ 2025-09-29 05:40:36,384 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m🛠️ Adding price features...\u001b[0m\n",
      "[ 2025-09-29 05:40:37,300 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m✅ Filled missing 'sell_price' with group forward/backward fill\u001b[0m\n",
      "[ 2025-09-29 05:40:37,423 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m✅ Added lag feature: price_lag_1\u001b[0m\n",
      "[ 2025-09-29 05:40:37,484 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m✅ Added price change feature: price_change_1d\u001b[0m\n",
      "[ 2025-09-29 05:40:37,507 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m✅ Added price increase/decrease flags for lag 1d\u001b[0m\n",
      "[ 2025-09-29 05:40:37,671 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m✅ Added lag feature: price_lag_7\u001b[0m\n",
      "[ 2025-09-29 05:40:37,709 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m✅ Added price change feature: price_change_7d\u001b[0m\n",
      "[ 2025-09-29 05:40:37,733 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m✅ Added price increase/decrease flags for lag 7d\u001b[0m\n",
      "[ 2025-09-29 05:40:37,920 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m✅ Added lag feature: price_lag_14\u001b[0m\n",
      "[ 2025-09-29 05:40:37,958 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m✅ Added price change feature: price_change_14d\u001b[0m\n",
      "[ 2025-09-29 05:40:37,986 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m✅ Added price increase/decrease flags for lag 14d\u001b[0m\n",
      "[ 2025-09-29 05:40:38,161 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m✅ Added lag feature: price_lag_28\u001b[0m\n",
      "[ 2025-09-29 05:40:38,204 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m✅ Added price change feature: price_change_28d\u001b[0m\n",
      "[ 2025-09-29 05:40:38,231 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m✅ Added price increase/decrease flags for lag 28d\u001b[0m\n",
      "[ 2025-09-29 05:40:39,846 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m✅ Added price volatility feature: price_volatility_7d\u001b[0m\n",
      "[ 2025-09-29 05:40:40,910 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m✅ Added price volatility feature: price_volatility_28d\u001b[0m\n",
      "[ 2025-09-29 05:40:40,919 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m🎉Price features added successfully\u001b[0m\n",
      "[ 2025-09-29 05:40:40,925 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m📅 Adding event features...\u001b[0m\n",
      "[ 2025-09-29 05:40:40,994 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m✅ Created 'has_event' flag\u001b[0m\n",
      "[ 2025-09-29 05:40:41,071 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m✅ Created event type flags for column: event_type_1\u001b[0m\n",
      "[ 2025-09-29 05:40:41,093 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m✅ Created event type flags for column: event_type_2\u001b[0m\n",
      "[ 2025-09-29 05:40:41,095 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m🎉 Event features added using columns: ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\u001b[0m\n",
      "[ 2025-09-29 05:40:41,099 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m🔄 Adding lag features...\u001b[0m\n",
      "[ 2025-09-29 05:40:42,212 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m✅ Added 7 sales lag features\u001b[0m\n",
      "[ 2025-09-29 05:40:42,743 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m✅ Added 4 sales lag features\u001b[0m\n",
      "[ 2025-09-29 05:40:43,209 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m✅ Added 3 sales lag features\u001b[0m\n",
      "[ 2025-09-29 05:40:43,214 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m🌀 Adding rolling features...\u001b[0m\n",
      "[ 2025-09-29 05:40:50,897 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m✅ Finished adding rolling features for 4 windows and 1 columns\u001b[0m\n",
      "[ 2025-09-29 05:40:50,905 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m🚀 Adding M5 advanced features...\u001b[0m\n",
      "[ 2025-09-29 05:40:52,562 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m📊 Adding trend features...\u001b[0m\n",
      "[ 2025-09-29 05:40:53,122 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m🔢 Adding ratio features...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_10360\\398017592.py:614: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  self.df.groupby(['store_id', 'item_id'])['zero_sales_flag']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-09-29 05:40:57,312 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m✅ Advanced features added successfully\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "df = pipeline.add_date_features()\n",
    "df = pipeline.add_snap_features()\n",
    "df = pipeline.add_price_features()\n",
    "df = pipeline.add_event_features()\n",
    "df = pipeline.add_lag_features()\n",
    "df = pipeline.add_rolling_features()\n",
    "df = pipeline.add_advanced_features()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "c9c4d8b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2958084, 82)"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "e9fa97f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sell_price: 1.3779527559055118 % missing\n",
      "revenue: 20.73159518120513 % missing\n",
      "price_lag_1: 1.4287626720539377 % missing\n",
      "price_change_1d: 1.4287626720539377 % missing\n",
      "price_lag_7: 1.7336221689444924 % missing\n",
      "price_change_7d: 1.7336221689444924 % missing\n",
      "price_lag_14: 2.0892915819834728 % missing\n",
      "price_change_14d: 2.0892915819834728 % missing\n",
      "price_lag_28: 2.8006304080614344 % missing\n",
      "price_change_28d: 2.8006304080614344 % missing\n",
      "price_volatility_7d: 1.4287626720539377 % missing\n",
      "price_volatility_28d: 1.4287626720539377 % missing\n",
      "revenue_lag_7: 21.087027954581412 % missing\n",
      "revenue_lag_14: 21.44246072795769 % missing\n",
      "revenue_lag_28: 22.153326274710253 % missing\n",
      "sales_roll_7_std: 0.05151983513652757 % missing\n",
      "sales_roll_14_std: 0.05151983513652757 % missing\n",
      "sales_roll_28_std: 0.05151983513652757 % missing\n",
      "sales_roll_56_std: 0.05151983513652757 % missing\n"
     ]
    }
   ],
   "source": [
    "for col in df.columns:\n",
    "    if df[col].isnull().any():\n",
    "        print(f\"{col}: {df[col].isnull().mean()*100} % missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "1a1d271b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          0.000000\n",
       "1          0.000000\n",
       "2          0.000000\n",
       "3          0.000000\n",
       "4          0.000000\n",
       "             ...   \n",
       "2958079    0.142857\n",
       "2958080    0.142857\n",
       "2958081    0.142857\n",
       "2958082    0.285714\n",
       "2958083    0.142857\n",
       "Name: sales_roll_7_mean, Length: 2958084, dtype: float32"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sales_roll_7_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "bbd45972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-09-29 05:59:42,471 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32mHandling missing values...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_10360\\1764989693.py:679: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  self.df[col] = self.df.groupby(self.group_cols)[col].transform(\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_10360\\1764989693.py:672: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  self.df[col] = self.df.groupby(self.group_cols)[col].transform(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-09-29 06:00:07,162 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32mAll missing values handled successfully\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "f_df = pipeline.handle_missing_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "2f95cf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in f_df.columns:\n",
    "    if f_df[col].isnull().any():\n",
    "        print(f\"{col}: {f_df[col].isnull().sum()} missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "7ae614f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store_id  item_id        \n",
      "CA_1      HOUSEHOLD_2_428    1941\n",
      "          HOUSEHOLD_2_418    1941\n",
      "          HOUSEHOLD_2_409    1941\n",
      "          HOUSEHOLD_2_410    1941\n",
      "          HOUSEHOLD_2_411    1941\n",
      "                             ... \n",
      "          HOBBIES_2_090         1\n",
      "          HOBBIES_2_089         1\n",
      "          HOBBIES_2_088         1\n",
      "          HOBBIES_2_087         1\n",
      "          HOUSEHOLD_1_202       1\n",
      "Name: count, Length: 1524, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "group_cols = ['store_id', 'item_id'] \n",
    "\n",
    "missing_rows = df[df['price_lag_1'].isnull()]\n",
    "print(missing_rows[group_cols].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "c53bf9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-09-29 06:02:21,256 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m🧪 Optimizing data types for memory efficiency...\u001b[0m\n",
      "[ 2025-09-29 06:02:34,013 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m🎯 Data type optimization completed successfully\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "f_df = pipeline.optimize_dtypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e340c7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-09-29 05:30:45,421 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32mOptimizing data types for memory efficiency...\u001b[0m\n",
      "[ 2025-09-29 05:30:46,634 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32mData type optimization completed\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "648932e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-09-29 04:22:16,006 ] src.utils.config_loader - \u001b[32mINFO\u001b[0m - \u001b[32mUsing cached config for C:\\Users\\Administrator\\OneDrive\\Desktop\\SalesAI\\backend\\configs\\ml_features.yaml\u001b[0m\n",
      "[ 2025-09-29 04:22:16,011 ] src.utils.config_loader - \u001b[32mINFO\u001b[0m - \u001b[32mFeatures configuration loaded and validated\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "config = load_features_config('ml_features.yaml')\n",
    "missing_config = config.get('missing_values', {})\n",
    "strategy_config = missing_config.get('strategy', {})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b571cf2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'strategy': {'time_series_features': 'forward_fill',\n",
       "  'price_features': 'group_forward_fill',\n",
       "  'flag_features': 'zero_fill',\n",
       "  'other_features': 'mean_fill'},\n",
       " 'validation': {'max_missing_pct': 50}}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cbd964d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "categorical_cols = df.select_dtypes(include=['category', 'object']).columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "8496df0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2958084, 29)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "79cad16c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sales', 'wm_yr_wk', 'weekday', 'wday', 'month', 'year', 'snap_CA',\n",
       "       'snap_TX', 'snap_WI', 'day', 'quarter', 'week_of_year', 'is_weekend',\n",
       "       'has_event', 'snap_any', 'sell_price', 'revenue'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "74dbbe4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'd',\n",
       "       'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bf80017c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[col].isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1842409e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-09-29 03:29:38,919 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m✅ Filled 40761 missing values in numeric column 'sell_price' using 'group_forward_fill'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "group_cols = ['store_id', 'item_id'] \n",
    "# Handle numeric columns\n",
    "for col in numeric_cols:\n",
    "    if df[col].isnull().any():\n",
    "        missing_count = df[col].isnull().sum()\n",
    "        missing_pct = missing_count / len(df) * 100\n",
    "\n",
    "        # Determine strategy\n",
    "        if any(pattern in col for pattern in ['lag', 'roll', 'ewm']):\n",
    "            strategy = strategy_config.get('time_series_features', 'forward_fill')\n",
    "        elif 'sell_price' in col:\n",
    "            strategy = strategy_config.get('price_features', 'group_forward_fill')\n",
    "        elif col.endswith('_flag') or 'is_' in col:\n",
    "            strategy = strategy_config.get('flag_features', 'zero_fill')\n",
    "        else:\n",
    "            strategy = strategy_config.get('other_features', 'mean_fill')\n",
    "         # Apply strategy\n",
    "        if strategy == 'forward_fill':\n",
    "            df[col] = (df.groupby(group_cols,observed=False)[col]\n",
    "                        .transform(lambda x: x.ffill().fillna(method='bfill')))\n",
    "        elif strategy == 'group_forward_fill':\n",
    "            df[col] = (df.groupby(group_cols,observed=False)[col]\n",
    "                        .transform(lambda x: x.ffill()\n",
    "                                    .bfill().fillna(x.mean())))\n",
    "        elif strategy == 'zero_fill':\n",
    "            df[col] = df[col].fillna(0)\n",
    "        elif strategy == 'mean_fill':\n",
    "            df[col] = df[col].fillna(df[col].mean())\n",
    "        else:\n",
    "            logger.warning(f\"⚠️ Unknown numeric fill strategy '{strategy}' for column '{col}', skipping.\")\n",
    "\n",
    "        logger.info(f\"✅ Filled {missing_count} missing values in numeric column '{col}' using '{strategy}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bf7f1c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[col].isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d3c24920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NaN, 'SuperBowl', 'ValentinesDay', 'PresidentsDay', 'LentStart', ..., 'Chanukah End', 'NewYear', 'OrthodoxChristmas', 'MartinLutherKingDay', 'Easter']\n",
       "Length: 31\n",
       "Categories (30, object): ['Chanukah End', 'Christmas', 'Cinco De Mayo', 'ColumbusDay', ..., 'SuperBowl', 'Thanksgiving', 'ValentinesDay', 'VeteransDay']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['event_name_1'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "6b4ffc4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2958084, 29)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "c34ba02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-09-29 04:13:07,084 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m🗓️ Adding M5-specific date features...\u001b[0m\n",
      "[ 2025-09-29 04:13:07,088 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32mColumns before adding date features: ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'd', 'sales', 'date', 'wm_yr_wk', 'weekday', 'wday', 'month', 'year', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'day', 'quarter', 'week_of_year', 'is_weekend', 'has_event', 'snap_any', 'sell_price', 'revenue']\u001b[0m\n",
      "date\n",
      "[ 2025-09-29 04:13:09,085 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m🎉 Holiday features added successfully\u001b[0m\n",
      "[ 2025-09-29 04:13:09,092 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m🌀 Cyclical encoding applied successfully\u001b[0m\n",
      "[ 2025-09-29 04:13:09,093 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m✅ Date features added successfully\u001b[0m\n",
      "[ 2025-09-29 04:13:09,093 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32mColumns after adding date features: ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'd', 'sales', 'date', 'wm_yr_wk', 'weekday', 'wday', 'month', 'year', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'day', 'quarter', 'week_of_year', 'is_weekend', 'has_event', 'snap_any', 'sell_price', 'revenue', 'is_holiday']\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "df_date = pipeline.add_date_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "5f05a096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2958084, 30)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_date.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "1529f6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-09-29 04:18:54,291 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m🛠️ Adding SNAP benefit features...\u001b[0m\n",
      "[ 2025-09-29 04:18:54,392 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m✅ Added 'snap_benefit_period' feature\u001b[0m\n",
      "[ 2025-09-29 04:18:54,396 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m🎉 SNAP features added using columns: ['snap_CA', 'snap_TX', 'snap_WI']\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "df = pipeline.add_snap_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "28c9ec18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2958084, 29)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "44970750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-09-29 04:22:50,041 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m🛠️ Adding price features...\u001b[0m\n",
      "[ 2025-09-29 04:22:50,778 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m✅ Filled missing 'sell_price' with group forward/backward fill\u001b[0m\n",
      "[ 2025-09-29 04:22:50,927 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m✅ Added lag feature: price_lag_1\u001b[0m\n",
      "[ 2025-09-29 04:22:50,964 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m✅ Added price change feature: price_change_1d\u001b[0m\n",
      "[ 2025-09-29 04:22:50,982 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m✅ Added price increase/decrease flags for lag 1d\u001b[0m\n",
      "[ 2025-09-29 04:22:51,155 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m✅ Added lag feature: price_lag_7\u001b[0m\n",
      "[ 2025-09-29 04:22:51,185 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m✅ Added price change feature: price_change_7d\u001b[0m\n",
      "[ 2025-09-29 04:22:51,203 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m✅ Added price increase/decrease flags for lag 7d\u001b[0m\n",
      "[ 2025-09-29 04:22:51,362 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m✅ Added lag feature: price_lag_14\u001b[0m\n",
      "[ 2025-09-29 04:22:51,405 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m✅ Added price change feature: price_change_14d\u001b[0m\n",
      "[ 2025-09-29 04:22:51,428 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m✅ Added price increase/decrease flags for lag 14d\u001b[0m\n",
      "[ 2025-09-29 04:22:51,650 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m✅ Added lag feature: price_lag_28\u001b[0m\n",
      "[ 2025-09-29 04:22:51,702 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m✅ Added price change feature: price_change_28d\u001b[0m\n",
      "[ 2025-09-29 04:22:51,727 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m✅ Added price increase/decrease flags for lag 28d\u001b[0m\n",
      "[ 2025-09-29 04:22:53,348 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m✅ Added price volatility feature: price_volatility_7d\u001b[0m\n",
      "[ 2025-09-29 04:22:54,598 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m✅ Added price volatility feature: price_volatility_28d\u001b[0m\n",
      "[ 2025-09-29 04:22:54,602 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m🎉Price features added successfully\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "df = pipeline.add_price_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "eb1653f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'd',\n",
       "       'sales', 'date', 'wm_yr_wk', 'weekday', 'wday', 'month', 'year',\n",
       "       'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2',\n",
       "       'snap_CA', 'snap_TX', 'snap_WI', 'day', 'quarter', 'week_of_year',\n",
       "       'is_weekend', 'has_event', 'snap_any', 'sell_price', 'revenue',\n",
       "       'price_lag_1', 'price_change_1d', 'price_increased_1d',\n",
       "       'price_decreased_1d', 'price_lag_7', 'price_change_7d',\n",
       "       'price_increased_7d', 'price_decreased_7d', 'price_lag_14',\n",
       "       'price_change_14d', 'price_increased_14d', 'price_decreased_14d',\n",
       "       'price_lag_28', 'price_change_28d', 'price_increased_28d',\n",
       "       'price_decreased_28d', 'price_volatility_7d', 'price_volatility_28d'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "4c2a97d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2958084, 29)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "cc0faa59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82845"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['price_lag_28'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "c7cc86ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-09-29 04:28:25,704 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m📅 Adding event features...\u001b[0m\n",
      "[ 2025-09-29 04:28:25,784 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m✅ Created 'has_event' flag\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-09-29 04:28:25,835 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m✅ Created event type flags for column: event_type_1\u001b[0m\n",
      "[ 2025-09-29 04:28:25,855 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m✅ Created event type flags for column: event_type_2\u001b[0m\n",
      "[ 2025-09-29 04:28:25,858 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m🎉 Event features added using columns: ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "df = pipeline.add_event_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "4d5a2ede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          NaN\n",
       "1          NaN\n",
       "2          NaN\n",
       "3          NaN\n",
       "4          NaN\n",
       "          ... \n",
       "2958079    NaN\n",
       "2958080    NaN\n",
       "2958081    NaN\n",
       "2958082    NaN\n",
       "2958083    NaN\n",
       "Name: event_name_1, Length: 2958084, dtype: category\n",
       "Categories (30, object): ['Chanukah End', 'Christmas', 'Cinco De Mayo', 'ColumbusDay', ..., 'SuperBowl', 'Thanksgiving', 'ValentinesDay', 'VeteransDay']"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['event_name_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "59178ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'd',\n",
       "       'sales', 'date', 'wm_yr_wk', 'weekday', 'wday', 'month', 'year',\n",
       "       'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2',\n",
       "       'snap_CA', 'snap_TX', 'snap_WI', 'day', 'quarter', 'week_of_year',\n",
       "       'is_weekend', 'has_event', 'snap_any', 'sell_price', 'revenue',\n",
       "       'sales_lag_1', 'sales_lag_2', 'sales_lag_3', 'sales_lag_7',\n",
       "       'sales_lag_14', 'sales_lag_21', 'sales_lag_28', 'price_lag_1',\n",
       "       'price_lag_7', 'price_lag_14', 'price_lag_28', 'revenue_lag_7',\n",
       "       'revenue_lag_14', 'revenue_lag_28'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "e3c6121b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-09-29 05:04:28,859 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m🔄 Adding lag features...\u001b[0m\n",
      "[ 2025-09-29 05:04:30,084 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m✅ Added 7 sales lag features\u001b[0m\n",
      "[ 2025-09-29 05:04:30,629 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m✅ Added 4 sales lag features\u001b[0m\n",
      "[ 2025-09-29 05:04:31,144 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m✅ Added 3 sales lag features\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "df = pipeline.add_lag_features()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "9f18574d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2958084, 43)"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "40d2cb85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "634286"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['price_lag_14'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "c55ffe40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2958084, 29)"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "1e9655f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-09-29 05:27:57,159 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m🚀 Adding M5 advanced features...\u001b[0m\n",
      "[ 2025-09-29 05:28:01,249 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m📊 Adding trend features...\u001b[0m\n",
      "[ 2025-09-29 05:28:01,936 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m🔢 Adding ratio features...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_10360\\2664400638.py:720: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  self.df.groupby(['store_id', 'item_id'])['zero_sales_flag']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-09-29 05:28:04,446 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m✅ Advanced features added successfully\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "df = pipeline.add_advanced_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "698d2f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2958084, 38)"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42bbaa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "salesenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
