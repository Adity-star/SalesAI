{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb76836e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\OneDrive\\Desktop\\SalesAI\\backend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\anaconda3\\envs\\salesenv\\lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e0015a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-10-01 15:58:01,828 ] root - \u001b[32mINFO\u001b[0m - \u001b[32mLogger is configured and ready.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-10-01 15:58:03,700 ] src.utils.config_loader - \u001b[32mINFO\u001b[0m - \u001b[32mLoaded configuration from: C:\\Users\\Administrator\\OneDrive\\Desktop\\SalesAI\\backend\\configs\\ml_config.yaml\u001b[0m\n",
      "[ 2025-10-01 15:58:03,703 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Config loaded successfully using config_loader\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "import optuna \n",
    "import mlflow\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from prophet import Prophet\n",
    "\n",
    "from src.logger import logger\n",
    "from typing import Dict, Any, List, Optional, Union,Tuple\n",
    "from pathlib import Path\n",
    "from src.exception import CustomException \n",
    "\n",
    "from src.utils.mlflow_utils import MLflowManager\n",
    "from src.features.feature_pipeline import FeaturePipeline\n",
    "from src.data_pipelines.validators import DataValidator\n",
    "from src.models.advanced_ensemble import AdvancedEnsemble\n",
    "from src.models.digonistics import diagnose_model_performance\n",
    "from src.models.ensemble_model import EnsembleModel\n",
    "from src.utils.config_loader import ConfigLoader\n",
    "from src.visualizations.shap_visualizer import ShapExplainer \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, MinMaxScaler, RobustScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60b36fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, config_path: Optional[Union[str, Path]] = None):\n",
    "        self.config_loader = ConfigLoader()\n",
    "\n",
    "        self.config = self.config_loader.load_yaml(file_path=\"ml_config.yaml\")\n",
    "\n",
    "        self.training_config = self.config.get('training',{})\n",
    "\n",
    "        self.model_config: Dict[str, Any] = self.config.get('models', {})\n",
    "        self.model_config: Dict[str, Any] = self.config.get('models', {})\n",
    "\n",
    "        self.mlflow_manager = MLflowManager(config_path)\n",
    "        self.feature_engineer = None\n",
    "\n",
    "        self.data_validator = DataValidator(config_path)\n",
    "\n",
    "        self.models: Dict[str, Any] = {}\n",
    "        self.scalers: Dict[str, Any] = {}\n",
    "        self.encoders: Dict[str, Any] = {}\n",
    "        self.feature_cols: List[str] = []\n",
    "\n",
    "\n",
    "    def prepare_data(\n",
    "        self, df: pd.DataFrame, target_col: str = \"sales\",\n",
    "        date_col: str = \"date\", group_cols: Optional[List[str]] = None,\n",
    "        categorical_cols: Optional[List[str]] = None\n",
    "    ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "\n",
    "        logger.info(\"üõ† Preparing data for training\")\n",
    "\n",
    "        required_cols = [date_col, target_col]\n",
    "        if group_cols:\n",
    "            required_cols.extend(group_cols)\n",
    "\n",
    "        missing_cols = set(required_cols) - set(df.columns)\n",
    "        if missing_cols:\n",
    "            logger.error(f\"‚ùå Missing required columns for training: {missing_cols}\")\n",
    "            raise CustomException(f\"Missing required columns for training: {missing_cols}\", sys)\n",
    "\n",
    "        try:\n",
    "            pipeline = FeaturePipeline(df, target_col=target_col, group_cols=group_cols)\n",
    "            df_features = pipeline.run()\n",
    "            logger.info(\"‚úÖ Feature pipeline executed successfully.\")\n",
    "\n",
    "            if categorical_cols:\n",
    "                df_features = pipeline.create_target_encoding(df_features, target_col, categorical_cols)\n",
    "                logger.info(\"üéØ Applied target encoding to categorical columns.\")\n",
    "\n",
    "            # Chronological split\n",
    "            df_sorted = df_features.sort_values(date_col)\n",
    "            train_size = int(len(df_sorted) * (1 - self.training_config[\"test_size\"] - self.training_config[\"validation_size\"]))\n",
    "            val_size = int(len(df_sorted) * self.training_config[\"validation_size\"])\n",
    "\n",
    "            train_df = df_sorted[:train_size]\n",
    "            val_df = df_sorted[train_size:train_size + val_size]\n",
    "            test_df = df_sorted[train_size + val_size:]\n",
    "\n",
    "            # Drop rows with missing target\n",
    "            train_df = train_df.dropna(subset=[target_col])\n",
    "            val_df = val_df.dropna(subset=[target_col])\n",
    "            test_df = test_df.dropna(subset=[target_col])\n",
    "\n",
    "            logger.info(f\"üìä Data split ‚Üí Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "            return train_df, val_df, test_df\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error during data preparation: {e}\")\n",
    "            raise CustomException(e, sys)\n",
    "\n",
    "\n",
    "    def preprocess_features(\n",
    "        self,\n",
    "        train_df: pd.DataFrame,\n",
    "        val_df: pd.DataFrame,\n",
    "        test_df: pd.DataFrame,\n",
    "        target_col: str,\n",
    "        exclude_cols: List[str] = [\"date\"]\n",
    "    ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        try:\n",
    "            logger.info(\"üîÑ Starting feature preprocessing...\")\n",
    "\n",
    "            feature_cols = [col for col in train_df.columns if col not in exclude_cols + [target_col]]\n",
    "            self.feature_cols = feature_cols\n",
    "\n",
    "            X_train, X_val, X_test = train_df[feature_cols].copy(), val_df[feature_cols].copy(), test_df[feature_cols].copy()\n",
    "            y_train, y_val, y_test = train_df[target_col].values, val_df[target_col].values, test_df[target_col].values\n",
    "\n",
    "            categorical_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "\n",
    "            for col in categorical_cols:\n",
    "                if self.training_config.get(\"encoder\", \"label\") == \"label\":\n",
    "                    # Train LabelEncoder on training data\n",
    "                    if col not in self.encoders:\n",
    "                        le = LabelEncoder()\n",
    "                        le.fit(X_train[col].astype(str).fillna(\"missing\"))\n",
    "                        self.encoders[col] = le\n",
    "                    else:\n",
    "                        le = self.encoders[col]\n",
    "\n",
    "                    def transform_safe(encoder, series):\n",
    "                        known_classes = set(encoder.classes_)\n",
    "                        unknowns = series[~series.isin(known_classes)]\n",
    "                        if not unknowns.empty:\n",
    "                            logger.warning(f\"‚ö†Ô∏è Unseen labels in column '{col}': {unknowns.unique()}\")\n",
    "                            # Add \"unknown\" class if not already present\n",
    "                            if \"unknown\" not in encoder.classes_:\n",
    "                                encoder.classes_ = np.append(encoder.classes_, \"unknown\")\n",
    "                            series = series.apply(lambda x: x if x in known_classes else \"unknown\")\n",
    "                        return encoder.transform(series)\n",
    "\n",
    "                    for df_name, df in zip([\"Train\", \"Val\", \"Test\"], [X_train, X_val, X_test]):\n",
    "                        series = df[col].astype(str).fillna(\"missing\")\n",
    "                        transformed = transform_safe(le, series)\n",
    "                        df[col] = transformed.astype(np.int32)\n",
    "\n",
    "                        logger.info(f\"‚úÖ Label encoded '{col}' in {df_name} set.\")\n",
    "\n",
    "                elif self.training_config[\"encoder\"] == \"onehot\":\n",
    "                    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "                    X_train_encoded = ohe.fit_transform(X_train[[col]])\n",
    "                    X_val_encoded = ohe.transform(X_val[[col]])\n",
    "                    X_test_encoded = ohe.transform(X_test[[col]])\n",
    "\n",
    "                    encoded_cols = [f\"{col}_{cat}\" for cat in ohe.categories_[0]]\n",
    "                    X_train = X_train.drop(columns=col).join(pd.DataFrame(X_train_encoded, columns=encoded_cols, index=X_train.index))\n",
    "                    X_val = X_val.drop(columns=col).join(pd.DataFrame(X_val_encoded, columns=encoded_cols, index=X_val.index))\n",
    "                    X_test = X_test.drop(columns=col).join(pd.DataFrame(X_test_encoded, columns=encoded_cols, index=X_test.index))\n",
    "\n",
    "                    self.encoders[col] = ohe\n",
    "                    logger.info(f\"‚úÖ One-hot encoded '{col}'.\")\n",
    "\n",
    "            # Scaling numeric features\n",
    "            scaler_type = self.training_config.get(\"scaler\", \"standard\")\n",
    "            if scaler_type == \"standard\":\n",
    "                scaler = StandardScaler()\n",
    "            elif scaler_type == \"minmax\":\n",
    "                scaler = MinMaxScaler()\n",
    "            elif scaler_type == \"robust\":\n",
    "                scaler = RobustScaler()\n",
    "            else:\n",
    "                raise CustomException(f\"Unsupported scaler type: {scaler_type}\")\n",
    "\n",
    "            X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "            X_val_scaled = pd.DataFrame(scaler.transform(X_val), columns=X_val.columns, index=X_val.index)\n",
    "            X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "            self.scalers[\"scaler\"] = scaler\n",
    "\n",
    "            logger.info(f\"‚úÖ Preprocessing complete. Total features used: {len(self.feature_cols)} üß†\")\n",
    "\n",
    "            return X_train_scaled, X_val_scaled, X_test_scaled, y_train, y_val, y_test\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Preprocessing failed: {e}\")\n",
    "            raise CustomException(f\"Error in preprocess_features: {e}\")\n",
    "\n",
    "\n",
    "    def calculate_metrics(self, y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n",
    "        try:\n",
    "            logger.info(\"üìä Calculating model evaluation metrics...\")\n",
    "\n",
    "            # Avoid division by zero in MAPE\n",
    "            non_zero_mask = y_true != 0\n",
    "            if not np.any(non_zero_mask):\n",
    "                mape = np.nan\n",
    "                logger.warning(\"‚ö†Ô∏è All values in y_true are zero. MAPE is undefined.\")\n",
    "            else:\n",
    "                mape = np.mean(np.abs((y_true[non_zero_mask] - y_pred[non_zero_mask]) / y_true[non_zero_mask])) * 100\n",
    "\n",
    "            metrics = {\n",
    "                \"rmse\": round(np.sqrt(mean_squared_error(y_true, y_pred)), 4),\n",
    "                \"mae\": round(mean_absolute_error(y_true, y_pred), 4),\n",
    "                \"mape\": round(mape, 4),\n",
    "                \"r2\": round(r2_score(y_true, y_pred), 4),\n",
    "            }\n",
    "\n",
    "            logger.info(f\"‚úÖ Metrics calculated: {metrics}\")\n",
    "            return metrics\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Failed to calculate metrics: {e}\")\n",
    "            raise CustomException(f\"Error in calculate_metrics: {e}\")\n",
    "\n",
    "\n",
    "    def train_xgboost(self, \n",
    "                  X_train: np.ndarray, y_train: np.ndarray,\n",
    "                  X_val: np.ndarray, y_val: np.ndarray,\n",
    "                  use_optuna: bool = True) -> xgb.XGBRegressor:\n",
    "        \"\"\"\n",
    "        Train an XGBoost regressor with optional Optuna hyperparameter optimization.\n",
    "\n",
    "        Args:\n",
    "            X_train, y_train: Training data\n",
    "            X_val, y_val: Validation data\n",
    "            use_optuna (bool): Whether to perform Optuna hyperparameter search\n",
    "\n",
    "        Returns:\n",
    "            Trained XGBRegressor model\n",
    "        \"\"\"\n",
    "        logger.info(\"üöÄ Starting training for XGBoost model...\")\n",
    "\n",
    "        if len(X_train) == 0 or len(X_val) == 0:\n",
    "            raise ValueError(\"‚ùå Training or validation data is empty.\")\n",
    "\n",
    "        # Detect GPU support\n",
    "        try:\n",
    "            tree_method = \"gpu_hist\" if xgb.get_config().get(\"use_gpu\", False) else \"hist\"\n",
    "            logger.info(f\"‚öôÔ∏è Using tree_method: `{tree_method}`\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"‚ö†Ô∏è Failed to detect GPU, using CPU. Reason: {e}\")\n",
    "            tree_method = \"hist\"\n",
    "\n",
    "        best_params = {}\n",
    "\n",
    "        if use_optuna:\n",
    "            logger.info(\"üîç Running Optuna hyperparameter optimization for XGBoost...\")\n",
    "\n",
    "            def objective(trial):\n",
    "                params = {\n",
    "                    'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "                    'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "                    'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.3, log=True),\n",
    "                    'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "                    'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "                    'gamma': trial.suggest_float('gamma', 0, 1.0),\n",
    "                    'reg_alpha': trial.suggest_float('reg_alpha', 0, 5.0),\n",
    "                    'reg_lambda': trial.suggest_float('reg_lambda', 0, 5.0),\n",
    "                    'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "                    'random_state': 42,\n",
    "                    'tree_method': tree_method\n",
    "                }\n",
    "\n",
    "                model = xgb.XGBRegressor(**params, early_stopping_rounds=50)\n",
    "                model.fit(\n",
    "                    X_train, y_train,\n",
    "                    eval_set=[(X_val, y_val)],\n",
    "                    verbose=False\n",
    "                )\n",
    "                y_pred = model.predict(X_val)\n",
    "                rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "                return rmse\n",
    "\n",
    "            try:\n",
    "                study = optuna.create_study(\n",
    "                    direction=\"minimize\",\n",
    "                    sampler=optuna.samplers.TPESampler(seed=42),\n",
    "                    pruner=optuna.pruners.MedianPruner()\n",
    "                )\n",
    "                study.optimize(objective, n_trials=self.training_config.get('optuna_trials', 50))\n",
    "\n",
    "                best_params = study.best_params\n",
    "                logger.info(f\"üèÜ Optuna best params found: {best_params}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"‚ùå Optuna optimization failed: {e}\")\n",
    "                raise CustomException(f\"Optuna error: {e}\")\n",
    "        else:\n",
    "            logger.info(\"üì¶ Using config-defined hyperparameters for XGBoost...\")\n",
    "            best_params = self.model_config.get(\"xgboost\", {}).get(\"params\", {})\n",
    "            if not best_params:\n",
    "                raise ValueError(\"‚ùå No XGBoost parameters found in model_config.\")\n",
    "            logger.info(f\"‚úÖ Loaded XGBoost params: {best_params}\")\n",
    "\n",
    "        # Add fixed parameters\n",
    "        best_params.update({\n",
    "            \"random_state\": 42,\n",
    "            \"tree_method\": tree_method\n",
    "        })\n",
    "\n",
    "        # Train final model\n",
    "        try:\n",
    "            logger.info(\"üõ†Ô∏è Training final XGBoost model...\")\n",
    "            model = xgb.XGBRegressor(**best_params, early_stopping_rounds=50)\n",
    "            model.fit(\n",
    "                X_train, y_train,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                verbose=True\n",
    "            )\n",
    "\n",
    "            self.models[\"xgboost\"] = model\n",
    "            try:\n",
    "                logger.info(\"Generating SHAP explanations for XGBoost...\")\n",
    "                shap_explainer = ShapExplainer(model, \"lightgbm\")\n",
    "                background_sample = X_train.sample(n=min(100, len(X_train)), random_state=42)\n",
    "                shap_explainer.fit_explainer(background_sample)\n",
    "                \n",
    "                if not hasattr(self, 'shap_explainers'):\n",
    "                    self.shap_explainers = {}\n",
    "                self.shap_explainers['xgboost'] = shap_explainer\n",
    "                logger.info(\"SHAP explainer fitted for XGBoost\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"SHAP generation failed for XGBoost: {e}\")\n",
    "            logger.info(f\"‚úÖ Model trained successfully. Best iteration: {model.best_iteration}\")\n",
    "            return model\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå XGBoost model training failed: {e}\")\n",
    "            raise CustomException(f\"XGBoost training error: {e}\")\n",
    "\n",
    "\n",
    "    def train_lightgbm(self, \n",
    "                   X_train: np.ndarray, y_train: np.ndarray,\n",
    "                   X_val: np.ndarray, y_val: np.ndarray,\n",
    "                   use_optuna: bool = True) -> lgb.LGBMRegressor:\n",
    "        \"\"\"\n",
    "        Train a LightGBM regressor with optional Optuna hyperparameter optimization.\n",
    "\n",
    "        Args:\n",
    "            X_train, y_train: Training dataset\n",
    "            X_val, y_val: Validation dataset\n",
    "            use_optuna (bool): Whether to use Optuna for hyperparameter tuning\n",
    "\n",
    "        Returns:\n",
    "            Trained LGBMRegressor model\n",
    "        \"\"\"\n",
    "\n",
    "        logger.info(\"‚öôÔ∏è Starting LightGBM training...\")\n",
    "\n",
    "        best_params = {}\n",
    "\n",
    "        if use_optuna:\n",
    "            logger.info(\"üîç Optuna hyperparameter optimization enabled for LightGBM\")\n",
    "\n",
    "            def objective(trial):\n",
    "                params = {\n",
    "                    \"num_leaves\": trial.suggest_int(\"num_leaves\", 31, 256),\n",
    "                    \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.005, 0.3, log=True),\n",
    "                    \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "                    \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "                    \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "                    \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "                    \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0, 5.0),\n",
    "                    \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0, 5.0),\n",
    "                    \"min_split_gain\": trial.suggest_float(\"min_split_gain\", 0, 1.0),\n",
    "                    \"random_state\": 42,\n",
    "                    \"verbosity\": -1,\n",
    "                    \"objective\": \"regression\",\n",
    "                    \"metric\": \"rmse\",\n",
    "                    \"boosting_type\": \"gbdt\"\n",
    "                }\n",
    "\n",
    "                model = lgb.LGBMRegressor(**params)\n",
    "                model.fit(\n",
    "                    X_train, y_train,\n",
    "                    eval_set=[(X_val, y_val)],\n",
    "                    callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]\n",
    "                )\n",
    "\n",
    "                y_pred = model.predict(X_val)\n",
    "                rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "                return rmse\n",
    "\n",
    "            try:\n",
    "                study = optuna.create_study(\n",
    "                    direction=\"minimize\",\n",
    "                    sampler=optuna.samplers.TPESampler(seed=42),\n",
    "                    pruner=optuna.pruners.MedianPruner()\n",
    "                )\n",
    "                study.optimize(objective, n_trials=self.training_config.get(\"optuna_trials\", 50))\n",
    "                best_params = study.best_params\n",
    "\n",
    "                logger.info(f\"üèÜ Best LightGBM params via Optuna: {best_params}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"‚ùå Optuna optimization failed: {e}\")\n",
    "                raise CustomException(f\"Optuna LightGBM error: {e}\")\n",
    "\n",
    "            # Add required fixed params\n",
    "            best_params.update({\n",
    "                \"random_state\": 42,\n",
    "                \"verbosity\": -1,\n",
    "                \"objective\": \"regression\",\n",
    "                \"metric\": \"rmse\",\n",
    "                \"boosting_type\": \"gbdt\"\n",
    "            })\n",
    "\n",
    "        else:\n",
    "            logger.info(\"üì¶ Using static config parameters for LightGBM...\")\n",
    "            best_params = self.model_config.get(\"lightgbm\", {}).get(\"params\", {})\n",
    "            if not best_params:\n",
    "                raise ValueError(\"‚ùå No LightGBM parameters found in `model_config`\")\n",
    "            logger.info(f\"‚úÖ Loaded config params: {best_params}\")\n",
    "\n",
    "        # Final training\n",
    "        try:\n",
    "            logger.info(\"üõ†Ô∏è Training final LightGBM model...\")\n",
    "            model = lgb.LGBMRegressor(**best_params)\n",
    "            model.fit(\n",
    "                X_train, y_train,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                callbacks=[lgb.early_stopping(50), lgb.log_evaluation(50)]\n",
    "            )\n",
    "\n",
    "            self.models[\"lightgbm\"] = model\n",
    "            try:\n",
    "                logger.info(\"Generating SHAP explanations for LightGBM...\")\n",
    "                shap_explainer = ShapExplainer(model, \"lightgbm\")\n",
    "                background_sample = X_train.sample(n=min(100, len(X_train)), random_state=42)\n",
    "                shap_explainer.fit_explainer(background_sample)\n",
    "                \n",
    "                if not hasattr(self, 'shap_explainers'):\n",
    "                    self.shap_explainers = {}\n",
    "                self.shap_explainers['lightgbm'] = shap_explainer\n",
    "                logger.info(\"‚úÖ SHAP explainer fitted for LightGBM\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"‚ö†Ô∏è SHAP generation failed for LightGBM: {e}\")\n",
    "\n",
    "\n",
    "            logger.info(\"‚úÖ LightGBM model trained and stored.\")\n",
    "            return model\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå LightGBM training failed: {e}\")\n",
    "            raise CustomException(f\"LightGBM training error: {e}\")\n",
    "\n",
    "\n",
    "    def train_prophet(self, \n",
    "                    train_df: pd.DataFrame, \n",
    "                    val_df: pd.DataFrame,\n",
    "                    date_col: str = 'date', \n",
    "                    target_col: str = 'sales') -> Prophet:\n",
    "        \"\"\"\n",
    "        Train a Prophet model with optional regressors and evaluate on validation set.\n",
    "\n",
    "        Args:\n",
    "            train_df: Training DataFrame\n",
    "            val_df: Validation DataFrame\n",
    "            date_col: Name of date column\n",
    "            target_col: Name of target variable\n",
    "\n",
    "        Returns:\n",
    "            Trained Prophet model\n",
    "        \"\"\"\n",
    "\n",
    "        logger.info(\"üìÖ Starting Prophet training...\")\n",
    "\n",
    "        try:\n",
    "            # --- Prepare training data ---\n",
    "            prophet_train = train_df[[date_col, target_col]].rename(\n",
    "                columns={date_col: 'ds', target_col: 'y'}\n",
    "            ).dropna().sort_values('ds')\n",
    "\n",
    "            # Load Prophet hyperparameters from config\n",
    "            prophet_params = self.model_config.get('prophet', {}).get('params', {})\n",
    "            prophet_params.update({\n",
    "                'stan_backend': 'CMDSTANPY',\n",
    "                'mcmc_samples': 0,             # No Bayesian sampling = faster\n",
    "                'uncertainty_samples': 100     # Reasonable uncertainty\n",
    "            })\n",
    "\n",
    "            model = Prophet(**prophet_params)\n",
    "\n",
    "            # --- Select numeric regressors ---\n",
    "            numeric_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "            regressor_cols = [c for c in numeric_cols if c not in [target_col, 'year', 'month', 'day', 'week', 'quarter']]\n",
    "\n",
    "            # Reduce to top 5 highest variance\n",
    "            if len(regressor_cols) > 5:\n",
    "                variances = {col: train_df[col].var() for col in regressor_cols}\n",
    "                regressor_cols = sorted(variances, key=variances.get, reverse=True)[:5]\n",
    "\n",
    "            for col in regressor_cols:\n",
    "                if col in train_df and train_df[col].std() > 0:\n",
    "                    model.add_regressor(col)\n",
    "                    prophet_train[col] = train_df[col]\n",
    "                    logger.info(f\"‚ûï Added regressor to Prophet: {col}\")\n",
    "                else:\n",
    "                    logger.warning(f\"‚ö†Ô∏è Skipping regressor '{col}' due to zero variance or missing data.\")\n",
    "\n",
    "            # --- Fit the model ---\n",
    "            model.fit(prophet_train)\n",
    "            self.models['prophet'] = model\n",
    "            logger.info(\"‚úÖ Prophet model trained successfully.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Prophet training failed with error: {e}\")\n",
    "\n",
    "            logger.info(\"üîÅ Retrying Prophet with fallback parameters...\")\n",
    "\n",
    "            try:\n",
    "                model = Prophet(\n",
    "                    yearly_seasonality=True,\n",
    "                    weekly_seasonality=True,\n",
    "                    daily_seasonality=False,\n",
    "                    changepoint_prior_scale=0.05,\n",
    "                    seasonality_prior_scale=10.0,\n",
    "                    uncertainty_samples=50,\n",
    "                    mcmc_samples=0\n",
    "                )\n",
    "                fallback_train = prophet_train[['ds', 'y']]\n",
    "                model.fit(fallback_train)\n",
    "                self.models['prophet'] = model\n",
    "                logger.info(\"‚úÖ Prophet fallback model trained successfully.\")\n",
    "            except Exception as fallback_error:\n",
    "                logger.error(\"‚ùå Fallback Prophet training also failed.\")\n",
    "                raise CustomException(f\"Prophet training completely failed: {fallback_error}\")\n",
    "\n",
    "        # --- Validation Prediction ---\n",
    "        try:\n",
    "            prophet_val = val_df[[date_col, target_col]].rename(\n",
    "                columns={date_col: 'ds', target_col: 'y'}\n",
    "            ).dropna().sort_values('ds')\n",
    "\n",
    "            # Add matching regressors to validation data\n",
    "            for col in regressor_cols:\n",
    "                if col in val_df:\n",
    "                    prophet_val[col] = val_df[col]\n",
    "                else:\n",
    "                    logger.warning(f\"‚ö†Ô∏è Regressor '{col}' missing in validation set\")\n",
    "\n",
    "            forecast = model.predict(prophet_val)\n",
    "\n",
    "            y_true = prophet_val['y'].values\n",
    "            y_pred = forecast['yhat'].values\n",
    "\n",
    "            val_rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "            val_mae = mean_absolute_error(y_true, y_pred)\n",
    "            val_r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "            logger.info(f\"üìä Prophet Validation Metrics ‚Äî RMSE: {val_rmse:.4f} | MAE: {val_mae:.4f} | R¬≤: {val_r2:.4f}\")\n",
    "        except Exception as eval_error:\n",
    "            logger.error(f\"‚ùå Failed during Prophet validation: {eval_error}\")\n",
    "            raise CustomException(f\"Prophet evaluation failed: {eval_error}\")\n",
    "\n",
    "        return model\n",
    "\n",
    "    \n",
    "    def train_all_models(self, train_df: pd.DataFrame, val_df: pd.DataFrame,\n",
    "                        test_df: pd.DataFrame, target_col: str = 'sales',\n",
    "                        use_optuna: bool = True) -> Dict[str, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Train all models (XGBoost, LightGBM, Prophet) and build ensemble.\n",
    "        Improvements applied: bug fixes, defensive checks, improved logging.\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        logger.info(\"üöÄ Starting full model training pipeline...\")\n",
    "\n",
    "        # Start MLflow run via manager (assume manager wraps mlflow.start_run)\n",
    "        run_name = f\"sales_forecast_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        run_id = self.mlflow_manager.start_run(run_name, tags={\"model_type\": \"ensemble\", \"use_optuna\": str(use_optuna)})\n",
    "        logger.info(f\"üéØ MLflow run started with run_id={run_id}\")\n",
    "\n",
    "        try:\n",
    "            # ------------------------\n",
    "            # Preprocess Data\n",
    "            # ------------------------\n",
    "            logger.info(\"üßπ Preprocessing features and target variables...\")\n",
    "            X_train, X_val, X_test, y_train, y_val, y_test = self.preprocess_features(\n",
    "                train_df, val_df, test_df, target_col\n",
    "            )\n",
    "            logger.info(f\"üìä Data sizes - Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "            self.mlflow_manager.log_params({\n",
    "                \"train_size\": len(train_df),\n",
    "                \"val_size\": len(val_df),\n",
    "                \"test_size\": len(test_df),\n",
    "                \"n_features\": X_train.shape[1]\n",
    "            })\n",
    "\n",
    "            # Keep placeholders for predictions (so we can always compose test_predictions)\n",
    "            xgb_pred = lgb_pred = prophet_pred = None\n",
    "\n",
    "            # ------------\n",
    "            # Train XGBoost\n",
    "            # ------------\n",
    "            logger.info(\"üî• Training XGBoost model...\")\n",
    "            try:\n",
    "                xgb_model = self.train_xgboost(X_train, y_train, X_val, y_val, use_optuna)\n",
    "                xgb_pred = xgb_model.predict(X_test)\n",
    "                xgb_metrics = self.calculate_metrics(y_test, xgb_pred)\n",
    "\n",
    "                self.mlflow_manager.log_metrics({f\"xgboost_{k}\": v for k, v in xgb_metrics.items()})\n",
    "                self.mlflow_manager.log_model(xgb_model, \"xgboost\", input_example=X_train.iloc[:5])\n",
    "\n",
    "                # Feature importance: handle sklearn wrapper or booster object\n",
    "                try:\n",
    "                    if hasattr(xgb_model, \"feature_importances_\"):\n",
    "                        imp = xgb_model.feature_importances_\n",
    "                    else:\n",
    "                        # try booster score fallbacks\n",
    "                        booster = getattr(xgb_model, \"get_booster\", None)\n",
    "                        if callable(booster):\n",
    "                            score = xgb_model.get_booster().get_score(importance_type='gain')\n",
    "                            imp = [score.get(f, 0.0) for f in self.feature_cols]\n",
    "                        else:\n",
    "                            imp = [0.0] * len(self.feature_cols)\n",
    "                except Exception:\n",
    "                    imp = [0.0] * len(self.feature_cols)\n",
    "\n",
    "                feature_importance = pd.DataFrame({\n",
    "                    'feature': self.feature_cols,\n",
    "                    'importance': imp\n",
    "                }).sort_values('importance', ascending=False).head(20)\n",
    "\n",
    "                logger.info(f\"üåü Top XGBoost features:\\n{feature_importance.to_string()}\")\n",
    "                for i, (_, row) in enumerate(feature_importance.iterrows()):\n",
    "                    self.mlflow_manager.log_params({f\"xgb_top_feature_{i}\": f\"{row['feature']} ({row['importance']:.4f})\"})\n",
    "\n",
    "                results['xgboost'] = {\n",
    "                    'model': xgb_model,\n",
    "                    'metrics': xgb_metrics,\n",
    "                    'predictions': xgb_pred,\n",
    "                    'actual': y_test\n",
    "                }\n",
    "                logger.info(\"‚úÖ XGBoost training complete!\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"‚ùå XGBoost training failed: {e}\", exc_info=True)\n",
    "                raise\n",
    "\n",
    "            # ------------\n",
    "            # Train LightGBM\n",
    "            # ------------\n",
    "            logger.info(\"üî• Training LightGBM model...\")\n",
    "            try:\n",
    "                lgb_model = self.train_lightgbm(X_train, y_train, X_val, y_val, use_optuna)\n",
    "                lgb_pred = lgb_model.predict(X_test)\n",
    "                lgb_metrics = self.calculate_metrics(y_test, lgb_pred)\n",
    "\n",
    "                self.mlflow_manager.log_metrics({f\"lightgbm_{k}\": v for k, v in lgb_metrics.items()})\n",
    "                self.mlflow_manager.log_model(lgb_model, \"lightgbm\", input_example=X_train.iloc[:5])\n",
    "\n",
    "                # LightGBM feature importance defensive\n",
    "                try:\n",
    "                    if hasattr(lgb_model, \"feature_importance\"):\n",
    "                        imp = lgb_model.feature_importance()\n",
    "                    elif hasattr(lgb_model, \"booster_\"):\n",
    "                        imp = lgb_model.booster_.feature_importance()\n",
    "                    else:\n",
    "                        imp = [0.0] * len(self.feature_cols)\n",
    "                except Exception:\n",
    "                    imp = [0.0] * len(self.feature_cols)\n",
    "\n",
    "                lgb_importance = pd.DataFrame({\n",
    "                    'feature': self.feature_cols,\n",
    "                    'importance': imp\n",
    "                }).sort_values('importance', ascending=False).head(20)\n",
    "\n",
    "                logger.info(f\"üåü Top LightGBM features:\\n{lgb_importance.to_string()}\")\n",
    "\n",
    "                results['lightgbm'] = {\n",
    "                    'model': lgb_model,\n",
    "                    'metrics': lgb_metrics,\n",
    "                    'predictions': lgb_pred,\n",
    "                    'actual': y_test\n",
    "                }\n",
    "                logger.info(\"‚úÖ LightGBM training complete!\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"‚ùå LightGBM training failed: {e}\", exc_info=True)\n",
    "                raise\n",
    "\n",
    "            # ------------------------\n",
    "            # Train Prophet (optional)\n",
    "            # ------------------------\n",
    "            prophet_enabled = self.model_config.get('prophet', {}).get('enabled', True)\n",
    "            if prophet_enabled:\n",
    "                logger.info(\"üåü Training Prophet model...\")\n",
    "                try:\n",
    "                    prophet_model = self.train_prophet(train_df, val_df)\n",
    "\n",
    "                    future = test_df[['date']].rename(columns={'date': 'ds'}).copy()\n",
    "\n",
    "                    # Add regressors if they exist both in model and test_df\n",
    "                    regressor_cols = []\n",
    "                    if hasattr(prophet_model, 'extra_regressors') and isinstance(prophet_model.extra_regressors, dict):\n",
    "                        regressor_cols = [c for c in prophet_model.extra_regressors.keys() if c in test_df.columns]\n",
    "                    else:\n",
    "                        # fallback: check if test_df contains likely regressor columns used in training\n",
    "                        regressor_cols = [c for c in test_df.columns if c not in ['date', target_col]]\n",
    "\n",
    "                    for col in regressor_cols:\n",
    "                        future[col] = test_df[col]\n",
    "\n",
    "                    prophet_pred = prophet_model.predict(future)['yhat'].values\n",
    "                    prophet_metrics = self.calculate_metrics(y_test, prophet_pred)\n",
    "\n",
    "                    self.mlflow_manager.log_metrics({f\"prophet_{k}\": v for k, v in prophet_metrics.items()})\n",
    "                    self.mlflow_manager.log_model(prophet_model, \"prophet\", input_example=future.iloc[:5])\n",
    "\n",
    "                    results['prophet'] = {\n",
    "                        'model': prophet_model,\n",
    "                        'metrics': prophet_metrics,\n",
    "                        'predictions': prophet_pred,\n",
    "                        'actual': y_test\n",
    "                    }\n",
    "                    logger.info(\"‚úÖ Prophet training complete!\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"‚ö†Ô∏è Prophet training failed: {e}. Falling back to XGBoost+LightGBM ensemble.\")\n",
    "                    prophet_enabled = False\n",
    "                    prophet_pred = None\n",
    "            else:\n",
    "                logger.info(\"‚ÑπÔ∏è Prophet training skipped by config.\")\n",
    "                prophet_pred = None\n",
    "\n",
    "            # ------------------------\n",
    "            # Create test_predictions dict (must exist before ensemble optimizer)\n",
    "            # ------------------------\n",
    "            test_predictions = {\n",
    "                'xgboost': xgb_pred,\n",
    "                'lightgbm': lgb_pred,\n",
    "                'prophet': prophet_pred\n",
    "            }\n",
    "\n",
    "            # ------------------------\n",
    "            # Build Weighted Ensemble (if prophet missing use 2-model weighting)\n",
    "            # ------------------------\n",
    "            logger.info(\"üß© Building stacking/weighted ensemble...\")\n",
    "            try:\n",
    "                if prophet_enabled and prophet_pred is not None:\n",
    "                    # Simple equal-weight ensemble when Prophet available (or you can compute a better blend)\n",
    "                    ensemble_pred = (xgb_pred + lgb_pred + prophet_pred) / 3.0\n",
    "                    ensemble_weights = {'xgboost': 1/3, 'lightgbm': 1/3, 'prophet': 1/3}\n",
    "                    logger.info(\"‚öñÔ∏è Using equal weights for XGB/LGB/Prophet ensemble.\")\n",
    "                else:\n",
    "                    # compute weights based on validation R2 (defensive: avoid div by zero)\n",
    "                    xgb_val_pred = xgb_model.predict(X_val)\n",
    "                    lgb_val_pred = lgb_model.predict(X_val)\n",
    "\n",
    "                    xgb_val_r2 = r2_score(y_val, xgb_val_pred)\n",
    "                    lgb_val_r2 = r2_score(y_val, lgb_val_pred)\n",
    "\n",
    "                    # ensure positive and min floor\n",
    "                    min_weight = 0.2\n",
    "                    denom = (xgb_val_r2 + lgb_val_r2) if (xgb_val_r2 + lgb_val_r2) != 0 else 1.0\n",
    "                    xgb_weight = max(min_weight, xgb_val_r2 / denom)\n",
    "                    lgb_weight = max(min_weight, lgb_val_r2 / denom)\n",
    "\n",
    "                    total = xgb_weight + lgb_weight\n",
    "                    xgb_weight /= total\n",
    "                    lgb_weight /= total\n",
    "\n",
    "                    ensemble_weights = {'xgboost': xgb_weight, 'lightgbm': lgb_weight}\n",
    "                    ensemble_pred = xgb_weight * xgb_pred + lgb_weight * lgb_pred\n",
    "                    logger.info(f\"‚öñÔ∏è Ensemble weights - XGBoost: {xgb_weight:.3f}, LightGBM: {lgb_weight:.3f}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"‚ùå Failed to build weighted ensemble: {e}\", exc_info=True)\n",
    "                raise\n",
    "\n",
    "            # ------------------------\n",
    "            # Use AdvancedEnsemble to find optimal blend on test predictions\n",
    "            # ------------------------\n",
    "            try:\n",
    "                advanced_ensemble = AdvancedEnsemble()\n",
    "                # ensure we pass only available predictions to the optimizer\n",
    "                available_preds = {k: v for k, v in test_predictions.items() if v is not None}\n",
    "                if len(available_preds) >= 2:\n",
    "                    _, optimal_weights = advanced_ensemble.create_blended_ensemble(available_preds, y_test, optimization_metric='rmse')\n",
    "                    # map optimal_weights back to model keys (assumed ordering inside create_blended_ensemble)\n",
    "                else:\n",
    "                    optimal_weights = ensemble_weights\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"‚ö†Ô∏è Advanced ensemble optimization failed: {e}. Using heuristic weights.\")\n",
    "                optimal_weights = ensemble_weights\n",
    "\n",
    "            # Build EnsembleModel object using models that exist\n",
    "            ensemble_models = {'xgboost': xgb_model, 'lightgbm': lgb_model}\n",
    "            if 'prophet' in results:\n",
    "                ensemble_models['prophet'] = results['prophet']['model']\n",
    "\n",
    "            ensemble_model = EnsembleModel(ensemble_models, optimal_weights)\n",
    "            self.models['ensemble'] = ensemble_model\n",
    "\n",
    "            # Evaluate ensemble\n",
    "            ensemble_metrics = self.calculate_metrics(y_test, ensemble_pred)\n",
    "            self.mlflow_manager.log_metrics({f\"ensemble_{k}\": v for k, v in ensemble_metrics.items()})\n",
    "            self.mlflow_manager.log_model(ensemble_model, \"ensemble\", input_example=X_train.iloc[:5])\n",
    "\n",
    "            results['ensemble'] = {\n",
    "                'model': ensemble_model,\n",
    "                'metrics': ensemble_metrics,\n",
    "                'predictions': ensemble_pred,\n",
    "                'actual': y_test\n",
    "            }\n",
    "            logger.info(\"üèÜ Ensemble training complete!\")\n",
    "\n",
    "            # ------------------------\n",
    "            # Generate SHAP visualizations (best-effort)\n",
    "            # ------------------------\n",
    "            logger.info(\"üìä Generating SHAP visualizations and explanations...\")\n",
    "            try:\n",
    "                shap_artifacts = self._generate_shap_artifacts(X_test, y_test, test_df)\n",
    "                if isinstance(shap_artifacts, dict):\n",
    "                    if shap_artifacts.get('metrics'):\n",
    "                        self.mlflow_manager.log_metrics(shap_artifacts['metrics'])\n",
    "                    if shap_artifacts.get('params'):\n",
    "                        self.mlflow_manager.log_params(shap_artifacts['params'])\n",
    "                    for file_path in shap_artifacts.get('files', []):\n",
    "                        self.mlflow_manager.log_artifact(file_path, \"shap_explanations\")\n",
    "                    logger.info(\"‚úÖ SHAP artifacts logged to MLflow\")\n",
    "                else:\n",
    "                    logger.warning(\"‚ö†Ô∏è SHAP artifacts returned unexpected type; skipping MLflow logging.\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"‚ö†Ô∏è SHAP visualization generation failed: {e}\")\n",
    "\n",
    "            # ------------------------\n",
    "            # Diagnostics & Visualizations\n",
    "            # ------------------------\n",
    "            try:\n",
    "                test_predictions_for_diag = {\n",
    "                    k: v for k, v in {\n",
    "                        'xgboost': xgb_pred,\n",
    "                        'lightgbm': lgb_pred,\n",
    "                        'ensemble': ensemble_pred\n",
    "                    }.items() if v is not None\n",
    "                }\n",
    "                diagnosis = diagnose_model_performance(train_df, val_df, test_df, test_predictions_for_diag, target_col)\n",
    "                logger.info(\"üìã Diagnostic recommendations:\")\n",
    "                for rec in diagnosis.get('recommendations', []):\n",
    "                    logger.warning(f\"‚ö†Ô∏è - {rec}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"‚ùå Diagnostics failed: {e}\", exc_info=True)\n",
    "\n",
    "            try:\n",
    "                self._generate_and_log_visualizations(results, test_df, target_col)\n",
    "            except Exception as viz_error:\n",
    "                logger.error(f\"‚ùå Visualization generation failed: {viz_error}\", exc_info=True)\n",
    "\n",
    "            # Save artifacts to disk / local artifact store\n",
    "            logger.info(\"üíæ Saving artifacts...\")\n",
    "            try:\n",
    "                self.save_artifacts()\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"‚ö†Ô∏è save_artifacts() failed: {e}\")\n",
    "\n",
    "            # Obtain run id from mlflow.active_run if available, else fall back to run_id returned earlier\n",
    "            try:\n",
    "                import mlflow\n",
    "                active = mlflow.active_run()\n",
    "                current_run_id = active.info.run_id if active else run_id\n",
    "            except Exception:\n",
    "                current_run_id = run_id\n",
    "\n",
    "            # End the run via manager (assume it closes mlflow)\n",
    "            self.mlflow_manager.end_run()\n",
    "            logger.info(\"üèÅ MLflow run ended.\")\n",
    "\n",
    "            # ------------------------\n",
    "            # Sync to S3 (best-effort)\n",
    "            # ------------------------\n",
    "            logger.info(\"‚òÅÔ∏è Syncing artifacts to S3...\")\n",
    "            try:\n",
    "                from src.utils.mlflow_s3_utils import MLflowS3Manager\n",
    "                s3_manager = MLflowS3Manager()\n",
    "                s3_manager.sync_mlflow_artifacts_to_s3(current_run_id)\n",
    "                logger.info(\"‚úì Successfully synced artifacts to S3\")\n",
    "\n",
    "                from src.utils.s3_verification import verify_s3_artifacts, log_s3_verification_results\n",
    "                logger.info(\"üîç Verifying S3 artifact storage...\")\n",
    "                verification_results = verify_s3_artifacts(\n",
    "                    run_id=current_run_id,\n",
    "                    expected_artifacts=[\n",
    "                        'models/',\n",
    "                        'scalers.pkl',\n",
    "                        'encoders.pkl',\n",
    "                        'feature_cols.pkl',\n",
    "                        'visualizations/',\n",
    "                        'reports/'\n",
    "                    ]\n",
    "                )\n",
    "                log_s3_verification_results(verification_results)\n",
    "                if not verification_results.get(\"success\", False):\n",
    "                    logger.warning(\"‚ö†Ô∏è S3 artifact verification failed after sync\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"‚ùå Failed to sync artifacts to S3: {e}\", exc_info=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            # Ensure MLflow run closed and marked failed\n",
    "            try:\n",
    "                self.mlflow_manager.end_run(status=\"FAILED\")\n",
    "            except Exception:\n",
    "                logger.warning(\"‚ö†Ô∏è Could not gracefully end MLflow run on exception.\")\n",
    "            logger.error(f\"üí• Training pipeline failed: {e}\", exc_info=True)\n",
    "            raise\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "    def _create_combined_html_report(self, saved_files: Dict[str, str], save_dir: str) -> None:\n",
    "        \"\"\"Create a combined HTML report with all visualizations\"\"\"\n",
    "        import os\n",
    "        from datetime import datetime\n",
    "        import base64\n",
    "\n",
    "        try:\n",
    "            logger.info(\"üìù Creating combined HTML report for visualizations...\")\n",
    "\n",
    "            html_content = \"\"\"\n",
    "            <!DOCTYPE html>\n",
    "            <html>\n",
    "            <head>\n",
    "                <title>Model Comparison Report</title>\n",
    "                <style>\n",
    "                    body {{\n",
    "                        font-family: Arial, sans-serif;\n",
    "                        margin: 20px;\n",
    "                        background-color: #f5f5f5;\n",
    "                    }}\n",
    "                    h1, h2 {\n",
    "                        color: #333;\n",
    "                    }\n",
    "                    .section {\n",
    "                        background-color: white;\n",
    "                        padding: 20px;\n",
    "                        margin-bottom: 20px;\n",
    "                        border-radius: 8px;\n",
    "                        box-shadow: 0 2px 4px rgba(0,0,0,0.1);\n",
    "                    }\n",
    "                    .timestamp {\n",
    "                        color: #666;\n",
    "                        font-size: 14px;\n",
    "                    }\n",
    "                    iframe {\n",
    "                        width: 100%;\n",
    "                        height: 800px;\n",
    "                        border: 1px solid #ddd;\n",
    "                        border-radius: 4px;\n",
    "                        margin-top: 10px;\n",
    "                    }\n",
    "                    img {\n",
    "                        max-width: 100%;\n",
    "                        height: auto;\n",
    "                        border-radius: 4px;\n",
    "                        margin-top: 10px;\n",
    "                    }\n",
    "                </style>\n",
    "            </head>\n",
    "            <body>\n",
    "                <h1>Sales Forecast Model Comparison Report</h1>\n",
    "                <p class=\"timestamp\">Generated on: {timestamp}</p>\n",
    "            \"\"\"\n",
    "\n",
    "            html_content = html_content.format(timestamp=datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "            sections = [\n",
    "                ('metrics_comparison', 'Model Performance Metrics'),\n",
    "                ('predictions_comparison', 'Predictions Comparison'),\n",
    "                ('residuals_analysis', 'Residuals Analysis'),\n",
    "                ('error_distribution', 'Error Distribution'),\n",
    "                ('feature_importance', 'Feature Importance'),\n",
    "                ('summary', 'Summary Statistics')\n",
    "            ]\n",
    "\n",
    "            for key, title in sections:\n",
    "                if key in saved_files:\n",
    "                    html_content += f'<div class=\"section\"><h2>{title}</h2>'\n",
    "\n",
    "                    try:\n",
    "                        with open(saved_files[key], 'rb') as f:\n",
    "                            img_data = base64.b64encode(f.read()).decode()\n",
    "                        html_content += f'<img src=\"data:image/png;base64,{img_data}\" alt=\"{title}\">'\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"‚ö†Ô∏è Failed to embed image for section '{title}': {e}\")\n",
    "\n",
    "                    html_content += '</div>'\n",
    "\n",
    "            html_content += \"\"\"\n",
    "            </body>\n",
    "            </html>\n",
    "            \"\"\"\n",
    "\n",
    "            report_path = os.path.join(save_dir, 'model_comparison_report.html')\n",
    "            with open(report_path, 'w') as f:\n",
    "                f.write(html_content)\n",
    "\n",
    "            logger.info(f\"‚úÖ Combined HTML report created at: {report_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"üí• Failed to create combined HTML report: {e}\", exc_info=True)\n",
    "\n",
    "\n",
    "    def _generate_shap_artifacts(self, X_test: pd.DataFrame, y_test: np.ndarray, \n",
    "                            test_df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Generate SHAP explanations and visualizations.\"\"\"\n",
    "        import tempfile\n",
    "        import os\n",
    "        from src.visualizations.shap_visualizer import ShapVisualizer\n",
    "        \n",
    "        artifacts = {'metrics': {}, 'params': {}, 'files': []}\n",
    "        \n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            for model_name in ['xgboost', 'lightgbm']:\n",
    "                if not hasattr(self, 'shap_explainers') or model_name not in self.shap_explainers:\n",
    "                    logger.warning(f\"‚ö†Ô∏è No SHAP explainer found for {model_name}\")\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    explainer = self.shap_explainers[model_name]\n",
    "                    visualizer = ShapVisualizer(explainer)\n",
    "                    \n",
    "                    # Generate explanations\n",
    "                    test_sample = X_test.sample(n=min(100, len(X_test)), random_state=42)\n",
    "                    explanation = explainer.explain_prediction(test_sample)\n",
    "                    \n",
    "                    # Global importance\n",
    "                    importance_df = explainer.get_global_importance(explanation, top_k=15)\n",
    "                    csv_path = os.path.join(temp_dir, f\"{model_name}_feature_importance.csv\")\n",
    "                    importance_df.to_csv(csv_path, index=False)\n",
    "                    artifacts['files'].append(csv_path)\n",
    "                    \n",
    "                    # Global importance visualization\n",
    "                    html_path = os.path.join(temp_dir, f\"{model_name}_global_importance.html\")\n",
    "                    visualizer.plot_global_importance(explanation, save_path=html_path)\n",
    "                    artifacts['files'].append(html_path)\n",
    "                    \n",
    "                    # Summary plot\n",
    "                    summary_path = os.path.join(temp_dir, f\"{model_name}_summary.html\")\n",
    "                    visualizer.plot_summary_plot(explanation, save_path=summary_path)\n",
    "                    artifacts['files'].append(summary_path)\n",
    "                    \n",
    "                    # Individual waterfall plots (first 3 samples)\n",
    "                    for i in range(min(3, len(test_sample))):\n",
    "                        waterfall_path = os.path.join(temp_dir, f\"{model_name}_waterfall_{i}.html\")\n",
    "                        visualizer.plot_waterfall_explanation(explanation, sample_idx=i, save_path=waterfall_path)\n",
    "                        artifacts['files'].append(waterfall_path)\n",
    "                    \n",
    "                    # Business explanations\n",
    "                    business_exp = visualizer.create_business_explanation(\n",
    "                        explanation, \n",
    "                        sample_idx=0,\n",
    "                        store_id=str(test_sample.iloc[0].get('store_id', 'Unknown')),\n",
    "                        item_id=str(test_sample.iloc[0].get('item_id', 'Unknown')),\n",
    "                        pred_date=str(test_sample.iloc[0].get('date', 'Unknown'))\n",
    "                    )\n",
    "                    \n",
    "                    # Log metrics\n",
    "                    artifacts['metrics'][f'{model_name}_shap_top_feature'] = importance_df.iloc[0]['feature']\n",
    "                    artifacts['metrics'][f'{model_name}_shap_top_importance'] = float(importance_df.iloc[0]['importance'])\n",
    "                    \n",
    "                    # Log params (top 5 features)\n",
    "                    for i, row in importance_df.head(5).iterrows():\n",
    "                        artifacts['params'][f'{model_name}_feature_{i+1}'] = f\"{row['feature']} ({row['importance']:.4f})\"\n",
    "                    \n",
    "                    logger.info(f\"‚úÖ Generated SHAP artifacts for {model_name}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"‚ùå Failed to generate SHAP artifacts for {model_name}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        return artifacts\n",
    "\n",
    "\n",
    "    def save_artifacts(self, version: str = None):\n",
    "        \"\"\"\n",
    "        Save scalers, encoders, feature columns, and trained models.\n",
    "        Also logs everything to MLflow for version tracking.\n",
    "        \"\"\"\n",
    "        import os\n",
    "        import joblib\n",
    "        from datetime import datetime\n",
    "\n",
    "        version = version or datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        base_dir = f'/tmp/artifacts/{version}'\n",
    "\n",
    "        try:\n",
    "            os.makedirs(base_dir, exist_ok=True)\n",
    "            logger.info(f\"üìÅ Created artifact directory: {base_dir}\")\n",
    "\n",
    "            # Save preprocessing objects\n",
    "            joblib.dump(self.scalers, os.path.join(base_dir, 'scalers.pkl'))\n",
    "            joblib.dump(self.encoders, os.path.join(base_dir, 'encoders.pkl'))\n",
    "            joblib.dump(self.feature_cols, os.path.join(base_dir, 'feature_cols.pkl'))\n",
    "            logger.info(\"üíæ Saved scalers, encoders, and feature columns.\")\n",
    "\n",
    "            # Save model directories\n",
    "            model_dirs = {\n",
    "                'xgboost': os.path.join(base_dir, 'models/xgboost'),\n",
    "                'lightgbm': os.path.join(base_dir, 'models/lightgbm'),\n",
    "                'ensemble': os.path.join(base_dir, 'models/ensemble')\n",
    "            }\n",
    "\n",
    "            for mname, mdir in model_dirs.items():\n",
    "                os.makedirs(mdir, exist_ok=True)\n",
    "                if mname in self.models:\n",
    "                    model = self.models[mname]\n",
    "                    joblib.dump(model, os.path.join(mdir, f\"{mname}_model.pkl\"))\n",
    "                    logger.info(f\"üõ†Ô∏è Saved model: {mname} -> {mdir}\")\n",
    "\n",
    "            # Save metadata for reproducibility\n",
    "            metadata = {\n",
    "                \"version\": version,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"models_saved\": list(self.models.keys()),\n",
    "                \"feature_count\": len(self.feature_cols),\n",
    "            }\n",
    "            joblib.dump(metadata, os.path.join(base_dir, 'metadata.pkl'))\n",
    "            logger.info(\"üìú Saved metadata.\")\n",
    "\n",
    "            # Log artifacts to MLflow\n",
    "            if hasattr(self, \"mlflow_manager\") and self.mlflow_manager:\n",
    "                self.mlflow_manager.log_artifacts(base_dir)\n",
    "                logger.info(\"üöÄ Artifacts logged to MLflow.\")\n",
    "            else:\n",
    "                logger.warning(\"‚ö†Ô∏è mlflow_manager not found or None. Skipping MLflow logging.\")\n",
    "\n",
    "            logger.info(f\"‚úÖ Artifacts saved successfully in {base_dir}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Failed to save artifacts: {e}\", exc_info=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-10-01 16:21:05,420 ] src.utils.config_loader - \u001b[32mINFO\u001b[0m - \u001b[32mLoaded configuration from: C:\\Users\\Administrator\\OneDrive\\Desktop\\SalesAI\\backend\\configs\\ml_config.yaml\u001b[0m\n",
      "[ 2025-10-01 16:21:05,482 ] src.utils.config_loader - \u001b[32mINFO\u001b[0m - \u001b[32mLoaded configuration from: C:\\Users\\Administrator\\OneDrive\\Desktop\\SalesAI\\backend\\configs\\ml_config.yaml\u001b[0m\n",
      "[ 2025-10-01 16:21:05,489 ] src.utils.service_discovery - \u001b[32mINFO\u001b[0m - \u001b[32mUsing MLFLOW_TRACKING_URI from env: http://localhost:5001\u001b[0m\n",
      "[ 2025-10-01 16:21:05,496 ] src.utils.mlflow_utils - \u001b[32mINFO\u001b[0m - \u001b[32mMlflow tracking URI discovered: http://localhost:5001\u001b[0m\n",
      "[ 2025-10-01 16:21:07,331 ] src.utils.mlflow_utils - \u001b[32mINFO\u001b[0m - \u001b[32mSet MLflow experiment: sales_forecasting\u001b[0m\n",
      "[ 2025-10-01 16:21:07,335 ] src.utils.service_discovery - \u001b[32mINFO\u001b[0m - \u001b[32mUsing MLFLOW_S3_ENDPOINT_URL from env: http://localhost:9000\u001b[0m\n",
      "[ 2025-10-01 16:21:07,353 ] src.utils.mlflow_utils - \u001b[32mINFO\u001b[0m - \u001b[32mConfigured MinIO endpoint: http://localhost:9000\u001b[0m\n",
      "[ 2025-10-01 16:21:07,371 ] src.data_pipelines.validators - \u001b[32mINFO\u001b[0m - \u001b[32müîç Initializing DataValidator...\u001b[0m\n",
      "[ 2025-10-01 16:21:07,397 ] src.utils.config_loader - \u001b[32mINFO\u001b[0m - \u001b[32mLoaded configuration from: C:\\Users\\Administrator\\OneDrive\\Desktop\\SalesAI\\backend\\configs\\ml_config.yaml\u001b[0m\n",
      "[ 2025-10-01 16:21:07,401 ] src.data_pipelines.validators - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Loaded validation config from: ml_config.yaml\u001b[0m\n",
      "[ 2025-10-01 16:21:07,407 ] src.data_pipelines.validators - \u001b[32mINFO\u001b[0m - \u001b[32müìã Validation Config Loaded:\u001b[0m\n",
      "[ 2025-10-01 16:21:07,412 ] src.data_pipelines.validators - \u001b[32mINFO\u001b[0m - \u001b[32m - Required Columns: ['date', 'sales', 'store_id', 'product_id']\u001b[0m\n",
      "[ 2025-10-01 16:21:07,415 ] src.data_pipelines.validators - \u001b[32mINFO\u001b[0m - \u001b[32m - Data Types: {'date': 'datetime64[ns]', 'sales': 'float64', 'store_id': 'object', 'product_id': 'object'}\u001b[0m\n",
      "[ 2025-10-01 16:21:07,418 ] src.data_pipelines.validators - \u001b[32mINFO\u001b[0m - \u001b[32m - Value Ranges: {'sales': {'min': 0, 'max': 1000000}}\u001b[0m\n",
      "[ 2025-10-01 16:21:07,421 ] src.data_pipelines.validators - \u001b[32mINFO\u001b[0m - \u001b[32m - Check Duplicates: True\u001b[0m\n",
      "[ 2025-10-01 16:21:07,424 ] src.data_pipelines.validators - \u001b[32mINFO\u001b[0m - \u001b[32m - Outlier Detection Method: zscore\u001b[0m\n",
      "[ 2025-10-01 16:21:07,427 ] src.data_pipelines.validators - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ DataValidator initialized successfully.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "trainer = ModelTrainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(r\"C:\\Users\\Administrator\\OneDrive\\Desktop\\SalesAI\\backend\\data\\features\\m5\\m5_features.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94d44105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2958084, 82)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a582b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(df)\n",
    "new_df = df.iloc[:n//9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de174f95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(328676, 82)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4508dc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-10-01 16:06:12,550 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müõ† Preparing data for training\u001b[0m\n",
      "[ 2025-10-01 16:06:12,686 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müöÄ Initialized FeaturePipeline with target: sales, groups: ['store_id'], country: US\u001b[0m\n",
      "[ 2025-10-01 16:06:12,693 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müö¶ Starting Feature Pipeline...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-10-01 16:06:12,702 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müìÖ Adding date features\u001b[0m\n",
      "[ 2025-10-01 16:06:15,553 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Added date features: ['year', 'month', 'day', 'dayofweek', 'quarter', 'weekofyear', 'is_weekend', 'is_holiday']\u001b[0m\n",
      "[ 2025-10-01 16:06:15,558 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müìÖ Date features added.\u001b[0m\n",
      "[ 2025-10-01 16:06:15,562 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müï∞ Adding lag features\u001b[0m\n",
      "[ 2025-10-01 16:06:16,073 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created lag feature: sales_lag_1\u001b[0m\n",
      "[ 2025-10-01 16:06:16,082 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created lag feature: sales_lag_2\u001b[0m\n",
      "[ 2025-10-01 16:06:16,099 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created lag feature: sales_lag_3\u001b[0m\n",
      "[ 2025-10-01 16:06:16,114 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created lag feature: sales_lag_7\u001b[0m\n",
      "[ 2025-10-01 16:06:16,124 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created lag feature: sales_lag_14\u001b[0m\n",
      "[ 2025-10-01 16:06:16,141 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created lag feature: sales_lag_21\u001b[0m\n",
      "[ 2025-10-01 16:06:16,162 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created lag feature: sales_lag_30\u001b[0m\n",
      "[ 2025-10-01 16:06:16,168 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müï∞ Lag features added.\u001b[0m\n",
      "[ 2025-10-01 16:06:16,174 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müìà Adding rolling features\u001b[0m\n",
      "[ 2025-10-01 16:06:16,785 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created rolling feature: sales_roll_3_mean\u001b[0m\n",
      "[ 2025-10-01 16:06:16,839 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created rolling feature: sales_roll_3_std\u001b[0m\n",
      "[ 2025-10-01 16:06:16,929 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created rolling feature: sales_roll_3_min\u001b[0m\n",
      "[ 2025-10-01 16:06:17,024 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created rolling feature: sales_roll_3_max\u001b[0m\n",
      "[ 2025-10-01 16:06:17,253 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created rolling feature: sales_roll_3_median\u001b[0m\n",
      "[ 2025-10-01 16:06:17,314 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created rolling feature: sales_roll_7_mean\u001b[0m\n",
      "[ 2025-10-01 16:06:17,378 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created rolling feature: sales_roll_7_std\u001b[0m\n",
      "[ 2025-10-01 16:06:17,432 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created rolling feature: sales_roll_7_min\u001b[0m\n",
      "[ 2025-10-01 16:06:17,487 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created rolling feature: sales_roll_7_max\u001b[0m\n",
      "[ 2025-10-01 16:06:17,693 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created rolling feature: sales_roll_7_median\u001b[0m\n",
      "[ 2025-10-01 16:06:17,778 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created rolling feature: sales_roll_14_mean\u001b[0m\n",
      "[ 2025-10-01 16:06:17,917 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created rolling feature: sales_roll_14_std\u001b[0m\n",
      "[ 2025-10-01 16:06:18,040 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created rolling feature: sales_roll_14_min\u001b[0m\n",
      "[ 2025-10-01 16:06:18,109 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created rolling feature: sales_roll_14_max\u001b[0m\n",
      "[ 2025-10-01 16:06:18,495 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created rolling feature: sales_roll_14_median\u001b[0m\n",
      "[ 2025-10-01 16:06:18,596 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created rolling feature: sales_roll_21_mean\u001b[0m\n",
      "[ 2025-10-01 16:06:18,682 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created rolling feature: sales_roll_21_std\u001b[0m\n",
      "[ 2025-10-01 16:06:18,763 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created rolling feature: sales_roll_21_min\u001b[0m\n",
      "[ 2025-10-01 16:06:18,834 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created rolling feature: sales_roll_21_max\u001b[0m\n",
      "[ 2025-10-01 16:06:19,065 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created rolling feature: sales_roll_21_median\u001b[0m\n",
      "[ 2025-10-01 16:06:19,128 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created rolling feature: sales_roll_30_mean\u001b[0m\n",
      "[ 2025-10-01 16:06:19,189 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created rolling feature: sales_roll_30_std\u001b[0m\n",
      "[ 2025-10-01 16:06:19,237 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created rolling feature: sales_roll_30_min\u001b[0m\n",
      "[ 2025-10-01 16:06:19,293 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created rolling feature: sales_roll_30_max\u001b[0m\n",
      "[ 2025-10-01 16:06:19,488 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created rolling feature: sales_roll_30_median\u001b[0m\n",
      "[ 2025-10-01 16:06:19,488 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müìà Rolling features added.\u001b[0m\n",
      "[ 2025-10-01 16:06:19,496 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müîÑ Adding cyclical features (month and dayofweek)...\u001b[0m\n",
      "[ 2025-10-01 16:06:19,555 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Cyclical features added successfully.\u001b[0m\n",
      "[ 2025-10-01 16:06:19,558 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müîÑ Cyclical features added.\u001b[0m\n",
      "[ 2025-10-01 16:06:20,377 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müßπ Handling missing values for 98 numeric columns.\u001b[0m\n",
      "[ 2025-10-01 16:06:20,800 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müîÑ Filled 1 missing values in time-based feature 'sales_lag_1' using forward/backward fill.\u001b[0m\n",
      "[ 2025-10-01 16:06:20,807 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müîÑ Filled 2 missing values in time-based feature 'sales_lag_2' using forward/backward fill.\u001b[0m\n",
      "[ 2025-10-01 16:06:20,821 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müîÑ Filled 3 missing values in time-based feature 'sales_lag_3' using forward/backward fill.\u001b[0m\n",
      "[ 2025-10-01 16:06:20,834 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müîÑ Filled 7 missing values in time-based feature 'sales_lag_7' using forward/backward fill.\u001b[0m\n",
      "[ 2025-10-01 16:06:20,841 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müîÑ Filled 14 missing values in time-based feature 'sales_lag_14' using forward/backward fill.\u001b[0m\n",
      "[ 2025-10-01 16:06:20,852 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müîÑ Filled 21 missing values in time-based feature 'sales_lag_21' using forward/backward fill.\u001b[0m\n",
      "[ 2025-10-01 16:06:20,877 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müîÑ Filled 1 missing values in time-based feature 'sales_roll_7_std' using forward/backward fill.\u001b[0m\n",
      "[ 2025-10-01 16:06:20,906 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müîÑ Filled 1 missing values in time-based feature 'sales_roll_14_std' using forward/backward fill.\u001b[0m\n",
      "[ 2025-10-01 16:06:20,979 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müîÑ Filled 30 missing values in time-based feature 'sales_lag_30' using forward/backward fill.\u001b[0m\n",
      "[ 2025-10-01 16:06:21,011 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müîÑ Filled 1 missing values in time-based feature 'sales_roll_3_std' using forward/backward fill.\u001b[0m\n",
      "[ 2025-10-01 16:06:21,087 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müîÑ Filled 1 missing values in time-based feature 'sales_roll_21_std' using forward/backward fill.\u001b[0m\n",
      "[ 2025-10-01 16:06:21,207 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müîÑ Filled 1 missing values in time-based feature 'sales_roll_30_std' using forward/backward fill.\u001b[0m\n",
      "[ 2025-10-01 16:06:21,229 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Completed handling missing values.\u001b[0m\n",
      "[ 2025-10-01 16:06:21,232 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müßπ Handled missing values.\u001b[0m\n",
      "[ 2025-10-01 16:06:21,235 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müèÅ Feature Pipeline completed successfully.\u001b[0m\n",
      "[ 2025-10-01 16:06:21,237 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Feature pipeline executed successfully.\u001b[0m\n",
      "[ 2025-10-01 16:06:21,244 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müéØ Creating target encoding for categorical columns\u001b[0m\n",
      "[ 2025-10-01 16:06:21,926 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müìä Global mean of target 'sales': 0.9237\u001b[0m\n",
      "[ 2025-10-01 16:06:21,941 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müî§ Applying target encoding for column: store_id\u001b[0m\n",
      "[ 2025-10-01 16:06:22,167 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created target-encoded feature: store_id_target_encoded\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\OneDrive\\Desktop\\SalesAI\\backend\\src\\features\\feature_pipeline.py:342: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  means = df.groupby(col)[target_col].mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-10-01 16:06:22,176 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müèÅ Target encoding completed for 1 columns.\u001b[0m\n",
      "[ 2025-10-01 16:06:22,182 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müéØ Applied target encoding to categorical columns.\u001b[0m\n",
      "[ 2025-10-01 16:06:23,664 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müìä Data split ‚Üí Train: 230073, Val: 32867, Test: 65736\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train_df,value_df,test_df = trainer.prepare_data(\n",
    "    df=new_df,\n",
    "    target_col=\"sales\",\n",
    "    date_col=\"date\",\n",
    "    group_cols=['store_id'],\n",
    "    categorical_cols=['store_id']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "143aafd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                         0\n",
       "item_id                    0\n",
       "dept_id                    0\n",
       "cat_id                     0\n",
       "store_id                   0\n",
       "                          ..\n",
       "month_sin                  0\n",
       "month_cos                  0\n",
       "dow_sin                    0\n",
       "dow_cos                    0\n",
       "store_id_target_encoded    0\n",
       "Length: 111, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96979e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-10-01 16:06:28,247 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müîÑ Starting feature preprocessing...\u001b[0m\n",
      "[ 2025-10-01 16:06:29,344 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'id' in Train set.\u001b[0m\n",
      "[ 2025-10-01 16:06:29,421 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'id' in Val set.\u001b[0m\n",
      "[ 2025-10-01 16:06:29,531 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'id' in Test set.\u001b[0m\n",
      "[ 2025-10-01 16:06:29,964 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'item_id' in Train set.\u001b[0m\n",
      "[ 2025-10-01 16:06:30,017 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'item_id' in Val set.\u001b[0m\n",
      "[ 2025-10-01 16:06:30,091 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'item_id' in Test set.\u001b[0m\n",
      "[ 2025-10-01 16:06:30,439 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'dept_id' in Train set.\u001b[0m\n",
      "[ 2025-10-01 16:06:30,488 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'dept_id' in Val set.\u001b[0m\n",
      "[ 2025-10-01 16:06:30,553 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'dept_id' in Test set.\u001b[0m\n",
      "[ 2025-10-01 16:06:30,864 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'cat_id' in Train set.\u001b[0m\n",
      "[ 2025-10-01 16:06:30,909 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'cat_id' in Val set.\u001b[0m\n",
      "[ 2025-10-01 16:06:30,978 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'cat_id' in Test set.\u001b[0m\n",
      "[ 2025-10-01 16:06:31,243 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'store_id' in Train set.\u001b[0m\n",
      "[ 2025-10-01 16:06:31,273 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'store_id' in Val set.\u001b[0m\n",
      "[ 2025-10-01 16:06:31,323 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'store_id' in Test set.\u001b[0m\n",
      "[ 2025-10-01 16:06:31,584 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'state_id' in Train set.\u001b[0m\n",
      "[ 2025-10-01 16:06:31,619 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'state_id' in Val set.\u001b[0m\n",
      "[ 2025-10-01 16:06:31,666 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'state_id' in Test set.\u001b[0m\n",
      "[ 2025-10-01 16:06:31,973 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'd' in Train set.\u001b[0m\n",
      "[ 2025-10-01 16:06:32,034 ] __main__ - \u001b[33mWARNING\u001b[0m - \u001b[33m‚ö†Ô∏è Unseen labels in column 'd': ['d_1359' 'd_1360' 'd_1361' 'd_1362' 'd_1363' 'd_1364' 'd_1365' 'd_1366'\n",
      " 'd_1367' 'd_1368' 'd_1369' 'd_1370' 'd_1371' 'd_1372' 'd_1373' 'd_1374'\n",
      " 'd_1375' 'd_1376' 'd_1377' 'd_1378' 'd_1379' 'd_1380' 'd_1381' 'd_1382'\n",
      " 'd_1383' 'd_1384' 'd_1385' 'd_1386' 'd_1387' 'd_1388' 'd_1389' 'd_1390'\n",
      " 'd_1391' 'd_1392' 'd_1393' 'd_1394' 'd_1395' 'd_1396' 'd_1397' 'd_1398'\n",
      " 'd_1399' 'd_1400' 'd_1401' 'd_1402' 'd_1403' 'd_1404' 'd_1405' 'd_1406'\n",
      " 'd_1407' 'd_1408' 'd_1409' 'd_1410' 'd_1411' 'd_1412' 'd_1413' 'd_1414'\n",
      " 'd_1415' 'd_1416' 'd_1417' 'd_1418' 'd_1419' 'd_1420' 'd_1421' 'd_1422'\n",
      " 'd_1423' 'd_1424' 'd_1425' 'd_1426' 'd_1427' 'd_1428' 'd_1429' 'd_1430'\n",
      " 'd_1431' 'd_1432' 'd_1433' 'd_1434' 'd_1435' 'd_1436' 'd_1437' 'd_1438'\n",
      " 'd_1439' 'd_1440' 'd_1441' 'd_1442' 'd_1443' 'd_1444' 'd_1445' 'd_1446'\n",
      " 'd_1447' 'd_1448' 'd_1449' 'd_1450' 'd_1451' 'd_1452' 'd_1453' 'd_1454'\n",
      " 'd_1455' 'd_1456' 'd_1457' 'd_1458' 'd_1459' 'd_1460' 'd_1461' 'd_1462'\n",
      " 'd_1463' 'd_1464' 'd_1465' 'd_1466' 'd_1467' 'd_1468' 'd_1469' 'd_1470'\n",
      " 'd_1471' 'd_1472' 'd_1473' 'd_1474' 'd_1475' 'd_1476' 'd_1477' 'd_1478'\n",
      " 'd_1479' 'd_1480' 'd_1481' 'd_1482' 'd_1483' 'd_1484' 'd_1485' 'd_1486'\n",
      " 'd_1487' 'd_1488' 'd_1489' 'd_1490' 'd_1491' 'd_1492' 'd_1493' 'd_1494'\n",
      " 'd_1495' 'd_1496' 'd_1497' 'd_1498' 'd_1499' 'd_1500' 'd_1501' 'd_1502'\n",
      " 'd_1503' 'd_1504' 'd_1505' 'd_1506' 'd_1507' 'd_1508' 'd_1509' 'd_1510'\n",
      " 'd_1511' 'd_1512' 'd_1513' 'd_1514' 'd_1515' 'd_1516' 'd_1517' 'd_1518'\n",
      " 'd_1519' 'd_1520' 'd_1521' 'd_1522' 'd_1523' 'd_1524' 'd_1525' 'd_1526'\n",
      " 'd_1527' 'd_1528' 'd_1529' 'd_1530' 'd_1531' 'd_1532' 'd_1533' 'd_1534'\n",
      " 'd_1535' 'd_1536' 'd_1537' 'd_1538' 'd_1539' 'd_1540' 'd_1541' 'd_1542'\n",
      " 'd_1543' 'd_1544' 'd_1545' 'd_1546' 'd_1547' 'd_1548' 'd_1549' 'd_1550'\n",
      " 'd_1551' 'd_1552' 'd_1553']\u001b[0m\n",
      "[ 2025-10-01 16:06:32,065 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'd' in Val set.\u001b[0m\n",
      "[ 2025-10-01 16:06:32,137 ] __main__ - \u001b[33mWARNING\u001b[0m - \u001b[33m‚ö†Ô∏è Unseen labels in column 'd': ['d_1553' 'd_1554' 'd_1555' 'd_1556' 'd_1557' 'd_1558' 'd_1559' 'd_1560'\n",
      " 'd_1561' 'd_1562' 'd_1563' 'd_1564' 'd_1565' 'd_1566' 'd_1567' 'd_1568'\n",
      " 'd_1569' 'd_1570' 'd_1571' 'd_1572' 'd_1573' 'd_1574' 'd_1575' 'd_1576'\n",
      " 'd_1577' 'd_1578' 'd_1579' 'd_1580' 'd_1581' 'd_1582' 'd_1583' 'd_1584'\n",
      " 'd_1585' 'd_1586' 'd_1587' 'd_1588' 'd_1589' 'd_1590' 'd_1591' 'd_1592'\n",
      " 'd_1593' 'd_1594' 'd_1595' 'd_1596' 'd_1597' 'd_1598' 'd_1599' 'd_1600'\n",
      " 'd_1601' 'd_1602' 'd_1603' 'd_1604' 'd_1605' 'd_1606' 'd_1607' 'd_1608'\n",
      " 'd_1609' 'd_1610' 'd_1611' 'd_1612' 'd_1613' 'd_1614' 'd_1615' 'd_1616'\n",
      " 'd_1617' 'd_1618' 'd_1619' 'd_1620' 'd_1621' 'd_1622' 'd_1623' 'd_1624'\n",
      " 'd_1625' 'd_1626' 'd_1627' 'd_1628' 'd_1629' 'd_1630' 'd_1631' 'd_1632'\n",
      " 'd_1633' 'd_1634' 'd_1635' 'd_1636' 'd_1637' 'd_1638' 'd_1639' 'd_1640'\n",
      " 'd_1641' 'd_1642' 'd_1643' 'd_1644' 'd_1645' 'd_1646' 'd_1647' 'd_1648'\n",
      " 'd_1649' 'd_1650' 'd_1651' 'd_1652' 'd_1653' 'd_1654' 'd_1655' 'd_1656'\n",
      " 'd_1657' 'd_1658' 'd_1659' 'd_1660' 'd_1661' 'd_1662' 'd_1663' 'd_1664'\n",
      " 'd_1665' 'd_1666' 'd_1667' 'd_1668' 'd_1669' 'd_1670' 'd_1671' 'd_1672'\n",
      " 'd_1673' 'd_1674' 'd_1675' 'd_1676' 'd_1677' 'd_1678' 'd_1679' 'd_1680'\n",
      " 'd_1681' 'd_1682' 'd_1683' 'd_1684' 'd_1685' 'd_1686' 'd_1687' 'd_1688'\n",
      " 'd_1689' 'd_1690' 'd_1691' 'd_1692' 'd_1693' 'd_1694' 'd_1695' 'd_1696'\n",
      " 'd_1697' 'd_1698' 'd_1699' 'd_1700' 'd_1701' 'd_1702' 'd_1703' 'd_1704'\n",
      " 'd_1705' 'd_1706' 'd_1707' 'd_1708' 'd_1709' 'd_1710' 'd_1711' 'd_1712'\n",
      " 'd_1713' 'd_1714' 'd_1715' 'd_1716' 'd_1717' 'd_1718' 'd_1719' 'd_1720'\n",
      " 'd_1721' 'd_1722' 'd_1723' 'd_1724' 'd_1725' 'd_1726' 'd_1727' 'd_1728'\n",
      " 'd_1729' 'd_1730' 'd_1731' 'd_1732' 'd_1733' 'd_1734' 'd_1735' 'd_1736'\n",
      " 'd_1737' 'd_1738' 'd_1739' 'd_1740' 'd_1741' 'd_1742' 'd_1743' 'd_1744'\n",
      " 'd_1745' 'd_1746' 'd_1747' 'd_1748' 'd_1749' 'd_1750' 'd_1751' 'd_1752'\n",
      " 'd_1753' 'd_1754' 'd_1755' 'd_1756' 'd_1757' 'd_1758' 'd_1759' 'd_1760'\n",
      " 'd_1761' 'd_1762' 'd_1763' 'd_1764' 'd_1765' 'd_1766' 'd_1767' 'd_1768'\n",
      " 'd_1769' 'd_1770' 'd_1771' 'd_1772' 'd_1773' 'd_1774' 'd_1775' 'd_1776'\n",
      " 'd_1777' 'd_1778' 'd_1779' 'd_1780' 'd_1781' 'd_1782' 'd_1783' 'd_1784'\n",
      " 'd_1785' 'd_1786' 'd_1787' 'd_1788' 'd_1789' 'd_1790' 'd_1791' 'd_1792'\n",
      " 'd_1793' 'd_1794' 'd_1795' 'd_1796' 'd_1797' 'd_1798' 'd_1799' 'd_1800'\n",
      " 'd_1801' 'd_1802' 'd_1803' 'd_1804' 'd_1805' 'd_1806' 'd_1807' 'd_1808'\n",
      " 'd_1809' 'd_1810' 'd_1811' 'd_1812' 'd_1813' 'd_1814' 'd_1815' 'd_1816'\n",
      " 'd_1817' 'd_1818' 'd_1819' 'd_1820' 'd_1821' 'd_1822' 'd_1823' 'd_1824'\n",
      " 'd_1825' 'd_1826' 'd_1827' 'd_1828' 'd_1829' 'd_1830' 'd_1831' 'd_1832'\n",
      " 'd_1833' 'd_1834' 'd_1835' 'd_1836' 'd_1837' 'd_1838' 'd_1839' 'd_1840'\n",
      " 'd_1841' 'd_1842' 'd_1843' 'd_1844' 'd_1845' 'd_1846' 'd_1847' 'd_1848'\n",
      " 'd_1849' 'd_1850' 'd_1851' 'd_1852' 'd_1853' 'd_1854' 'd_1855' 'd_1856'\n",
      " 'd_1857' 'd_1858' 'd_1859' 'd_1860' 'd_1861' 'd_1862' 'd_1863' 'd_1864'\n",
      " 'd_1865' 'd_1866' 'd_1867' 'd_1868' 'd_1869' 'd_1870' 'd_1871' 'd_1872'\n",
      " 'd_1873' 'd_1874' 'd_1875' 'd_1876' 'd_1877' 'd_1878' 'd_1879' 'd_1880'\n",
      " 'd_1881' 'd_1882' 'd_1883' 'd_1884' 'd_1885' 'd_1886' 'd_1887' 'd_1888'\n",
      " 'd_1889' 'd_1890' 'd_1891' 'd_1892' 'd_1893' 'd_1894' 'd_1895' 'd_1896'\n",
      " 'd_1897' 'd_1898' 'd_1899' 'd_1900' 'd_1901' 'd_1902' 'd_1903' 'd_1904'\n",
      " 'd_1905' 'd_1906' 'd_1907' 'd_1908' 'd_1909' 'd_1910' 'd_1911' 'd_1912'\n",
      " 'd_1913' 'd_1914' 'd_1915' 'd_1916' 'd_1917' 'd_1918' 'd_1919' 'd_1920'\n",
      " 'd_1921' 'd_1922' 'd_1923' 'd_1924' 'd_1925' 'd_1926' 'd_1927' 'd_1928'\n",
      " 'd_1929' 'd_1930' 'd_1931' 'd_1932' 'd_1933' 'd_1934' 'd_1935' 'd_1936'\n",
      " 'd_1937' 'd_1938' 'd_1939' 'd_1940' 'd_1941']\u001b[0m\n",
      "[ 2025-10-01 16:06:32,187 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'd' in Test set.\u001b[0m\n",
      "[ 2025-10-01 16:06:32,499 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'event_name_1' in Train set.\u001b[0m\n",
      "[ 2025-10-01 16:06:32,535 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'event_name_1' in Val set.\u001b[0m\n",
      "[ 2025-10-01 16:06:32,607 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'event_name_1' in Test set.\u001b[0m\n",
      "[ 2025-10-01 16:06:32,908 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'event_type_1' in Train set.\u001b[0m\n",
      "[ 2025-10-01 16:06:32,955 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'event_type_1' in Val set.\u001b[0m\n",
      "[ 2025-10-01 16:06:33,049 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'event_type_1' in Test set.\u001b[0m\n",
      "[ 2025-10-01 16:06:33,330 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'event_name_2' in Train set.\u001b[0m\n",
      "[ 2025-10-01 16:06:33,356 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'event_name_2' in Val set.\u001b[0m\n",
      "[ 2025-10-01 16:06:33,400 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'event_name_2' in Test set.\u001b[0m\n",
      "[ 2025-10-01 16:06:33,699 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'event_type_2' in Train set.\u001b[0m\n",
      "[ 2025-10-01 16:06:33,734 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'event_type_2' in Val set.\u001b[0m\n",
      "[ 2025-10-01 16:06:33,802 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'event_type_2' in Test set.\u001b[0m\n",
      "[ 2025-10-01 16:06:34,421 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'store_id_target_encoded' in Train set.\u001b[0m\n",
      "[ 2025-10-01 16:06:34,471 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'store_id_target_encoded' in Val set.\u001b[0m\n",
      "[ 2025-10-01 16:06:34,557 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'store_id_target_encoded' in Test set.\u001b[0m\n",
      "[ 2025-10-01 16:06:42,766 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Preprocessing complete. Total features used: 109 üß†\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = trainer.preprocess_features(\n",
    "                train_df, value_df, test_df, target_col='sales'\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "003ffa56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-09-29 23:56:54,026 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚öôÔ∏è Starting LightGBM training...\u001b[0m\n",
      "[ 2025-09-29 23:56:54,030 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müì¶ Using static config parameters for LightGBM...\u001b[0m\n",
      "[ 2025-09-29 23:56:54,037 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Loaded config params: {'num_leaves': 31, 'learning_rate': 0.05, 'n_estimators': 100, 'objective': 'regression', 'random_state': 42}\u001b[0m\n",
      "[ 2025-09-29 23:56:54,040 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müõ†Ô∏è Training final LightGBM model...\u001b[0m\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's l2: 0.0580903\n",
      "[100]\tvalid_0's l2: 0.0087255\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l2: 0.0087255\n",
      "[ 2025-09-29 23:57:00,590 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ LightGBM model trained and stored.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model_light = trainer.train_lightgbm(X_train,y_train,X_val,y_val,use_optuna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "29c11b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_light.predict(X_val)\n",
    "rmse = np.sqrt(mean_squared_error(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e5174eea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09341039322822894"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "055cdb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = 'sales'\n",
    "exclude_cols = 'date'\n",
    "feature_cols = [col for col in train_df.columns ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2f53256b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature': ['id',\n",
       "  'item_id',\n",
       "  'dept_id',\n",
       "  'cat_id',\n",
       "  'store_id',\n",
       "  'state_id',\n",
       "  'd',\n",
       "  'sales',\n",
       "  'date',\n",
       "  'wm_yr_wk',\n",
       "  'weekday',\n",
       "  'wday',\n",
       "  'month',\n",
       "  'year',\n",
       "  'event_name_1',\n",
       "  'event_type_1',\n",
       "  'event_name_2',\n",
       "  'event_type_2',\n",
       "  'snap_CA',\n",
       "  'snap_TX',\n",
       "  'snap_WI',\n",
       "  'day',\n",
       "  'quarter',\n",
       "  'week_of_year',\n",
       "  'is_weekend',\n",
       "  'has_event',\n",
       "  'snap_any',\n",
       "  'sell_price',\n",
       "  'revenue',\n",
       "  'is_holiday',\n",
       "  'snap_benefit_period',\n",
       "  'price_lag_1',\n",
       "  'price_change_1d',\n",
       "  'price_increased_1d',\n",
       "  'price_decreased_1d',\n",
       "  'price_lag_7',\n",
       "  'price_change_7d',\n",
       "  'price_increased_7d',\n",
       "  'price_decreased_7d',\n",
       "  'price_lag_14',\n",
       "  'price_change_14d',\n",
       "  'price_increased_14d',\n",
       "  'price_decreased_14d',\n",
       "  'price_lag_28',\n",
       "  'price_change_28d',\n",
       "  'price_increased_28d',\n",
       "  'price_decreased_28d',\n",
       "  'price_volatility_7d',\n",
       "  'price_volatility_28d',\n",
       "  'event_sporting',\n",
       "  'event_cultural',\n",
       "  'event_national',\n",
       "  'event_religious',\n",
       "  'sales_lag_1',\n",
       "  'sales_lag_2',\n",
       "  'sales_lag_3',\n",
       "  'sales_lag_7',\n",
       "  'sales_lag_14',\n",
       "  'sales_lag_21',\n",
       "  'sales_lag_28',\n",
       "  'revenue_lag_7',\n",
       "  'revenue_lag_14',\n",
       "  'revenue_lag_28',\n",
       "  'sales_roll_7_mean',\n",
       "  'sales_roll_7_std',\n",
       "  'sales_roll_14_mean',\n",
       "  'sales_roll_14_std',\n",
       "  'sales_roll_28_mean',\n",
       "  'sales_roll_28_std',\n",
       "  'sales_roll_56_mean',\n",
       "  'sales_roll_56_std',\n",
       "  'sales_ewm_7',\n",
       "  'sales_ewm_14',\n",
       "  'sales_ewm_28',\n",
       "  'time_index',\n",
       "  'sales_velocity',\n",
       "  'sales_acceleration',\n",
       "  'sales_ratio_to_7d_avg',\n",
       "  'sales_ratio_to_28d_avg',\n",
       "  'days_since_first_sale',\n",
       "  'zero_sales_flag',\n",
       "  'consecutive_zero_days',\n",
       "  'dayofweek',\n",
       "  'weekofyear',\n",
       "  'sales_lag_30',\n",
       "  'sales_roll_3_mean',\n",
       "  'sales_roll_3_std',\n",
       "  'sales_roll_3_min',\n",
       "  'sales_roll_3_max',\n",
       "  'sales_roll_3_median',\n",
       "  'sales_roll_7_min',\n",
       "  'sales_roll_7_max',\n",
       "  'sales_roll_7_median',\n",
       "  'sales_roll_14_min',\n",
       "  'sales_roll_14_max',\n",
       "  'sales_roll_14_median',\n",
       "  'sales_roll_21_mean',\n",
       "  'sales_roll_21_std',\n",
       "  'sales_roll_21_min',\n",
       "  'sales_roll_21_max',\n",
       "  'sales_roll_21_median',\n",
       "  'sales_roll_30_mean',\n",
       "  'sales_roll_30_std',\n",
       "  'sales_roll_30_min',\n",
       "  'sales_roll_30_max',\n",
       "  'sales_roll_30_median',\n",
       "  'month_sin',\n",
       "  'month_cos',\n",
       "  'dow_sin',\n",
       "  'dow_cos',\n",
       "  'store_id_target_encoded'],\n",
       " 'importance': array([  0,   0,   0,   0,   0,   0,   1,   7,   0,   1,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   2,   0,   0,   0,   0,   0,  11,\n",
       "        116,   0,   0,   0,   0,   0,   0,   1,   0,   0,   0,   3,   0,\n",
       "          0,   0,  11,   0,   0,   0,   0,   0,   0,   0,   0,   0, 148,\n",
       "         74,   0,   1,   0,   0,   0,   2,   4,   0,  44,   8,  19,   6,\n",
       "        244,  17,  11,   7, 335,   8,   5,  10, 180, 143, 254, 262,   0,\n",
       "          0,   0,   0,   0,   0, 162, 108,   4, 706,  38,   0,   6,   0,\n",
       "          0,   3,   0,  13,   1,   0,  12,   0,   6,   0,   0,   4,   0,\n",
       "          0,   2,   0,   0,   0])}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_importance =({\n",
    "                'feature': feature_cols,\n",
    "                'importance': model_light.feature_importances_\n",
    "            })\n",
    "lgb_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81a58642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-09-30 01:41:38,381 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müöÄ Starting training for XGBoost model...\u001b[0m\n",
      "[ 2025-09-30 01:41:38,422 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚öôÔ∏è Using tree_method: `hist`\u001b[0m\n",
      "[ 2025-09-30 01:41:38,424 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müì¶ Using config-defined hyperparameters for XGBoost...\u001b[0m\n",
      "[ 2025-09-30 01:41:38,428 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Loaded XGBoost params: {'n_estimators': 100, 'max_depth': 6, 'learning_rate': 0.1, 'objective': 'reg:squarederror', 'random_state': 42}\u001b[0m\n",
      "[ 2025-09-30 01:41:38,431 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müõ†Ô∏è Training final XGBoost model...\u001b[0m\n",
      "[0]\tvalidation_0-rmse:2.29048\n",
      "[1]\tvalidation_0-rmse:2.06796\n",
      "[2]\tvalidation_0-rmse:1.86772\n",
      "[3]\tvalidation_0-rmse:1.68659\n",
      "[4]\tvalidation_0-rmse:1.52195\n",
      "[5]\tvalidation_0-rmse:1.37407\n",
      "[6]\tvalidation_0-rmse:1.24226\n",
      "[7]\tvalidation_0-rmse:1.12233\n",
      "[8]\tvalidation_0-rmse:1.01449\n",
      "[9]\tvalidation_0-rmse:0.91729\n",
      "[10]\tvalidation_0-rmse:0.82957\n",
      "[11]\tvalidation_0-rmse:0.75128\n",
      "[12]\tvalidation_0-rmse:0.68052\n",
      "[13]\tvalidation_0-rmse:0.61559\n",
      "[14]\tvalidation_0-rmse:0.55765\n",
      "[15]\tvalidation_0-rmse:0.50591\n",
      "[16]\tvalidation_0-rmse:0.45936\n",
      "[17]\tvalidation_0-rmse:0.41790\n",
      "[18]\tvalidation_0-rmse:0.38034\n",
      "[19]\tvalidation_0-rmse:0.34668\n",
      "[20]\tvalidation_0-rmse:0.31650\n",
      "[21]\tvalidation_0-rmse:0.28984\n",
      "[22]\tvalidation_0-rmse:0.26591\n",
      "[23]\tvalidation_0-rmse:0.24470\n",
      "[24]\tvalidation_0-rmse:0.22589\n",
      "[25]\tvalidation_0-rmse:0.20929\n",
      "[26]\tvalidation_0-rmse:0.19401\n",
      "[27]\tvalidation_0-rmse:0.18054\n",
      "[28]\tvalidation_0-rmse:0.16857\n",
      "[29]\tvalidation_0-rmse:0.15819\n",
      "[30]\tvalidation_0-rmse:0.14904\n",
      "[31]\tvalidation_0-rmse:0.14116\n",
      "[32]\tvalidation_0-rmse:0.13401\n",
      "[33]\tvalidation_0-rmse:0.12806\n",
      "[34]\tvalidation_0-rmse:0.12265\n",
      "[35]\tvalidation_0-rmse:0.11808\n",
      "[36]\tvalidation_0-rmse:0.11382\n",
      "[37]\tvalidation_0-rmse:0.11023\n",
      "[38]\tvalidation_0-rmse:0.10713\n",
      "[39]\tvalidation_0-rmse:0.10452\n",
      "[40]\tvalidation_0-rmse:0.10209\n",
      "[41]\tvalidation_0-rmse:0.10004\n",
      "[42]\tvalidation_0-rmse:0.09808\n",
      "[43]\tvalidation_0-rmse:0.09631\n",
      "[44]\tvalidation_0-rmse:0.09469\n",
      "[45]\tvalidation_0-rmse:0.09310\n",
      "[46]\tvalidation_0-rmse:0.09212\n",
      "[47]\tvalidation_0-rmse:0.09101\n",
      "[48]\tvalidation_0-rmse:0.09011\n",
      "[49]\tvalidation_0-rmse:0.08935\n",
      "[50]\tvalidation_0-rmse:0.08864\n",
      "[51]\tvalidation_0-rmse:0.08785\n",
      "[52]\tvalidation_0-rmse:0.08730\n",
      "[53]\tvalidation_0-rmse:0.08655\n",
      "[54]\tvalidation_0-rmse:0.08577\n",
      "[55]\tvalidation_0-rmse:0.08537\n",
      "[56]\tvalidation_0-rmse:0.08482\n",
      "[57]\tvalidation_0-rmse:0.08424\n",
      "[58]\tvalidation_0-rmse:0.08387\n",
      "[59]\tvalidation_0-rmse:0.08346\n",
      "[60]\tvalidation_0-rmse:0.08283\n",
      "[61]\tvalidation_0-rmse:0.08258\n",
      "[62]\tvalidation_0-rmse:0.08239\n",
      "[63]\tvalidation_0-rmse:0.08194\n",
      "[64]\tvalidation_0-rmse:0.08167\n",
      "[65]\tvalidation_0-rmse:0.08146\n",
      "[66]\tvalidation_0-rmse:0.08090\n",
      "[67]\tvalidation_0-rmse:0.08093\n",
      "[68]\tvalidation_0-rmse:0.08067\n",
      "[69]\tvalidation_0-rmse:0.08017\n",
      "[70]\tvalidation_0-rmse:0.07996\n",
      "[71]\tvalidation_0-rmse:0.07948\n",
      "[72]\tvalidation_0-rmse:0.07939\n",
      "[73]\tvalidation_0-rmse:0.07923\n",
      "[74]\tvalidation_0-rmse:0.07900\n",
      "[75]\tvalidation_0-rmse:0.07882\n",
      "[76]\tvalidation_0-rmse:0.07866\n",
      "[77]\tvalidation_0-rmse:0.07837\n",
      "[78]\tvalidation_0-rmse:0.07817\n",
      "[79]\tvalidation_0-rmse:0.07790\n",
      "[80]\tvalidation_0-rmse:0.07781\n",
      "[81]\tvalidation_0-rmse:0.07746\n",
      "[82]\tvalidation_0-rmse:0.07737\n",
      "[83]\tvalidation_0-rmse:0.07725\n",
      "[84]\tvalidation_0-rmse:0.07694\n",
      "[85]\tvalidation_0-rmse:0.07683\n",
      "[86]\tvalidation_0-rmse:0.07678\n",
      "[87]\tvalidation_0-rmse:0.07654\n",
      "[88]\tvalidation_0-rmse:0.07642\n",
      "[89]\tvalidation_0-rmse:0.07639\n",
      "[90]\tvalidation_0-rmse:0.07633\n",
      "[91]\tvalidation_0-rmse:0.07609\n",
      "[92]\tvalidation_0-rmse:0.07575\n",
      "[93]\tvalidation_0-rmse:0.07563\n",
      "[94]\tvalidation_0-rmse:0.07561\n",
      "[95]\tvalidation_0-rmse:0.07548\n",
      "[96]\tvalidation_0-rmse:0.07544\n",
      "[97]\tvalidation_0-rmse:0.07522\n",
      "[98]\tvalidation_0-rmse:0.07494\n",
      "[99]\tvalidation_0-rmse:0.07475\n",
      "[ 2025-09-30 01:41:45,478 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32mGenerating SHAP explanations for XGBoost...\u001b[0m\n",
      "[ 2025-09-30 01:41:45,481 ] src.visualizations.shap_visualizer - \u001b[32mINFO\u001b[0m - \u001b[32mInitialized SHAP explainer for lightgbm\u001b[0m\n",
      "[ 2025-09-30 01:41:45,522 ] src.visualizations.shap_visualizer - \u001b[32mINFO\u001b[0m - \u001b[32mFitting SHAP explainer...\u001b[0m\n",
      "[ 2025-09-30 01:41:45,597 ] src.visualizations.shap_visualizer - \u001b[32mINFO\u001b[0m - \u001b[32mCreated TreeExplainer for LightGBM\u001b[0m\n",
      "[ 2025-09-30 01:41:45,600 ] src.visualizations.shap_visualizer - \u001b[32mINFO\u001b[0m - \u001b[32mSHAP explainer fitted with 100 background samples\u001b[0m\n",
      "[ 2025-09-30 01:41:45,602 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32mSHAP explainer fitted for XGBoost\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model_xgb = trainer.train_xgboost(X_train, y_train, X_val, y_val, use_optuna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4e9ef998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-09-30 00:02:23,872 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müìä Calculating model evaluation metrics...\u001b[0m\n",
      "[ 2025-09-30 00:02:23,951 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Metrics calculated: {'rmse': 0.1025, 'mae': 0.0249, 'mape': 1.9689, 'r2': 0.9987}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "xgp_pred = model_xgb.predict(X_test)\n",
    "xgb_metric = trainer.calculate_metrics(y_test,xgp_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dde6c0da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rmse': 0.1025, 'mae': 0.0249, 'mape': 1.9689, 'r2': 0.9987}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a2682194",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = ({\n",
    "                'feature': feature_cols,\n",
    "                'importance': model_xgb.feature_importances_\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "13500bfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature': ['id',\n",
       "  'item_id',\n",
       "  'dept_id',\n",
       "  'cat_id',\n",
       "  'store_id',\n",
       "  'state_id',\n",
       "  'd',\n",
       "  'sales',\n",
       "  'date',\n",
       "  'wm_yr_wk',\n",
       "  'weekday',\n",
       "  'wday',\n",
       "  'month',\n",
       "  'year',\n",
       "  'event_name_1',\n",
       "  'event_type_1',\n",
       "  'event_name_2',\n",
       "  'event_type_2',\n",
       "  'snap_CA',\n",
       "  'snap_TX',\n",
       "  'snap_WI',\n",
       "  'day',\n",
       "  'quarter',\n",
       "  'week_of_year',\n",
       "  'is_weekend',\n",
       "  'has_event',\n",
       "  'snap_any',\n",
       "  'sell_price',\n",
       "  'revenue',\n",
       "  'is_holiday',\n",
       "  'snap_benefit_period',\n",
       "  'price_lag_1',\n",
       "  'price_change_1d',\n",
       "  'price_increased_1d',\n",
       "  'price_decreased_1d',\n",
       "  'price_lag_7',\n",
       "  'price_change_7d',\n",
       "  'price_increased_7d',\n",
       "  'price_decreased_7d',\n",
       "  'price_lag_14',\n",
       "  'price_change_14d',\n",
       "  'price_increased_14d',\n",
       "  'price_decreased_14d',\n",
       "  'price_lag_28',\n",
       "  'price_change_28d',\n",
       "  'price_increased_28d',\n",
       "  'price_decreased_28d',\n",
       "  'price_volatility_7d',\n",
       "  'price_volatility_28d',\n",
       "  'event_sporting',\n",
       "  'event_cultural',\n",
       "  'event_national',\n",
       "  'event_religious',\n",
       "  'sales_lag_1',\n",
       "  'sales_lag_2',\n",
       "  'sales_lag_3',\n",
       "  'sales_lag_7',\n",
       "  'sales_lag_14',\n",
       "  'sales_lag_21',\n",
       "  'sales_lag_28',\n",
       "  'revenue_lag_7',\n",
       "  'revenue_lag_14',\n",
       "  'revenue_lag_28',\n",
       "  'sales_roll_7_mean',\n",
       "  'sales_roll_7_std',\n",
       "  'sales_roll_14_mean',\n",
       "  'sales_roll_14_std',\n",
       "  'sales_roll_28_mean',\n",
       "  'sales_roll_28_std',\n",
       "  'sales_roll_56_mean',\n",
       "  'sales_roll_56_std',\n",
       "  'sales_ewm_7',\n",
       "  'sales_ewm_14',\n",
       "  'sales_ewm_28',\n",
       "  'time_index',\n",
       "  'sales_velocity',\n",
       "  'sales_acceleration',\n",
       "  'sales_ratio_to_7d_avg',\n",
       "  'sales_ratio_to_28d_avg',\n",
       "  'days_since_first_sale',\n",
       "  'zero_sales_flag',\n",
       "  'consecutive_zero_days',\n",
       "  'dayofweek',\n",
       "  'weekofyear',\n",
       "  'sales_lag_30',\n",
       "  'sales_roll_3_mean',\n",
       "  'sales_roll_3_std',\n",
       "  'sales_roll_3_min',\n",
       "  'sales_roll_3_max',\n",
       "  'sales_roll_3_median',\n",
       "  'sales_roll_7_min',\n",
       "  'sales_roll_7_max',\n",
       "  'sales_roll_7_median',\n",
       "  'sales_roll_14_min',\n",
       "  'sales_roll_14_max',\n",
       "  'sales_roll_14_median',\n",
       "  'sales_roll_21_mean',\n",
       "  'sales_roll_21_std',\n",
       "  'sales_roll_21_min',\n",
       "  'sales_roll_21_max',\n",
       "  'sales_roll_21_median',\n",
       "  'sales_roll_30_mean',\n",
       "  'sales_roll_30_std',\n",
       "  'sales_roll_30_min',\n",
       "  'sales_roll_30_max',\n",
       "  'sales_roll_30_median',\n",
       "  'month_sin',\n",
       "  'month_cos',\n",
       "  'dow_sin',\n",
       "  'dow_cos',\n",
       "  'store_id_target_encoded'],\n",
       " 'importance': array([6.2804884e-06, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 1.0735953e-05, 5.9981408e-05,\n",
       "        6.3641496e-06, 9.7796019e-06, 1.4110699e-05, 0.0000000e+00,\n",
       "        9.2104774e-06, 0.0000000e+00, 1.6822252e-05, 0.0000000e+00,\n",
       "        5.0963732e-05, 5.1373052e-05, 1.8432402e-05, 3.9379782e-05,\n",
       "        0.0000000e+00, 3.8742321e-05, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 1.5864887e-04, 4.2436374e-03, 0.0000000e+00,\n",
       "        0.0000000e+00, 2.0784606e-05, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 4.3834181e-04, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 3.4614114e-04, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 1.1670704e-04, 1.6439972e-05, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 3.1971787e-05, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 9.3542266e-04,\n",
       "        1.4114864e-04, 9.1415577e-06, 1.3629985e-05, 0.0000000e+00,\n",
       "        1.6353344e-05, 7.8855874e-06, 3.6171397e-05, 1.3362046e-05,\n",
       "        5.6023746e-05, 2.9127049e-04, 1.1573720e-04, 3.6101392e-05,\n",
       "        3.5080815e-05, 4.6629859e-03, 1.0575110e-04, 1.2890180e-04,\n",
       "        2.9398224e-04, 5.2684747e-02, 3.0473645e-03, 1.7276892e-04,\n",
       "        1.3380450e-04, 3.8563120e-01, 1.0246681e-04, 6.8550527e-02,\n",
       "        5.6559093e-02, 0.0000000e+00, 2.3321186e-01, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 4.0297586e-04,\n",
       "        1.3048577e-01, 4.3051496e-05, 5.5787668e-02, 1.2135854e-04,\n",
       "        0.0000000e+00, 5.9497866e-05, 1.5419293e-05, 0.0000000e+00,\n",
       "        1.2488739e-04, 6.8344584e-06, 5.4110616e-05, 2.8739574e-05,\n",
       "        0.0000000e+00, 4.2190470e-05, 1.6082635e-05, 2.6002079e-05,\n",
       "        6.2232693e-06, 0.0000000e+00, 2.0211917e-05, 0.0000000e+00,\n",
       "        1.7378003e-05, 2.2147611e-05, 1.1485146e-05, 1.0240833e-05,\n",
       "        0.0000000e+00], dtype=float32)}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d69d4542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-09-30 00:07:34,982 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32mTraining Prophet model\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:08:18 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-09-30 00:08:18,272 ] cmdstanpy - \u001b[32mINFO\u001b[0m - \u001b[32mChain [1] start processing\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:13:18 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-09-30 00:13:18,668 ] cmdstanpy - \u001b[32mINFO\u001b[0m - \u001b[32mChain [1] done processing\u001b[0m\n",
      "[ 2025-09-30 00:13:23,636 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32mProphet Val RMSE: 2.5317, MAE: 1.1956, R2: 0.0011\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model_prophet = trainer.train_prophet(train_df,value_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "489e9642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-10-01 16:21:19,920 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müöÄ Starting full model training pipeline...\u001b[0m\n",
      "[ 2025-10-01 16:21:20,931 ] src.utils.mlflow_utils - \u001b[32mINFO\u001b[0m - \u001b[32mStarted MLflow run: 9f1e2b0dfcb44bcc92772b5fb47fd35d\u001b[0m\n",
      "[ 2025-10-01 16:21:20,938 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müéØ MLflow run started with run_id=9f1e2b0dfcb44bcc92772b5fb47fd35d\u001b[0m\n",
      "[ 2025-10-01 16:21:20,940 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müßπ Preprocessing features and target variables...\u001b[0m\n",
      "[ 2025-10-01 16:21:20,943 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müîÑ Starting feature preprocessing...\u001b[0m\n",
      "[ 2025-10-01 16:21:22,894 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'id' in Train set.\u001b[0m\n",
      "[ 2025-10-01 16:21:22,984 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'id' in Val set.\u001b[0m\n",
      "[ 2025-10-01 16:21:23,132 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'id' in Test set.\u001b[0m\n",
      "[ 2025-10-01 16:21:23,810 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'item_id' in Train set.\u001b[0m\n",
      "[ 2025-10-01 16:21:23,883 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'item_id' in Val set.\u001b[0m\n",
      "[ 2025-10-01 16:21:23,982 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'item_id' in Test set.\u001b[0m\n",
      "[ 2025-10-01 16:21:24,416 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'dept_id' in Train set.\u001b[0m\n",
      "[ 2025-10-01 16:21:24,486 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'dept_id' in Val set.\u001b[0m\n",
      "[ 2025-10-01 16:21:24,564 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'dept_id' in Test set.\u001b[0m\n",
      "[ 2025-10-01 16:21:25,131 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'cat_id' in Train set.\u001b[0m\n",
      "[ 2025-10-01 16:21:25,175 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'cat_id' in Val set.\u001b[0m\n",
      "[ 2025-10-01 16:21:25,276 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'cat_id' in Test set.\u001b[0m\n",
      "[ 2025-10-01 16:21:25,607 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'store_id' in Train set.\u001b[0m\n",
      "[ 2025-10-01 16:21:25,670 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'store_id' in Val set.\u001b[0m\n",
      "[ 2025-10-01 16:21:25,749 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'store_id' in Test set.\u001b[0m\n",
      "[ 2025-10-01 16:21:26,103 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'state_id' in Train set.\u001b[0m\n",
      "[ 2025-10-01 16:21:26,143 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'state_id' in Val set.\u001b[0m\n",
      "[ 2025-10-01 16:21:26,195 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'state_id' in Test set.\u001b[0m\n",
      "[ 2025-10-01 16:21:26,574 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'd' in Train set.\u001b[0m\n",
      "[ 2025-10-01 16:21:26,643 ] __main__ - \u001b[33mWARNING\u001b[0m - \u001b[33m‚ö†Ô∏è Unseen labels in column 'd': ['d_1359' 'd_1360' 'd_1361' 'd_1362' 'd_1363' 'd_1364' 'd_1365' 'd_1366'\n",
      " 'd_1367' 'd_1368' 'd_1369' 'd_1370' 'd_1371' 'd_1372' 'd_1373' 'd_1374'\n",
      " 'd_1375' 'd_1376' 'd_1377' 'd_1378' 'd_1379' 'd_1380' 'd_1381' 'd_1382'\n",
      " 'd_1383' 'd_1384' 'd_1385' 'd_1386' 'd_1387' 'd_1388' 'd_1389' 'd_1390'\n",
      " 'd_1391' 'd_1392' 'd_1393' 'd_1394' 'd_1395' 'd_1396' 'd_1397' 'd_1398'\n",
      " 'd_1399' 'd_1400' 'd_1401' 'd_1402' 'd_1403' 'd_1404' 'd_1405' 'd_1406'\n",
      " 'd_1407' 'd_1408' 'd_1409' 'd_1410' 'd_1411' 'd_1412' 'd_1413' 'd_1414'\n",
      " 'd_1415' 'd_1416' 'd_1417' 'd_1418' 'd_1419' 'd_1420' 'd_1421' 'd_1422'\n",
      " 'd_1423' 'd_1424' 'd_1425' 'd_1426' 'd_1427' 'd_1428' 'd_1429' 'd_1430'\n",
      " 'd_1431' 'd_1432' 'd_1433' 'd_1434' 'd_1435' 'd_1436' 'd_1437' 'd_1438'\n",
      " 'd_1439' 'd_1440' 'd_1441' 'd_1442' 'd_1443' 'd_1444' 'd_1445' 'd_1446'\n",
      " 'd_1447' 'd_1448' 'd_1449' 'd_1450' 'd_1451' 'd_1452' 'd_1453' 'd_1454'\n",
      " 'd_1455' 'd_1456' 'd_1457' 'd_1458' 'd_1459' 'd_1460' 'd_1461' 'd_1462'\n",
      " 'd_1463' 'd_1464' 'd_1465' 'd_1466' 'd_1467' 'd_1468' 'd_1469' 'd_1470'\n",
      " 'd_1471' 'd_1472' 'd_1473' 'd_1474' 'd_1475' 'd_1476' 'd_1477' 'd_1478'\n",
      " 'd_1479' 'd_1480' 'd_1481' 'd_1482' 'd_1483' 'd_1484' 'd_1485' 'd_1486'\n",
      " 'd_1487' 'd_1488' 'd_1489' 'd_1490' 'd_1491' 'd_1492' 'd_1493' 'd_1494'\n",
      " 'd_1495' 'd_1496' 'd_1497' 'd_1498' 'd_1499' 'd_1500' 'd_1501' 'd_1502'\n",
      " 'd_1503' 'd_1504' 'd_1505' 'd_1506' 'd_1507' 'd_1508' 'd_1509' 'd_1510'\n",
      " 'd_1511' 'd_1512' 'd_1513' 'd_1514' 'd_1515' 'd_1516' 'd_1517' 'd_1518'\n",
      " 'd_1519' 'd_1520' 'd_1521' 'd_1522' 'd_1523' 'd_1524' 'd_1525' 'd_1526'\n",
      " 'd_1527' 'd_1528' 'd_1529' 'd_1530' 'd_1531' 'd_1532' 'd_1533' 'd_1534'\n",
      " 'd_1535' 'd_1536' 'd_1537' 'd_1538' 'd_1539' 'd_1540' 'd_1541' 'd_1542'\n",
      " 'd_1543' 'd_1544' 'd_1545' 'd_1546' 'd_1547' 'd_1548' 'd_1549' 'd_1550'\n",
      " 'd_1551' 'd_1552' 'd_1553']\u001b[0m\n",
      "[ 2025-10-01 16:21:26,684 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'd' in Val set.\u001b[0m\n",
      "[ 2025-10-01 16:21:26,765 ] __main__ - \u001b[33mWARNING\u001b[0m - \u001b[33m‚ö†Ô∏è Unseen labels in column 'd': ['d_1553' 'd_1554' 'd_1555' 'd_1556' 'd_1557' 'd_1558' 'd_1559' 'd_1560'\n",
      " 'd_1561' 'd_1562' 'd_1563' 'd_1564' 'd_1565' 'd_1566' 'd_1567' 'd_1568'\n",
      " 'd_1569' 'd_1570' 'd_1571' 'd_1572' 'd_1573' 'd_1574' 'd_1575' 'd_1576'\n",
      " 'd_1577' 'd_1578' 'd_1579' 'd_1580' 'd_1581' 'd_1582' 'd_1583' 'd_1584'\n",
      " 'd_1585' 'd_1586' 'd_1587' 'd_1588' 'd_1589' 'd_1590' 'd_1591' 'd_1592'\n",
      " 'd_1593' 'd_1594' 'd_1595' 'd_1596' 'd_1597' 'd_1598' 'd_1599' 'd_1600'\n",
      " 'd_1601' 'd_1602' 'd_1603' 'd_1604' 'd_1605' 'd_1606' 'd_1607' 'd_1608'\n",
      " 'd_1609' 'd_1610' 'd_1611' 'd_1612' 'd_1613' 'd_1614' 'd_1615' 'd_1616'\n",
      " 'd_1617' 'd_1618' 'd_1619' 'd_1620' 'd_1621' 'd_1622' 'd_1623' 'd_1624'\n",
      " 'd_1625' 'd_1626' 'd_1627' 'd_1628' 'd_1629' 'd_1630' 'd_1631' 'd_1632'\n",
      " 'd_1633' 'd_1634' 'd_1635' 'd_1636' 'd_1637' 'd_1638' 'd_1639' 'd_1640'\n",
      " 'd_1641' 'd_1642' 'd_1643' 'd_1644' 'd_1645' 'd_1646' 'd_1647' 'd_1648'\n",
      " 'd_1649' 'd_1650' 'd_1651' 'd_1652' 'd_1653' 'd_1654' 'd_1655' 'd_1656'\n",
      " 'd_1657' 'd_1658' 'd_1659' 'd_1660' 'd_1661' 'd_1662' 'd_1663' 'd_1664'\n",
      " 'd_1665' 'd_1666' 'd_1667' 'd_1668' 'd_1669' 'd_1670' 'd_1671' 'd_1672'\n",
      " 'd_1673' 'd_1674' 'd_1675' 'd_1676' 'd_1677' 'd_1678' 'd_1679' 'd_1680'\n",
      " 'd_1681' 'd_1682' 'd_1683' 'd_1684' 'd_1685' 'd_1686' 'd_1687' 'd_1688'\n",
      " 'd_1689' 'd_1690' 'd_1691' 'd_1692' 'd_1693' 'd_1694' 'd_1695' 'd_1696'\n",
      " 'd_1697' 'd_1698' 'd_1699' 'd_1700' 'd_1701' 'd_1702' 'd_1703' 'd_1704'\n",
      " 'd_1705' 'd_1706' 'd_1707' 'd_1708' 'd_1709' 'd_1710' 'd_1711' 'd_1712'\n",
      " 'd_1713' 'd_1714' 'd_1715' 'd_1716' 'd_1717' 'd_1718' 'd_1719' 'd_1720'\n",
      " 'd_1721' 'd_1722' 'd_1723' 'd_1724' 'd_1725' 'd_1726' 'd_1727' 'd_1728'\n",
      " 'd_1729' 'd_1730' 'd_1731' 'd_1732' 'd_1733' 'd_1734' 'd_1735' 'd_1736'\n",
      " 'd_1737' 'd_1738' 'd_1739' 'd_1740' 'd_1741' 'd_1742' 'd_1743' 'd_1744'\n",
      " 'd_1745' 'd_1746' 'd_1747' 'd_1748' 'd_1749' 'd_1750' 'd_1751' 'd_1752'\n",
      " 'd_1753' 'd_1754' 'd_1755' 'd_1756' 'd_1757' 'd_1758' 'd_1759' 'd_1760'\n",
      " 'd_1761' 'd_1762' 'd_1763' 'd_1764' 'd_1765' 'd_1766' 'd_1767' 'd_1768'\n",
      " 'd_1769' 'd_1770' 'd_1771' 'd_1772' 'd_1773' 'd_1774' 'd_1775' 'd_1776'\n",
      " 'd_1777' 'd_1778' 'd_1779' 'd_1780' 'd_1781' 'd_1782' 'd_1783' 'd_1784'\n",
      " 'd_1785' 'd_1786' 'd_1787' 'd_1788' 'd_1789' 'd_1790' 'd_1791' 'd_1792'\n",
      " 'd_1793' 'd_1794' 'd_1795' 'd_1796' 'd_1797' 'd_1798' 'd_1799' 'd_1800'\n",
      " 'd_1801' 'd_1802' 'd_1803' 'd_1804' 'd_1805' 'd_1806' 'd_1807' 'd_1808'\n",
      " 'd_1809' 'd_1810' 'd_1811' 'd_1812' 'd_1813' 'd_1814' 'd_1815' 'd_1816'\n",
      " 'd_1817' 'd_1818' 'd_1819' 'd_1820' 'd_1821' 'd_1822' 'd_1823' 'd_1824'\n",
      " 'd_1825' 'd_1826' 'd_1827' 'd_1828' 'd_1829' 'd_1830' 'd_1831' 'd_1832'\n",
      " 'd_1833' 'd_1834' 'd_1835' 'd_1836' 'd_1837' 'd_1838' 'd_1839' 'd_1840'\n",
      " 'd_1841' 'd_1842' 'd_1843' 'd_1844' 'd_1845' 'd_1846' 'd_1847' 'd_1848'\n",
      " 'd_1849' 'd_1850' 'd_1851' 'd_1852' 'd_1853' 'd_1854' 'd_1855' 'd_1856'\n",
      " 'd_1857' 'd_1858' 'd_1859' 'd_1860' 'd_1861' 'd_1862' 'd_1863' 'd_1864'\n",
      " 'd_1865' 'd_1866' 'd_1867' 'd_1868' 'd_1869' 'd_1870' 'd_1871' 'd_1872'\n",
      " 'd_1873' 'd_1874' 'd_1875' 'd_1876' 'd_1877' 'd_1878' 'd_1879' 'd_1880'\n",
      " 'd_1881' 'd_1882' 'd_1883' 'd_1884' 'd_1885' 'd_1886' 'd_1887' 'd_1888'\n",
      " 'd_1889' 'd_1890' 'd_1891' 'd_1892' 'd_1893' 'd_1894' 'd_1895' 'd_1896'\n",
      " 'd_1897' 'd_1898' 'd_1899' 'd_1900' 'd_1901' 'd_1902' 'd_1903' 'd_1904'\n",
      " 'd_1905' 'd_1906' 'd_1907' 'd_1908' 'd_1909' 'd_1910' 'd_1911' 'd_1912'\n",
      " 'd_1913' 'd_1914' 'd_1915' 'd_1916' 'd_1917' 'd_1918' 'd_1919' 'd_1920'\n",
      " 'd_1921' 'd_1922' 'd_1923' 'd_1924' 'd_1925' 'd_1926' 'd_1927' 'd_1928'\n",
      " 'd_1929' 'd_1930' 'd_1931' 'd_1932' 'd_1933' 'd_1934' 'd_1935' 'd_1936'\n",
      " 'd_1937' 'd_1938' 'd_1939' 'd_1940' 'd_1941']\u001b[0m\n",
      "[ 2025-10-01 16:21:26,824 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'd' in Test set.\u001b[0m\n",
      "[ 2025-10-01 16:21:27,476 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'event_name_1' in Train set.\u001b[0m\n",
      "[ 2025-10-01 16:21:27,562 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'event_name_1' in Val set.\u001b[0m\n",
      "[ 2025-10-01 16:21:27,710 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'event_name_1' in Test set.\u001b[0m\n",
      "[ 2025-10-01 16:21:28,521 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'event_type_1' in Train set.\u001b[0m\n",
      "[ 2025-10-01 16:21:28,592 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'event_type_1' in Val set.\u001b[0m\n",
      "[ 2025-10-01 16:21:28,690 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'event_type_1' in Test set.\u001b[0m\n",
      "[ 2025-10-01 16:21:29,317 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'event_name_2' in Train set.\u001b[0m\n",
      "[ 2025-10-01 16:21:29,409 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'event_name_2' in Val set.\u001b[0m\n",
      "[ 2025-10-01 16:21:29,505 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'event_name_2' in Test set.\u001b[0m\n",
      "[ 2025-10-01 16:21:30,173 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'event_type_2' in Train set.\u001b[0m\n",
      "[ 2025-10-01 16:21:30,208 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'event_type_2' in Val set.\u001b[0m\n",
      "[ 2025-10-01 16:21:30,322 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'event_type_2' in Test set.\u001b[0m\n",
      "[ 2025-10-01 16:21:31,181 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'store_id_target_encoded' in Train set.\u001b[0m\n",
      "[ 2025-10-01 16:21:31,291 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'store_id_target_encoded' in Val set.\u001b[0m\n",
      "[ 2025-10-01 16:21:31,413 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'store_id_target_encoded' in Test set.\u001b[0m\n",
      "[ 2025-10-01 16:21:38,407 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Preprocessing complete. Total features used: 109 üß†\u001b[0m\n",
      "[ 2025-10-01 16:21:38,466 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müìä Data sizes - Train: 230073, Val: 32867, Test: 65736\u001b[0m\n",
      "[ 2025-10-01 16:21:39,410 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müî• Training XGBoost model...\u001b[0m\n",
      "[ 2025-10-01 16:21:39,413 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müöÄ Starting training for XGBoost model...\u001b[0m\n",
      "[ 2025-10-01 16:21:39,435 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚öôÔ∏è Using tree_method: `hist`\u001b[0m\n",
      "[ 2025-10-01 16:21:39,438 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müì¶ Using config-defined hyperparameters for XGBoost...\u001b[0m\n",
      "[ 2025-10-01 16:21:39,441 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Loaded XGBoost params: {'n_estimators': 100, 'max_depth': 6, 'learning_rate': 0.1, 'objective': 'reg:squarederror', 'random_state': 42}\u001b[0m\n",
      "[ 2025-10-01 16:21:39,448 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müõ†Ô∏è Training final XGBoost model...\u001b[0m\n",
      "[0]\tvalidation_0-rmse:2.29048\n",
      "[1]\tvalidation_0-rmse:2.06796\n",
      "[2]\tvalidation_0-rmse:1.86772\n",
      "[3]\tvalidation_0-rmse:1.68659\n",
      "[4]\tvalidation_0-rmse:1.52195\n",
      "[5]\tvalidation_0-rmse:1.37407\n",
      "[6]\tvalidation_0-rmse:1.24226\n",
      "[7]\tvalidation_0-rmse:1.12233\n",
      "[8]\tvalidation_0-rmse:1.01449\n",
      "[9]\tvalidation_0-rmse:0.91729\n",
      "[10]\tvalidation_0-rmse:0.82957\n",
      "[11]\tvalidation_0-rmse:0.75128\n",
      "[12]\tvalidation_0-rmse:0.68052\n",
      "[13]\tvalidation_0-rmse:0.61559\n",
      "[14]\tvalidation_0-rmse:0.55765\n",
      "[15]\tvalidation_0-rmse:0.50591\n",
      "[16]\tvalidation_0-rmse:0.45936\n",
      "[17]\tvalidation_0-rmse:0.41790\n",
      "[18]\tvalidation_0-rmse:0.38034\n",
      "[19]\tvalidation_0-rmse:0.34668\n",
      "[20]\tvalidation_0-rmse:0.31650\n",
      "[21]\tvalidation_0-rmse:0.28984\n",
      "[22]\tvalidation_0-rmse:0.26591\n",
      "[23]\tvalidation_0-rmse:0.24470\n",
      "[24]\tvalidation_0-rmse:0.22589\n",
      "[25]\tvalidation_0-rmse:0.20929\n",
      "[26]\tvalidation_0-rmse:0.19401\n",
      "[27]\tvalidation_0-rmse:0.18054\n",
      "[28]\tvalidation_0-rmse:0.16857\n",
      "[29]\tvalidation_0-rmse:0.15819\n",
      "[30]\tvalidation_0-rmse:0.14904\n",
      "[31]\tvalidation_0-rmse:0.14116\n",
      "[32]\tvalidation_0-rmse:0.13401\n",
      "[33]\tvalidation_0-rmse:0.12806\n",
      "[34]\tvalidation_0-rmse:0.12265\n",
      "[35]\tvalidation_0-rmse:0.11808\n",
      "[36]\tvalidation_0-rmse:0.11382\n",
      "[37]\tvalidation_0-rmse:0.11023\n",
      "[38]\tvalidation_0-rmse:0.10713\n",
      "[39]\tvalidation_0-rmse:0.10452\n",
      "[40]\tvalidation_0-rmse:0.10209\n",
      "[41]\tvalidation_0-rmse:0.10004\n",
      "[42]\tvalidation_0-rmse:0.09808\n",
      "[43]\tvalidation_0-rmse:0.09631\n",
      "[44]\tvalidation_0-rmse:0.09469\n",
      "[45]\tvalidation_0-rmse:0.09310\n",
      "[46]\tvalidation_0-rmse:0.09212\n",
      "[47]\tvalidation_0-rmse:0.09101\n",
      "[48]\tvalidation_0-rmse:0.09011\n",
      "[49]\tvalidation_0-rmse:0.08935\n",
      "[50]\tvalidation_0-rmse:0.08864\n",
      "[51]\tvalidation_0-rmse:0.08785\n",
      "[52]\tvalidation_0-rmse:0.08730\n",
      "[53]\tvalidation_0-rmse:0.08655\n",
      "[54]\tvalidation_0-rmse:0.08577\n",
      "[55]\tvalidation_0-rmse:0.08537\n",
      "[56]\tvalidation_0-rmse:0.08482\n",
      "[57]\tvalidation_0-rmse:0.08424\n",
      "[58]\tvalidation_0-rmse:0.08387\n",
      "[59]\tvalidation_0-rmse:0.08346\n",
      "[60]\tvalidation_0-rmse:0.08283\n",
      "[61]\tvalidation_0-rmse:0.08258\n",
      "[62]\tvalidation_0-rmse:0.08239\n",
      "[63]\tvalidation_0-rmse:0.08194\n",
      "[64]\tvalidation_0-rmse:0.08167\n",
      "[65]\tvalidation_0-rmse:0.08146\n",
      "[66]\tvalidation_0-rmse:0.08090\n",
      "[67]\tvalidation_0-rmse:0.08093\n",
      "[68]\tvalidation_0-rmse:0.08067\n",
      "[69]\tvalidation_0-rmse:0.08017\n",
      "[70]\tvalidation_0-rmse:0.07996\n",
      "[71]\tvalidation_0-rmse:0.07948\n",
      "[72]\tvalidation_0-rmse:0.07939\n",
      "[73]\tvalidation_0-rmse:0.07923\n",
      "[74]\tvalidation_0-rmse:0.07900\n",
      "[75]\tvalidation_0-rmse:0.07882\n",
      "[76]\tvalidation_0-rmse:0.07866\n",
      "[77]\tvalidation_0-rmse:0.07837\n",
      "[78]\tvalidation_0-rmse:0.07817\n",
      "[79]\tvalidation_0-rmse:0.07790\n",
      "[80]\tvalidation_0-rmse:0.07781\n",
      "[81]\tvalidation_0-rmse:0.07746\n",
      "[82]\tvalidation_0-rmse:0.07737\n",
      "[83]\tvalidation_0-rmse:0.07725\n",
      "[84]\tvalidation_0-rmse:0.07694\n",
      "[85]\tvalidation_0-rmse:0.07683\n",
      "[86]\tvalidation_0-rmse:0.07678\n",
      "[87]\tvalidation_0-rmse:0.07654\n",
      "[88]\tvalidation_0-rmse:0.07642\n",
      "[89]\tvalidation_0-rmse:0.07639\n",
      "[90]\tvalidation_0-rmse:0.07633\n",
      "[91]\tvalidation_0-rmse:0.07609\n",
      "[92]\tvalidation_0-rmse:0.07575\n",
      "[93]\tvalidation_0-rmse:0.07563\n",
      "[94]\tvalidation_0-rmse:0.07561\n",
      "[95]\tvalidation_0-rmse:0.07548\n",
      "[96]\tvalidation_0-rmse:0.07544\n",
      "[97]\tvalidation_0-rmse:0.07522\n",
      "[98]\tvalidation_0-rmse:0.07494\n",
      "[99]\tvalidation_0-rmse:0.07475\n",
      "[ 2025-10-01 16:21:47,513 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32mGenerating SHAP explanations for XGBoost...\u001b[0m\n",
      "[ 2025-10-01 16:21:47,523 ] src.visualizations.shap_visualizer - \u001b[32mINFO\u001b[0m - \u001b[32mInitialized SHAP explainer for lightgbm\u001b[0m\n",
      "[ 2025-10-01 16:21:47,585 ] src.visualizations.shap_visualizer - \u001b[32mINFO\u001b[0m - \u001b[32mFitting SHAP explainer...\u001b[0m\n",
      "[ 2025-10-01 16:21:47,736 ] src.visualizations.shap_visualizer - \u001b[32mINFO\u001b[0m - \u001b[32mCreated TreeExplainer for LightGBM\u001b[0m\n",
      "[ 2025-10-01 16:21:47,739 ] src.visualizations.shap_visualizer - \u001b[32mINFO\u001b[0m - \u001b[32mSHAP explainer fitted with 100 background samples\u001b[0m\n",
      "[ 2025-10-01 16:21:47,741 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32mSHAP explainer fitted for XGBoost\u001b[0m\n",
      "[ 2025-10-01 16:21:47,745 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Model trained successfully. Best iteration: 99\u001b[0m\n",
      "[ 2025-10-01 16:21:47,806 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müìä Calculating model evaluation metrics...\u001b[0m\n",
      "[ 2025-10-01 16:21:47,831 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Metrics calculated: {'rmse': 0.1025, 'mae': 0.0249, 'mape': 1.9689, 'r2': 0.9987}\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\anaconda3\\envs\\salesenv\\lib\\site-packages\\xgboost\\sklearn.py:1028: UserWarning: [16:21:48] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\c_api\\c_api.cc:1427: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
      "  self.get_booster().save_model(fname)\n",
      "c:\\Users\\Administrator\\anaconda3\\envs\\salesenv\\lib\\site-packages\\_distutils_hack\\__init__.py:15: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Administrator\\anaconda3\\envs\\salesenv\\lib\\site-packages\\_distutils_hack\\__init__.py:30: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-10-01 16:22:03,379 ] src.utils.mlflow_utils - \u001b[32mINFO\u001b[0m - \u001b[32mSuccessfully logged XGBoost model 'xgboost'\u001b[0m\n",
      "[ 2025-10-01 16:22:03,432 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müåü Top XGBoost features:\n",
      "                   feature  importance\n",
      "73          sales_velocity    0.385631\n",
      "78         zero_sales_flag    0.233212\n",
      "84        sales_roll_3_std    0.130486\n",
      "75   sales_ratio_to_7d_avg    0.068551\n",
      "76  sales_ratio_to_28d_avg    0.056559\n",
      "86        sales_roll_3_max    0.055788\n",
      "69             sales_ewm_7    0.052685\n",
      "65      sales_roll_28_mean    0.004663\n",
      "26                 revenue    0.004244\n",
      "70            sales_ewm_14    0.003047\n",
      "51             sales_lag_1    0.000935\n",
      "33             price_lag_7    0.000438\n",
      "83       sales_roll_3_mean    0.000403\n",
      "37            price_lag_14    0.000346\n",
      "68       sales_roll_56_std    0.000294\n",
      "61       sales_roll_7_mean    0.000291\n",
      "71            sales_ewm_28    0.000173\n",
      "25              sell_price    0.000159\n",
      "52             sales_lag_2    0.000141\n",
      "72              time_index    0.000134\u001b[0m\n",
      "[ 2025-10-01 16:22:04,567 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ XGBoost training complete!\u001b[0m\n",
      "[ 2025-10-01 16:22:04,570 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müî• Training LightGBM model...\u001b[0m\n",
      "[ 2025-10-01 16:22:04,573 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚öôÔ∏è Starting LightGBM training...\u001b[0m\n",
      "[ 2025-10-01 16:22:04,577 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müì¶ Using static config parameters for LightGBM...\u001b[0m\n",
      "[ 2025-10-01 16:22:04,580 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Loaded config params: {'num_leaves': 31, 'learning_rate': 0.05, 'n_estimators': 100, 'objective': 'regression', 'random_state': 42}\u001b[0m\n",
      "[ 2025-10-01 16:22:04,589 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müõ†Ô∏è Training final LightGBM model...\u001b[0m\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029749 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9939\n",
      "[LightGBM] [Info] Number of data points in the train set: 230073, number of used features: 100\n",
      "[LightGBM] [Info] Start training from score 0.851578\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's l2: 0.0580903\n",
      "[100]\tvalid_0's l2: 0.0087255\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l2: 0.0087255\n",
      "[ 2025-10-01 16:22:12,765 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32mGenerating SHAP explanations for LightGBM...\u001b[0m\n",
      "[ 2025-10-01 16:22:12,774 ] src.visualizations.shap_visualizer - \u001b[32mINFO\u001b[0m - \u001b[32mInitialized SHAP explainer for lightgbm\u001b[0m\n",
      "[ 2025-10-01 16:22:12,829 ] src.visualizations.shap_visualizer - \u001b[32mINFO\u001b[0m - \u001b[32mFitting SHAP explainer...\u001b[0m\n",
      "[ 2025-10-01 16:22:13,067 ] src.visualizations.shap_visualizer - \u001b[32mINFO\u001b[0m - \u001b[32mCreated TreeExplainer for LightGBM\u001b[0m\n",
      "[ 2025-10-01 16:22:13,070 ] src.visualizations.shap_visualizer - \u001b[32mINFO\u001b[0m - \u001b[32mSHAP explainer fitted with 100 background samples\u001b[0m\n",
      "[ 2025-10-01 16:22:13,074 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ SHAP explainer fitted for LightGBM\u001b[0m\n",
      "[ 2025-10-01 16:22:13,075 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ LightGBM model trained and stored.\u001b[0m\n",
      "[ 2025-10-01 16:22:13,187 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müìä Calculating model evaluation metrics...\u001b[0m\n",
      "[ 2025-10-01 16:22:13,215 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Metrics calculated: {'rmse': 0.1335, 'mae': 0.0328, 'mape': 2.2987, 'r2': 0.9979}\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\anaconda3\\envs\\salesenv\\lib\\site-packages\\_distutils_hack\\__init__.py:15: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Administrator\\anaconda3\\envs\\salesenv\\lib\\site-packages\\_distutils_hack\\__init__.py:30: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-10-01 16:22:28,907 ] src.utils.mlflow_utils - \u001b[32mINFO\u001b[0m - \u001b[32mSuccessfully logged LightGBM model 'lightgbm'\u001b[0m\n",
      "[ 2025-10-01 16:22:28,930 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müåü Top LightGBM features:\n",
      "                   feature  importance\n",
      "86        sales_roll_3_max         706\n",
      "69             sales_ewm_7         335\n",
      "76  sales_ratio_to_28d_avg         262\n",
      "75   sales_ratio_to_7d_avg         254\n",
      "65      sales_roll_28_mean         244\n",
      "73          sales_velocity         180\n",
      "83       sales_roll_3_mean         162\n",
      "51             sales_lag_1         148\n",
      "74      sales_acceleration         143\n",
      "26                 revenue         116\n",
      "84        sales_roll_3_std         108\n",
      "52             sales_lag_2          74\n",
      "61       sales_roll_7_mean          44\n",
      "87     sales_roll_3_median          38\n",
      "63      sales_roll_14_mean          19\n",
      "66       sales_roll_28_std          17\n",
      "94      sales_roll_21_mean          13\n",
      "97       sales_roll_21_max          12\n",
      "41            price_lag_28          11\n",
      "25              sell_price          11\u001b[0m\n",
      "[ 2025-10-01 16:22:28,932 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ LightGBM training complete!\u001b[0m\n",
      "[ 2025-10-01 16:22:28,935 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚ÑπÔ∏è Prophet training skipped by config.\u001b[0m\n",
      "[ 2025-10-01 16:22:28,938 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müß© Building stacking/weighted ensemble...\u001b[0m\n",
      "[ 2025-10-01 16:22:29,072 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚öñÔ∏è Ensemble weights - XGBoost: 0.500, LightGBM: 0.500\u001b[0m\n",
      "[ 2025-10-01 16:22:29,078 ] src.models.advanced_ensemble - \u001b[32mINFO\u001b[0m - \u001b[32m‚öñÔ∏è Creating blended ensemble (optimized for rmse)\u001b[0m\n",
      "[ 2025-10-01 16:22:29,149 ] src.models.advanced_ensemble - \u001b[32mINFO\u001b[0m - \u001b[32müìä Blended Ensemble RMSE: 0.1044, MAE: 0.0262, R¬≤: 0.9987\u001b[0m\n",
      "[ 2025-10-01 16:22:29,151 ] src.models.advanced_ensemble - \u001b[32mINFO\u001b[0m - \u001b[32müî¢ Optimal blend weights: {'xgboost': 0.49999349491871187, 'lightgbm': 0.5000065050812882}\u001b[0m\n",
      "[ 2025-10-01 16:22:29,158 ] root - \u001b[32mINFO\u001b[0m - \u001b[32m‚öñÔ∏è Custom weights provided. Normalized weights: {'xgboost': 0.49999349491871187, 'lightgbm': 0.5000065050812882}\u001b[0m\n",
      "[ 2025-10-01 16:22:29,164 ] root - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Initialized EnsembleModel with models: ['xgboost', 'lightgbm']\u001b[0m\n",
      "[ 2025-10-01 16:22:29,168 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müìä Calculating model evaluation metrics...\u001b[0m\n",
      "[ 2025-10-01 16:22:29,188 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Metrics calculated: {'rmse': 0.1044, 'mae': 0.0262, 'mape': 1.9469, 'r2': 0.9987}\u001b[0m\n",
      "[ 2025-10-01 16:22:29,439 ] src.utils.mlflow_utils - \u001b[31mERROR\u001b[0m - \u001b[31mFailed to log model ensemble using MLflow's framework-specific methods: `python_model` must be a PythonModel instance or a callable object\u001b[0m\n",
      "[ 2025-10-01 16:22:29,441 ] src.utils.mlflow_utils - \u001b[32mINFO\u001b[0m - \u001b[32mFalling back to joblib serialization for model ensemble\u001b[0m\n",
      "[ 2025-10-01 16:22:30,059 ] src.utils.mlflow_utils - \u001b[32mINFO\u001b[0m - \u001b[32mSuccessfully saved ensemble model as a joblib artifact.\u001b[0m\n",
      "[ 2025-10-01 16:22:30,065 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müèÜ Ensemble training complete!\u001b[0m\n",
      "[ 2025-10-01 16:22:30,069 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müìä Generating SHAP visualizations and explanations...\u001b[0m\n",
      "[ 2025-10-01 16:22:30,095 ] src.visualizations.shap_visualizer - \u001b[32mINFO\u001b[0m - \u001b[32mGenerating SHAP explanations for 10 samples...\u001b[0m\n",
      "[ 2025-10-01 16:22:30,214 ] src.visualizations.shap_visualizer - \u001b[32mINFO\u001b[0m - \u001b[32mSHAP explanations generated successfully\u001b[0m\n",
      "[ 2025-10-01 16:22:37,040 ] src.visualizations.shap_visualizer - \u001b[32mINFO\u001b[0m - \u001b[32mGlobal importance plot saved to C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\tmp7du600o1\\xgboost_global_importance.html\u001b[0m\n",
      "[ 2025-10-01 16:22:39,795 ] src.visualizations.shap_visualizer - \u001b[32mINFO\u001b[0m - \u001b[32mSummary plot saved to C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\tmp7du600o1\\xgboost_summary_summary.png\u001b[0m\n",
      "[ 2025-10-01 16:22:46,554 ] src.visualizations.shap_visualizer - \u001b[32mINFO\u001b[0m - \u001b[32mWaterfall plot saved to C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\tmp7du600o1\\xgboost_waterfall_0.png\u001b[0m\n",
      "[ 2025-10-01 16:22:49,215 ] src.visualizations.shap_visualizer - \u001b[32mINFO\u001b[0m - \u001b[32mWaterfall plot saved to C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\tmp7du600o1\\xgboost_waterfall_1.png\u001b[0m\n",
      "[ 2025-10-01 16:22:51,282 ] src.visualizations.shap_visualizer - \u001b[32mINFO\u001b[0m - \u001b[32mWaterfall plot saved to C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\tmp7du600o1\\xgboost_waterfall_2.png\u001b[0m\n",
      "[ 2025-10-01 16:22:51,301 ] src.visualizations.shap_visualizer - \u001b[31mERROR\u001b[0m - \u001b[31mFailed to create business explanation: unsupported format string passed to NoneType.__format__\u001b[0m\n",
      "[ 2025-10-01 16:22:51,324 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Generated SHAP artifacts for xgboost\u001b[0m\n",
      "[ 2025-10-01 16:22:51,589 ] src.visualizations.shap_visualizer - \u001b[32mINFO\u001b[0m - \u001b[32mGenerating SHAP explanations for 10 samples...\u001b[0m\n",
      "[ 2025-10-01 16:22:51,621 ] src.visualizations.shap_visualizer - \u001b[32mINFO\u001b[0m - \u001b[32mSHAP explanations generated successfully\u001b[0m\n",
      "[ 2025-10-01 16:22:51,790 ] src.visualizations.shap_visualizer - \u001b[32mINFO\u001b[0m - \u001b[32mGlobal importance plot saved to C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\tmp7du600o1\\lightgbm_global_importance.html\u001b[0m\n",
      "[ 2025-10-01 16:22:55,180 ] src.visualizations.shap_visualizer - \u001b[32mINFO\u001b[0m - \u001b[32mSummary plot saved to C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\tmp7du600o1\\lightgbm_summary_summary.png\u001b[0m\n",
      "[ 2025-10-01 16:23:01,719 ] src.visualizations.shap_visualizer - \u001b[32mINFO\u001b[0m - \u001b[32mWaterfall plot saved to C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\tmp7du600o1\\lightgbm_waterfall_0.png\u001b[0m\n",
      "[ 2025-10-01 16:23:03,390 ] src.visualizations.shap_visualizer - \u001b[32mINFO\u001b[0m - \u001b[32mWaterfall plot saved to C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\tmp7du600o1\\lightgbm_waterfall_1.png\u001b[0m\n",
      "[ 2025-10-01 16:23:05,221 ] src.visualizations.shap_visualizer - \u001b[32mINFO\u001b[0m - \u001b[32mWaterfall plot saved to C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\tmp7du600o1\\lightgbm_waterfall_2.png\u001b[0m\n",
      "[ 2025-10-01 16:23:05,240 ] src.visualizations.shap_visualizer - \u001b[31mERROR\u001b[0m - \u001b[31mFailed to create business explanation: unsupported format string passed to NoneType.__format__\u001b[0m\n",
      "[ 2025-10-01 16:23:05,257 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Generated SHAP artifacts for lightgbm\u001b[0m\n",
      "[ 2025-10-01 16:23:05,313 ] __main__ - \u001b[33mWARNING\u001b[0m - \u001b[33m‚ö†Ô∏è SHAP visualization generation failed: must be real number, not str\u001b[0m\n",
      "[ 2025-10-01 16:23:05,320 ] src.models.digonistics - \u001b[32mINFO\u001b[0m - \u001b[32mü©∫ Starting model diagnostics...\u001b[0m\n",
      "[ 2025-10-01 16:23:05,333 ] src.models.digonistics - \u001b[32mINFO\u001b[0m - \u001b[32müîç Checking data quality...\u001b[0m\n",
      "[ 2025-10-01 16:23:05,576 ] src.models.digonistics - \u001b[32mINFO\u001b[0m - \u001b[32müìä Checking distribution shifts...\u001b[0m\n",
      "[ 2025-10-01 16:23:05,647 ] src.models.digonistics - \u001b[33mWARNING\u001b[0m - \u001b[33m‚ö†Ô∏è Test set shows distribution shift (mean shift: 34.6%)\u001b[0m\n",
      "[ 2025-10-01 16:23:05,652 ] src.models.digonistics - \u001b[32mINFO\u001b[0m - \u001b[32müßÆ Analyzing prediction residuals...\u001b[0m\n",
      "[ 2025-10-01 16:23:05,836 ] src.models.digonistics - \u001b[33mWARNING\u001b[0m - \u001b[33m‚ö†Ô∏è High number of features (109). Consider feature selection.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\anaconda3\\envs\\salesenv\\lib\\site-packages\\numpy\\lib\\function_base.py:2897: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in divide\n",
      "\n",
      "c:\\Users\\Administrator\\anaconda3\\envs\\salesenv\\lib\\site-packages\\numpy\\lib\\function_base.py:2898: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-10-01 16:23:09,298 ] src.models.digonistics - \u001b[32mINFO\u001b[0m - \u001b[32müí° Many zero sales (160711 in training). Consider log-transform or zero-inflated models.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\anaconda3\\envs\\salesenv\\lib\\site-packages\\numpy\\lib\\function_base.py:2897: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in divide\n",
      "\n",
      "c:\\Users\\Administrator\\anaconda3\\envs\\salesenv\\lib\\site-packages\\numpy\\lib\\function_base.py:2898: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-10-01 16:23:09,309 ] src.models.digonistics - \u001b[32mINFO\u001b[0m - \u001b[32müìÇ Diagnosis JSON saved to diagnostics\\diagnosis.json\u001b[0m\n",
      "[ 2025-10-01 16:23:09,315 ] src.models.digonistics - \u001b[32mINFO\u001b[0m - \u001b[32müìÑ Markdown diagnostic report saved to diagnostics\\diagnosis_report.md\u001b[0m\n",
      "[ 2025-10-01 16:23:09,317 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müìã Diagnostic recommendations:\u001b[0m\n",
      "[ 2025-10-01 16:23:09,322 ] __main__ - \u001b[33mWARNING\u001b[0m - \u001b[33m‚ö†Ô∏è - ‚ö†Ô∏è Test set shows distribution shift (mean shift: 34.6%)\u001b[0m\n",
      "[ 2025-10-01 16:23:09,325 ] __main__ - \u001b[33mWARNING\u001b[0m - \u001b[33m‚ö†Ô∏è - ‚ö†Ô∏è High number of features (109). Consider feature selection.\u001b[0m\n",
      "[ 2025-10-01 16:23:09,329 ] __main__ - \u001b[33mWARNING\u001b[0m - \u001b[33m‚ö†Ô∏è - üí° Many zero sales (160711 in training). Consider log-transform or zero-inflated models.\u001b[0m\n",
      "[ 2025-10-01 16:23:09,332 ] __main__ - \u001b[31mERROR\u001b[0m - \u001b[31m‚ùå Visualization generation failed: 'ModelTrainer' object has no attribute '_generate_and_log_visualizations'\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_21988\\1788269474.py\", line 814, in train_all_models\n",
      "    self._generate_and_log_visualizations(results, test_df, target_col)\n",
      "AttributeError: 'ModelTrainer' object has no attribute '_generate_and_log_visualizations'\n",
      "[ 2025-10-01 16:23:09,344 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müíæ Saving artifacts...\u001b[0m\n",
      "[ 2025-10-01 16:23:09,356 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müìÅ Created artifact directory: /tmp/artifacts/20251001_162309\u001b[0m\n",
      "[ 2025-10-01 16:23:09,466 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müíæ Saved scalers, encoders, and feature columns.\u001b[0m\n",
      "[ 2025-10-01 16:23:09,580 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müõ†Ô∏è Saved model: xgboost -> /tmp/artifacts/20251001_162309\\models/xgboost\u001b[0m\n",
      "[ 2025-10-01 16:23:09,684 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müõ†Ô∏è Saved model: lightgbm -> /tmp/artifacts/20251001_162309\\models/lightgbm\u001b[0m\n",
      "[ 2025-10-01 16:23:09,745 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müõ†Ô∏è Saved model: ensemble -> /tmp/artifacts/20251001_162309\\models/ensemble\u001b[0m\n",
      "[ 2025-10-01 16:23:09,751 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müìú Saved metadata.\u001b[0m\n",
      "[ 2025-10-01 16:23:13,225 ] src.utils.mlflow_utils - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Logged artifacts from '/tmp/artifacts/20251001_162309' to MLflow under 'reports'\u001b[0m\n",
      "[ 2025-10-01 16:23:13,227 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müöÄ Artifacts logged to MLflow.\u001b[0m\n",
      "[ 2025-10-01 16:23:13,229 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Artifacts saved successfully in /tmp/artifacts/20251001_162309\u001b[0m\n",
      "[ 2025-10-01 16:23:13,485 ] src.utils.mlflow_utils - \u001b[32mINFO\u001b[0m - \u001b[32mEnded MLflow run\u001b[0m\n",
      "[ 2025-10-01 16:23:13,494 ] src.utils.mlflow_utils - \u001b[33mWARNING\u001b[0m - \u001b[33mFailed to sync artifacts to S3: No module named 'utils'\u001b[0m\n",
      "[ 2025-10-01 16:23:13,498 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müèÅ MLflow run ended.\u001b[0m\n",
      "[ 2025-10-01 16:23:13,504 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚òÅÔ∏è Syncing artifacts to S3...\u001b[0m\n",
      "[ 2025-10-01 16:23:14,297 ] src.utils.service_discovery - \u001b[32mINFO\u001b[0m - \u001b[32mUsing MLFLOW_S3_ENDPOINT_URL from env: http://localhost:9000\u001b[0m\n",
      "[ 2025-10-01 16:23:16,480 ] src.utils.mlflow_s3_utils - \u001b[32mINFO\u001b[0m - \u001b[32mDownloading artifacts for run 9f1e2b0dfcb44bcc92772b5fb47fd35d\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts:   0%|          | 0/20 [00:00<?, ?it/s]2025/10/01 16:23:18 INFO mlflow.store.artifact.artifact_repo: The progress bar can be disabled by setting the environment variable MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR to false\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 106.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-10-01 16:23:23,479 ] src.utils.mlflow_s3_utils - \u001b[32mINFO\u001b[0m - \u001b[32mSuccessfully synced 20 artifact(s) for run 9f1e2b0dfcb44bcc92772b5fb47fd35d to S3\u001b[0m\n",
      "[ 2025-10-01 16:23:23,481 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úì Successfully synced artifacts to S3\u001b[0m\n",
      "[ 2025-10-01 16:23:23,486 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müîç Verifying S3 artifact storage...\u001b[0m\n",
      "[ 2025-10-01 16:23:23,729 ] src.utils.s3_verification - \u001b[31mERROR\u001b[0m - \u001b[31m‚úó S3 artifact verification FAILED\u001b[0m\n",
      "[ 2025-10-01 16:23:23,729 ] src.utils.s3_verification - \u001b[31mERROR\u001b[0m - \u001b[31m  - Artifact URI: mlflow-artifacts:/111669681926201532/9f1e2b0dfcb44bcc92772b5fb47fd35d/artifacts\u001b[0m\n",
      "[ 2025-10-01 16:23:23,737 ] src.utils.s3_verification - \u001b[31mERROR\u001b[0m - \u001b[31m  - Error: Artifact URI is not S3: mlflow-artifacts:/111669681926201532/9f1e2b0dfcb44bcc92772b5fb47fd35d/artifacts\u001b[0m\n",
      "[ 2025-10-01 16:23:23,742 ] __main__ - \u001b[33mWARNING\u001b[0m - \u001b[33m‚ö†Ô∏è S3 artifact verification failed after sync\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "results = trainer.train_all_models(train_df,value_df,test_df,target_col='sales',use_optuna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2175b70f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "salesenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
