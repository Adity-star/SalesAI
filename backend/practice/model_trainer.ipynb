{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb76836e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\OneDrive\\Desktop\\SalesAI\\backend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\anaconda3\\envs\\salesenv\\lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e0015a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-09-29 23:22:37,033 ] src.utils.config_loader - \u001b[32mINFO\u001b[0m - \u001b[32mLoaded configuration from: C:\\Users\\Administrator\\OneDrive\\Desktop\\SalesAI\\backend\\configs\\ml_config.yaml\u001b[0m\n",
      "[ 2025-09-29 23:22:37,038 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Config loaded successfully using config_loader\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import joblib\n",
    "from src.logger import logger\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from prophet import Prophet\n",
    "import optuna \n",
    "import mlflow\n",
    "import time\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from src.utils.mlflow_utils import MLflowManager\n",
    "from src.features.feature_pipeline import FeaturePipeline\n",
    "from src.data_pipelines.validators import DataValidator\n",
    "from src.models.advanced_ensemble import AdvancedEnsemble\n",
    "from src.models.digonistics import diagnose_model_performance\n",
    "from src.models.ensemble_model import EnsembleModel\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, MinMaxScaler, RobustScaler\n",
    "from src.utils.config_loader import ConfigLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "60b36fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import logging\n",
    "from src.logger import logger\n",
    "from typing import Dict, Any, List, Optional, Union\n",
    "from pathlib import Path\n",
    "from src.exception import CustomException  # make sure this is imported at the top\n",
    "import sys \n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, config_path: Optional[Union[str, Path]] = None):\n",
    "        self.config_loader = ConfigLoader()\n",
    "\n",
    "        self.config = self.config_loader.load_yaml(file_path=\"ml_config.yaml\")\n",
    "\n",
    "        self.training_config = self.config.get('training',{})\n",
    "\n",
    "        self.model_config: Dict[str, Any] = self.config.get('models', {})\n",
    "        self.model_config: Dict[str, Any] = self.config.get('models', {})\n",
    "\n",
    "        #self.mlflow_manager = MLflowManager(config_path)\n",
    "        self.feature_engineer = None\n",
    "\n",
    "        self.data_validator = DataValidator(config_path)\n",
    "\n",
    "        self.models: Dict[str, Any] = {}\n",
    "        self.scalers: Dict[str, Any] = {}\n",
    "        self.encoders: Dict[str, Any] = {}\n",
    "        self.feature_cols: List[str] = []\n",
    "\n",
    " # for exception context\n",
    "\n",
    "    def prepare_data(\n",
    "        self, df: pd.DataFrame, target_col: str = \"sales\",\n",
    "        date_col: str = \"date\", group_cols: Optional[List[str]] = None,\n",
    "        categorical_cols: Optional[List[str]] = None\n",
    "    ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "\n",
    "        logger.info(\"üõ† Preparing data for training\")\n",
    "\n",
    "        required_cols = [date_col, target_col]\n",
    "        if group_cols:\n",
    "            required_cols.extend(group_cols)\n",
    "\n",
    "        missing_cols = set(required_cols) - set(df.columns)\n",
    "        if missing_cols:\n",
    "            logger.error(f\"‚ùå Missing required columns for training: {missing_cols}\")\n",
    "            raise CustomException(f\"Missing required columns for training: {missing_cols}\", sys)\n",
    "\n",
    "        try:\n",
    "            pipeline = FeaturePipeline(df, target_col=target_col, group_cols=group_cols)\n",
    "            df_features = pipeline.run()\n",
    "            logger.info(\"‚úÖ Feature pipeline executed successfully.\")\n",
    "\n",
    "            if categorical_cols:\n",
    "                df_features = pipeline.create_target_encoding(df_features, target_col, categorical_cols)\n",
    "                logger.info(\"üéØ Applied target encoding to categorical columns.\")\n",
    "\n",
    "            # Chronological split\n",
    "            df_sorted = df_features.sort_values(date_col)\n",
    "            train_size = int(len(df_sorted) * (1 - self.training_config[\"test_size\"] - self.training_config[\"validation_size\"]))\n",
    "            val_size = int(len(df_sorted) * self.training_config[\"validation_size\"])\n",
    "\n",
    "            train_df = df_sorted[:train_size]\n",
    "            val_df = df_sorted[train_size:train_size + val_size]\n",
    "            test_df = df_sorted[train_size + val_size:]\n",
    "\n",
    "            # Drop rows with missing target\n",
    "            train_df = train_df.dropna(subset=[target_col])\n",
    "            val_df = val_df.dropna(subset=[target_col])\n",
    "            test_df = test_df.dropna(subset=[target_col])\n",
    "\n",
    "            logger.info(f\"üìä Data split ‚Üí Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "            return train_df, val_df, test_df\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error during data preparation: {e}\")\n",
    "            raise CustomException(e, sys)\n",
    "\n",
    "\n",
    "    def preprocess_features(\n",
    "        self,\n",
    "        train_df: pd.DataFrame,\n",
    "        val_df: pd.DataFrame,\n",
    "        test_df: pd.DataFrame,\n",
    "        target_col: str,\n",
    "        exclude_cols: List[str] = [\"date\"]\n",
    "    ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        try:\n",
    "            logger.info(\"üîÑ Starting feature preprocessing...\")\n",
    "\n",
    "            feature_cols = [col for col in train_df.columns if col not in exclude_cols + [target_col]]\n",
    "            self.feature_cols = feature_cols\n",
    "\n",
    "            X_train, X_val, X_test = train_df[feature_cols].copy(), val_df[feature_cols].copy(), test_df[feature_cols].copy()\n",
    "            y_train, y_val, y_test = train_df[target_col].values, val_df[target_col].values, test_df[target_col].values\n",
    "\n",
    "            categorical_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "\n",
    "            for col in categorical_cols:\n",
    "                if self.training_config.get(\"encoder\", \"label\") == \"label\":\n",
    "                    # Train LabelEncoder on training data\n",
    "                    if col not in self.encoders:\n",
    "                        le = LabelEncoder()\n",
    "                        le.fit(X_train[col].astype(str).fillna(\"missing\"))\n",
    "                        self.encoders[col] = le\n",
    "                    else:\n",
    "                        le = self.encoders[col]\n",
    "\n",
    "                    def transform_safe(encoder, series):\n",
    "                        known_classes = set(encoder.classes_)\n",
    "                        unknowns = series[~series.isin(known_classes)]\n",
    "                        if not unknowns.empty:\n",
    "                            logger.warning(f\"‚ö†Ô∏è Unseen labels in column '{col}': {unknowns.unique()}\")\n",
    "                            # Add \"unknown\" class if not already present\n",
    "                            if \"unknown\" not in encoder.classes_:\n",
    "                                encoder.classes_ = np.append(encoder.classes_, \"unknown\")\n",
    "                            series = series.apply(lambda x: x if x in known_classes else \"unknown\")\n",
    "                        return encoder.transform(series)\n",
    "\n",
    "                    for df_name, df in zip([\"Train\", \"Val\", \"Test\"], [X_train, X_val, X_test]):\n",
    "                        series = df[col].astype(str).fillna(\"missing\")\n",
    "                        transformed = transform_safe(le, series)\n",
    "                        df[col] = transformed.astype(np.int32)\n",
    "\n",
    "                        logger.info(f\"‚úÖ Label encoded '{col}' in {df_name} set.\")\n",
    "\n",
    "                elif self.training_config[\"encoder\"] == \"onehot\":\n",
    "                    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "                    X_train_encoded = ohe.fit_transform(X_train[[col]])\n",
    "                    X_val_encoded = ohe.transform(X_val[[col]])\n",
    "                    X_test_encoded = ohe.transform(X_test[[col]])\n",
    "\n",
    "                    encoded_cols = [f\"{col}_{cat}\" for cat in ohe.categories_[0]]\n",
    "                    X_train = X_train.drop(columns=col).join(pd.DataFrame(X_train_encoded, columns=encoded_cols, index=X_train.index))\n",
    "                    X_val = X_val.drop(columns=col).join(pd.DataFrame(X_val_encoded, columns=encoded_cols, index=X_val.index))\n",
    "                    X_test = X_test.drop(columns=col).join(pd.DataFrame(X_test_encoded, columns=encoded_cols, index=X_test.index))\n",
    "\n",
    "                    self.encoders[col] = ohe\n",
    "                    logger.info(f\"‚úÖ One-hot encoded '{col}'.\")\n",
    "\n",
    "            # Scaling numeric features\n",
    "            scaler_type = self.training_config.get(\"scaler\", \"standard\")\n",
    "            if scaler_type == \"standard\":\n",
    "                scaler = StandardScaler()\n",
    "            elif scaler_type == \"minmax\":\n",
    "                scaler = MinMaxScaler()\n",
    "            elif scaler_type == \"robust\":\n",
    "                scaler = RobustScaler()\n",
    "            else:\n",
    "                raise CustomException(f\"Unsupported scaler type: {scaler_type}\")\n",
    "\n",
    "            X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "            X_val_scaled = pd.DataFrame(scaler.transform(X_val), columns=X_val.columns, index=X_val.index)\n",
    "            X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "            self.scalers[\"scaler\"] = scaler\n",
    "\n",
    "            logger.info(f\"‚úÖ Preprocessing complete. Total features used: {len(self.feature_cols)} üß†\")\n",
    "\n",
    "            return X_train_scaled, X_val_scaled, X_test_scaled, y_train, y_val, y_test\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Preprocessing failed: {e}\")\n",
    "            raise CustomException(f\"Error in preprocess_features: {e}\")\n",
    "\n",
    "\n",
    "    def calculate_metrics(self, y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n",
    "        try:\n",
    "            logger.info(\"üìä Calculating model evaluation metrics...\")\n",
    "\n",
    "            # Avoid division by zero in MAPE\n",
    "            non_zero_mask = y_true != 0\n",
    "            if not np.any(non_zero_mask):\n",
    "                mape = np.nan\n",
    "                logger.warning(\"‚ö†Ô∏è All values in y_true are zero. MAPE is undefined.\")\n",
    "            else:\n",
    "                mape = np.mean(np.abs((y_true[non_zero_mask] - y_pred[non_zero_mask]) / y_true[non_zero_mask])) * 100\n",
    "\n",
    "            metrics = {\n",
    "                \"rmse\": round(np.sqrt(mean_squared_error(y_true, y_pred)), 4),\n",
    "                \"mae\": round(mean_absolute_error(y_true, y_pred), 4),\n",
    "                \"mape\": round(mape, 4),\n",
    "                \"r2\": round(r2_score(y_true, y_pred), 4),\n",
    "            }\n",
    "\n",
    "            logger.info(f\"‚úÖ Metrics calculated: {metrics}\")\n",
    "            return metrics\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Failed to calculate metrics: {e}\")\n",
    "            raise CustomException(f\"Error in calculate_metrics: {e}\")\n",
    "\n",
    "    \n",
    "\n",
    "    def train_xgboost(self, \n",
    "                  X_train: np.ndarray, y_train: np.ndarray,\n",
    "                  X_val: np.ndarray, y_val: np.ndarray,\n",
    "                  use_optuna: bool = True) -> xgb.XGBRegressor:\n",
    "        \"\"\"\n",
    "        Train an XGBoost regressor with optional Optuna hyperparameter optimization.\n",
    "\n",
    "        Args:\n",
    "            X_train, y_train: Training data\n",
    "            X_val, y_val: Validation data\n",
    "            use_optuna (bool): Whether to perform Optuna hyperparameter search\n",
    "\n",
    "        Returns:\n",
    "            Trained XGBRegressor model\n",
    "        \"\"\"\n",
    "        logger.info(\"üöÄ Starting training for XGBoost model...\")\n",
    "\n",
    "        if len(X_train) == 0 or len(X_val) == 0:\n",
    "            raise ValueError(\"‚ùå Training or validation data is empty.\")\n",
    "\n",
    "        # Detect GPU support\n",
    "        try:\n",
    "            tree_method = \"gpu_hist\" if xgb.get_config().get(\"use_gpu\", False) else \"hist\"\n",
    "            logger.info(f\"‚öôÔ∏è Using tree_method: `{tree_method}`\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"‚ö†Ô∏è Failed to detect GPU, using CPU. Reason: {e}\")\n",
    "            tree_method = \"hist\"\n",
    "\n",
    "        best_params = {}\n",
    "\n",
    "        if use_optuna:\n",
    "            logger.info(\"üîç Running Optuna hyperparameter optimization for XGBoost...\")\n",
    "\n",
    "            def objective(trial):\n",
    "                params = {\n",
    "                    'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "                    'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "                    'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.3, log=True),\n",
    "                    'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "                    'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "                    'gamma': trial.suggest_float('gamma', 0, 1.0),\n",
    "                    'reg_alpha': trial.suggest_float('reg_alpha', 0, 5.0),\n",
    "                    'reg_lambda': trial.suggest_float('reg_lambda', 0, 5.0),\n",
    "                    'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "                    'random_state': 42,\n",
    "                    'tree_method': tree_method\n",
    "                }\n",
    "\n",
    "                model = xgb.XGBRegressor(**params, early_stopping_rounds=50)\n",
    "                model.fit(\n",
    "                    X_train, y_train,\n",
    "                    eval_set=[(X_val, y_val)],\n",
    "                    verbose=False\n",
    "                )\n",
    "                y_pred = model.predict(X_val)\n",
    "                rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "                return rmse\n",
    "\n",
    "            try:\n",
    "                study = optuna.create_study(\n",
    "                    direction=\"minimize\",\n",
    "                    sampler=optuna.samplers.TPESampler(seed=42),\n",
    "                    pruner=optuna.pruners.MedianPruner()\n",
    "                )\n",
    "                study.optimize(objective, n_trials=self.training_config.get('optuna_trials', 50))\n",
    "\n",
    "                best_params = study.best_params\n",
    "                logger.info(f\"üèÜ Optuna best params found: {best_params}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"‚ùå Optuna optimization failed: {e}\")\n",
    "                raise CustomException(f\"Optuna error: {e}\")\n",
    "        else:\n",
    "            logger.info(\"üì¶ Using config-defined hyperparameters for XGBoost...\")\n",
    "            best_params = self.model_config.get(\"xgboost\", {}).get(\"params\", {})\n",
    "            if not best_params:\n",
    "                raise ValueError(\"‚ùå No XGBoost parameters found in model_config.\")\n",
    "            logger.info(f\"‚úÖ Loaded XGBoost params: {best_params}\")\n",
    "\n",
    "        # Add fixed parameters\n",
    "        best_params.update({\n",
    "            \"random_state\": 42,\n",
    "            \"tree_method\": tree_method\n",
    "        })\n",
    "\n",
    "        # Train final model\n",
    "        try:\n",
    "            logger.info(\"üõ†Ô∏è Training final XGBoost model...\")\n",
    "            model = xgb.XGBRegressor(**best_params, early_stopping_rounds=50)\n",
    "            model.fit(\n",
    "                X_train, y_train,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                verbose=True\n",
    "            )\n",
    "\n",
    "            self.models[\"xgboost\"] = model\n",
    "            logger.info(f\"‚úÖ Model trained successfully. Best iteration: {model.best_iteration}\")\n",
    "            return model\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå XGBoost model training failed: {e}\")\n",
    "            raise CustomException(f\"XGBoost training error: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "    def train_lightgbm(self, \n",
    "                   X_train: np.ndarray, y_train: np.ndarray,\n",
    "                   X_val: np.ndarray, y_val: np.ndarray,\n",
    "                   use_optuna: bool = True) -> lgb.LGBMRegressor:\n",
    "        \"\"\"\n",
    "        Train a LightGBM regressor with optional Optuna hyperparameter optimization.\n",
    "\n",
    "        Args:\n",
    "            X_train, y_train: Training dataset\n",
    "            X_val, y_val: Validation dataset\n",
    "            use_optuna (bool): Whether to use Optuna for hyperparameter tuning\n",
    "\n",
    "        Returns:\n",
    "            Trained LGBMRegressor model\n",
    "        \"\"\"\n",
    "\n",
    "        logger.info(\"‚öôÔ∏è Starting LightGBM training...\")\n",
    "\n",
    "        best_params = {}\n",
    "\n",
    "        if use_optuna:\n",
    "            logger.info(\"üîç Optuna hyperparameter optimization enabled for LightGBM\")\n",
    "\n",
    "            def objective(trial):\n",
    "                params = {\n",
    "                    \"num_leaves\": trial.suggest_int(\"num_leaves\", 31, 256),\n",
    "                    \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.005, 0.3, log=True),\n",
    "                    \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "                    \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "                    \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "                    \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "                    \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0, 5.0),\n",
    "                    \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0, 5.0),\n",
    "                    \"min_split_gain\": trial.suggest_float(\"min_split_gain\", 0, 1.0),\n",
    "                    \"random_state\": 42,\n",
    "                    \"verbosity\": -1,\n",
    "                    \"objective\": \"regression\",\n",
    "                    \"metric\": \"rmse\",\n",
    "                    \"boosting_type\": \"gbdt\"\n",
    "                }\n",
    "\n",
    "                model = lgb.LGBMRegressor(**params)\n",
    "                model.fit(\n",
    "                    X_train, y_train,\n",
    "                    eval_set=[(X_val, y_val)],\n",
    "                    callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]\n",
    "                )\n",
    "\n",
    "                y_pred = model.predict(X_val)\n",
    "                rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "                return rmse\n",
    "\n",
    "            try:\n",
    "                study = optuna.create_study(\n",
    "                    direction=\"minimize\",\n",
    "                    sampler=optuna.samplers.TPESampler(seed=42),\n",
    "                    pruner=optuna.pruners.MedianPruner()\n",
    "                )\n",
    "                study.optimize(objective, n_trials=self.training_config.get(\"optuna_trials\", 50))\n",
    "                best_params = study.best_params\n",
    "\n",
    "                logger.info(f\"üèÜ Best LightGBM params via Optuna: {best_params}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"‚ùå Optuna optimization failed: {e}\")\n",
    "                raise CustomException(f\"Optuna LightGBM error: {e}\")\n",
    "\n",
    "            # Add required fixed params\n",
    "            best_params.update({\n",
    "                \"random_state\": 42,\n",
    "                \"verbosity\": -1,\n",
    "                \"objective\": \"regression\",\n",
    "                \"metric\": \"rmse\",\n",
    "                \"boosting_type\": \"gbdt\"\n",
    "            })\n",
    "\n",
    "        else:\n",
    "            logger.info(\"üì¶ Using static config parameters for LightGBM...\")\n",
    "            best_params = self.model_config.get(\"lightgbm\", {}).get(\"params\", {})\n",
    "            if not best_params:\n",
    "                raise ValueError(\"‚ùå No LightGBM parameters found in `model_config`\")\n",
    "            logger.info(f\"‚úÖ Loaded config params: {best_params}\")\n",
    "\n",
    "        # Final training\n",
    "        try:\n",
    "            logger.info(\"üõ†Ô∏è Training final LightGBM model...\")\n",
    "            model = lgb.LGBMRegressor(**best_params)\n",
    "            model.fit(\n",
    "                X_train, y_train,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                callbacks=[lgb.early_stopping(50), lgb.log_evaluation(50)]\n",
    "            )\n",
    "\n",
    "            self.models[\"lightgbm\"] = model\n",
    "            logger.info(\"‚úÖ LightGBM model trained and stored.\")\n",
    "            return model\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå LightGBM training failed: {e}\")\n",
    "            raise CustomException(f\"LightGBM training error: {e}\")\n",
    "\n",
    "\n",
    "    def train_prophet(self, \n",
    "                    train_df: pd.DataFrame, \n",
    "                    val_df: pd.DataFrame,\n",
    "                    date_col: str = 'date', \n",
    "                    target_col: str = 'sales') -> Prophet:\n",
    "        \"\"\"\n",
    "        Train a Prophet model with optional regressors and evaluate on validation set.\n",
    "\n",
    "        Args:\n",
    "            train_df: Training DataFrame\n",
    "            val_df: Validation DataFrame\n",
    "            date_col: Name of date column\n",
    "            target_col: Name of target variable\n",
    "\n",
    "        Returns:\n",
    "            Trained Prophet model\n",
    "        \"\"\"\n",
    "\n",
    "        logger.info(\"üìÖ Starting Prophet training...\")\n",
    "\n",
    "        try:\n",
    "            # --- Prepare training data ---\n",
    "            prophet_train = train_df[[date_col, target_col]].rename(\n",
    "                columns={date_col: 'ds', target_col: 'y'}\n",
    "            ).dropna().sort_values('ds')\n",
    "\n",
    "            # Load Prophet hyperparameters from config\n",
    "            prophet_params = self.model_config.get('prophet', {}).get('params', {})\n",
    "            prophet_params.update({\n",
    "                'stan_backend': 'CMDSTANPY',\n",
    "                'mcmc_samples': 0,             # No Bayesian sampling = faster\n",
    "                'uncertainty_samples': 100     # Reasonable uncertainty\n",
    "            })\n",
    "\n",
    "            model = Prophet(**prophet_params)\n",
    "\n",
    "            # --- Select numeric regressors ---\n",
    "            numeric_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "            regressor_cols = [c for c in numeric_cols if c not in [target_col, 'year', 'month', 'day', 'week', 'quarter']]\n",
    "\n",
    "            # Reduce to top 5 highest variance\n",
    "            if len(regressor_cols) > 5:\n",
    "                variances = {col: train_df[col].var() for col in regressor_cols}\n",
    "                regressor_cols = sorted(variances, key=variances.get, reverse=True)[:5]\n",
    "\n",
    "            for col in regressor_cols:\n",
    "                if col in train_df and train_df[col].std() > 0:\n",
    "                    model.add_regressor(col)\n",
    "                    prophet_train[col] = train_df[col]\n",
    "                    logger.info(f\"‚ûï Added regressor to Prophet: {col}\")\n",
    "                else:\n",
    "                    logger.warning(f\"‚ö†Ô∏è Skipping regressor '{col}' due to zero variance or missing data.\")\n",
    "\n",
    "            # --- Fit the model ---\n",
    "            model.fit(prophet_train)\n",
    "            self.models['prophet'] = model\n",
    "            logger.info(\"‚úÖ Prophet model trained successfully.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Prophet training failed with error: {e}\")\n",
    "\n",
    "            logger.info(\"üîÅ Retrying Prophet with fallback parameters...\")\n",
    "\n",
    "            try:\n",
    "                model = Prophet(\n",
    "                    yearly_seasonality=True,\n",
    "                    weekly_seasonality=True,\n",
    "                    daily_seasonality=False,\n",
    "                    changepoint_prior_scale=0.05,\n",
    "                    seasonality_prior_scale=10.0,\n",
    "                    uncertainty_samples=50,\n",
    "                    mcmc_samples=0\n",
    "                )\n",
    "                fallback_train = prophet_train[['ds', 'y']]\n",
    "                model.fit(fallback_train)\n",
    "                self.models['prophet'] = model\n",
    "                logger.info(\"‚úÖ Prophet fallback model trained successfully.\")\n",
    "            except Exception as fallback_error:\n",
    "                logger.error(\"‚ùå Fallback Prophet training also failed.\")\n",
    "                raise CustomException(f\"Prophet training completely failed: {fallback_error}\")\n",
    "\n",
    "        # --- Validation Prediction ---\n",
    "        try:\n",
    "            prophet_val = val_df[[date_col, target_col]].rename(\n",
    "                columns={date_col: 'ds', target_col: 'y'}\n",
    "            ).dropna().sort_values('ds')\n",
    "\n",
    "            # Add matching regressors to validation data\n",
    "            for col in regressor_cols:\n",
    "                if col in val_df:\n",
    "                    prophet_val[col] = val_df[col]\n",
    "                else:\n",
    "                    logger.warning(f\"‚ö†Ô∏è Regressor '{col}' missing in validation set\")\n",
    "\n",
    "            forecast = model.predict(prophet_val)\n",
    "\n",
    "            y_true = prophet_val['y'].values\n",
    "            y_pred = forecast['yhat'].values\n",
    "\n",
    "            val_rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "            val_mae = mean_absolute_error(y_true, y_pred)\n",
    "            val_r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "            logger.info(f\"üìä Prophet Validation Metrics ‚Äî RMSE: {val_rmse:.4f} | MAE: {val_mae:.4f} | R¬≤: {val_r2:.4f}\")\n",
    "        except Exception as eval_error:\n",
    "            logger.error(f\"‚ùå Failed during Prophet validation: {eval_error}\")\n",
    "            raise CustomException(f\"Prophet evaluation failed: {eval_error}\")\n",
    "\n",
    "        return model\n",
    "\n",
    "    \n",
    "    def train_all_models(self, train_df: pd.DataFrame, val_df: pd.DataFrame,\n",
    "                        test_df: pd.DataFrame, target_col: str = 'sales',\n",
    "                        use_optuna: bool = True) -> Dict[str, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Train all models (XGBoost, LightGBM, Prophet) and build ensemble.\n",
    "        Improvements:\n",
    "            - Parallel training for speed\n",
    "            - Config-driven model selection\n",
    "            - Stacking ensemble (meta-learner)\n",
    "            - Robust error handling\n",
    "            - Runtime performance logging\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "\n",
    "        logger.info(\"üöÄ Starting full model training pipeline...\")\n",
    "\n",
    "        # # Start MLflow run\n",
    "        # run_id = self.mlflow_manager.start_run(\n",
    "        #     run_name=f\"sales_forecast_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "        #     tags={\"model_type\": \"ensemble\", \"use_optuna\": str(use_optuna)}\n",
    "        # )\n",
    "        #logger.info(f\"üéØ MLflow run started with run_id={run_id}\")\n",
    "\n",
    "        try:\n",
    "            # ------------------------\n",
    "            # Preprocess Data\n",
    "            # ------------------------\n",
    "            logger.info(\"üßπ Preprocessing features and target variables...\")\n",
    "            X_train, X_val, X_test, y_train, y_val, y_test = self.preprocess_features(\n",
    "                train_df, val_df, test_df, target_col\n",
    "            )\n",
    "            logger.info(f\"üìä Data sizes - Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "            # self.mlflow_manager.log_params({\n",
    "            #     \"train_size\": len(train_df),\n",
    "            #     \"val_size\": len(val_df),\n",
    "            #     \"test_size\": len(test_df),\n",
    "            #     \"n_features\": X_train.shape[1]\n",
    "            # })\n",
    "\n",
    "            # ------------\n",
    "            # Train Models\n",
    "            # ------------\n",
    "            # Train XGBoost\n",
    "            logger.info(\"üî• Training XGBoost model...\")\n",
    "            try:\n",
    "                xgb_model = self.train_xgboost(X_train, y_train, X_val, y_val, use_optuna)\n",
    "                xgb_pred = xgb_model.predict(X_test)\n",
    "                xgb_metrics = self.calculate_metrics(y_test, xgb_pred)\n",
    "\n",
    "                # self.mlflow_manager.log_metrics({f\"xgboost_{k}\": v for k, v in xgb_metrics.items()})\n",
    "                # self.mlflow_manager.log_model(xgb_model, \"xgboost\", input_example=X_train.iloc[:5])\n",
    "\n",
    "                feature_importance = pd.DataFrame({\n",
    "                    'feature': self.feature_cols,\n",
    "                    'importance': xgb_model.feature_importances_\n",
    "                }).sort_values('importance', ascending=False).head(20)\n",
    "\n",
    "                logger.info(f\"üåü Top XGBoost features:\\n{feature_importance.to_string()}\")\n",
    "                #self.mlflow_manager.log_params({f\"xgb_top_feature_{i}\": f\"{row['feature']} ({row['importance']:.4f})\"\n",
    "                                        #    for i, (_, row) in enumerate(feature_importance.iterrows())})\n",
    "\n",
    "                results['xgboost'] = {\n",
    "                    'model': xgb_model,\n",
    "                    'metrics': xgb_metrics,\n",
    "                    'predictions': xgb_pred,\n",
    "                    'actual': y_test\n",
    "                }\n",
    "                logger.info(\"‚úÖ XGBoost training complete!\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"‚ùå XGBoost training failed: {e}\", exc_info=True)\n",
    "                raise e\n",
    "\n",
    "            # Train LightGBM\n",
    "            logger.info(\"üî• Training LightGBM model...\")\n",
    "            try:\n",
    "                lgb_model = self.train_lightgbm(X_train, y_train, X_val, y_val, use_optuna)\n",
    "                lgb_pred = lgb_model.predict(X_test)\n",
    "                lgb_metrics = self.calculate_metrics(y_test, lgb_pred)\n",
    "\n",
    "                # self.mlflow_manager.log_metrics({f\"lightgbm_{k}\": v for k, v in lgb_metrics.items()})\n",
    "                # self.mlflow_manager.log_model(lgb_model, \"lightgbm\", input_example=X_train.iloc[:5])\n",
    "\n",
    "                lgb_importance = pd.DataFrame({\n",
    "                    'feature': self.feature_cols,\n",
    "                    'importance': lgb_model.feature_importances_\n",
    "                }).sort_values('importance', ascending=False).head(20)\n",
    "\n",
    "                logger.info(f\"üåü Top LightGBM features:\\n{lgb_importance.to_string()}\")\n",
    "\n",
    "                results['lightgbm'] = {\n",
    "                    'model': lgb_model,\n",
    "                    'metrics': lgb_metrics,\n",
    "                    'predictions': lgb_pred,\n",
    "                    'actual': y_test\n",
    "                }\n",
    "                logger.info(\"‚úÖ LightGBM training complete!\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"‚ùå LightGBM training failed: {e}\", exc_info=True)\n",
    "                raise e\n",
    "\n",
    "            # ------------------------\n",
    "            # Train Prophet (Sequential - slower but unavoidable)\n",
    "            # ------------------------\n",
    "            prophet_enabled = self.model_config.get('prophet', {}).get('enabled', True)\n",
    "            if prophet_enabled:\n",
    "                logger.info(\"üåü Training Prophet model...\")\n",
    "                try:\n",
    "                    prophet_model = self.train_prophet(train_df, val_df)\n",
    "\n",
    "                    future = test_df[['date']].rename(columns={'date': 'ds'})\n",
    "\n",
    "                    # Add regressors if available\n",
    "                    if hasattr(prophet_model, 'extra_regressors') and prophet_model.extra_regressors:\n",
    "                        regressor_cols = [col for col in prophet_model.extra_regressors.keys()]\n",
    "                        for col in regressor_cols:\n",
    "                            if col in test_df.columns:\n",
    "                                future[col] = test_df[col]\n",
    "\n",
    "                    prophet_pred = prophet_model.predict(future)['yhat'].values\n",
    "                    prophet_metrics = self.calculate_metrics(y_test, prophet_pred)\n",
    "\n",
    "                    # self.mlflow_manager.log_metrics({f\"prophet_{k}\": v for k, v in prophet_metrics.items()})\n",
    "\n",
    "                    results['prophet'] = {\n",
    "                        'model': prophet_model,\n",
    "                        'metrics': prophet_metrics,\n",
    "                        'predictions': prophet_pred,\n",
    "                        'actual': y_test\n",
    "                    }\n",
    "                    logger.info(\"‚úÖ Prophet training complete!\")\n",
    "\n",
    "                    # Ensemble predictions (all three)\n",
    "                    ensemble_pred = (xgb_pred + lgb_pred + prophet_pred) / 3\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"‚ö†Ô∏è Prophet training failed: {e}. Falling back to weighted ensemble of XGBoost and LightGBM.\")\n",
    "                    prophet_enabled = False\n",
    "            else:\n",
    "                logger.info(\"‚ÑπÔ∏è Prophet training skipped by config.\")\n",
    "                prophet_enabled = False\n",
    "\n",
    "            # ------------------------\n",
    "            # Build Stacking Ensemble (Meta-Model)\n",
    "            # ------------------------\n",
    "            logger.info(\"üß© Building stacking ensemble...\")\n",
    "\n",
    "            if not prophet_enabled:\n",
    "                try:\n",
    "                    xgb_val_pred = xgb_model.predict(X_val)\n",
    "                    lgb_val_pred = lgb_model.predict(X_val)\n",
    "\n",
    "                    xgb_val_r2 = r2_score(y_val, xgb_val_pred)\n",
    "                    lgb_val_r2 = r2_score(y_val, lgb_val_pred)\n",
    "\n",
    "                    min_weight = 0.2\n",
    "                    xgb_weight = max(min_weight, xgb_val_r2 / (xgb_val_r2 + lgb_val_r2))\n",
    "                    lgb_weight = max(min_weight, lgb_val_r2 / (xgb_val_r2 + lgb_val_r2))\n",
    "\n",
    "                    total_weight = xgb_weight + lgb_weight\n",
    "                    xgb_weight /= total_weight\n",
    "                    lgb_weight /= total_weight\n",
    "\n",
    "                    logger.info(f\"‚öñÔ∏è Ensemble weights - XGBoost: {xgb_weight:.3f}, LightGBM: {lgb_weight:.3f}\")\n",
    "\n",
    "                    ensemble_weights = {\n",
    "                        'xgboost': xgb_weight,\n",
    "                        'lightgbm': lgb_weight\n",
    "                    }\n",
    "\n",
    "                    ensemble_pred = xgb_weight * xgb_pred + lgb_weight * lgb_pred\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"‚ùå Failed to build weighted ensemble: {e}\", exc_info=True)\n",
    "                    raise e\n",
    "\n",
    "            # Create ensemble model object\n",
    "            ensemble_models = {\n",
    "                'xgboost': xgb_model,\n",
    "                'lightgbm': lgb_model\n",
    "            }\n",
    "            if 'prophet' in results:\n",
    "                ensemble_models['prophet'] = results['prophet']['model']\n",
    "                ensemble_weights = {\n",
    "                    'xgboost': 1 / 3,\n",
    "                    'lightgbm': 1 / 3,\n",
    "                    'prophet': 1 / 3\n",
    "                }\n",
    "\n",
    "            ensemble_model = EnsembleModel(ensemble_models, ensemble_weights)\n",
    "\n",
    "            self.models['ensemble'] = ensemble_model\n",
    "\n",
    "            ensemble_metrics = self.calculate_metrics(y_test, ensemble_pred)\n",
    "\n",
    "            # self.mlflow_manager.log_metrics({f\"ensemble_{k}\": v for k, v in ensemble_metrics.items()})\n",
    "            # self.mlflow_manager.log_model(ensemble_model, \"ensemble\", input_example=X_train.iloc[:5])\n",
    "\n",
    "            results['ensemble'] = {\n",
    "                'model': ensemble_model,\n",
    "                'metrics': ensemble_metrics,\n",
    "                'predictions': ensemble_pred,\n",
    "                'actual': y_test\n",
    "            }\n",
    "            logger.info(\"üèÜ Ensemble training complete!\")\n",
    "\n",
    "            logger.info(f\"üéâ Successfully logged models and metrics: {list(results.keys())}\")\n",
    "\n",
    "            logger.info(\"üîç Running diagnostics & visualizations...\")\n",
    "\n",
    "            # Run diagnostics\n",
    "            try:\n",
    "                test_predictions = {\n",
    "                    'xgboost': xgb_pred if 'xgboost' in results else None,\n",
    "                    'lightgbm': lgb_pred if 'lightgbm' in results else None,\n",
    "                    'ensemble': ensemble_pred\n",
    "                }\n",
    "\n",
    "                diagnosis = diagnose_model_performance(\n",
    "                    train_df, val_df, test_df, test_predictions, target_col\n",
    "                )\n",
    "\n",
    "                logger.info(\"üìã Diagnostic recommendations:\")\n",
    "                for rec in diagnosis['recommendations']:\n",
    "                    logger.warning(f\"‚ö†Ô∏è - {rec}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"‚ùå Diagnostics failed: {e}\", exc_info=True)\n",
    "\n",
    "            # Generate visualizations\n",
    "            logger.info(\"üìä Generating model comparison visualizations...\")\n",
    "            try:\n",
    "                self._generate_and_log_visualizations(results, test_df, target_col)\n",
    "            except Exception as viz_error:\n",
    "                logger.error(f\"‚ùå Visualization generation failed: {viz_error}\", exc_info=True)\n",
    "\n",
    "            # Save artifacts\n",
    "            logger.info(\"üíæ Saving artifacts...\")\n",
    "            self.save_artifacts()\n",
    "\n",
    "            # Get current run ID for verification\n",
    "            #current_run_id = mlflow.active_run().info.run_id\n",
    "\n",
    "            #self.mlflow_manager.end_run()\n",
    "            logger.info(\"üèÅ MLflow run ended.\")\n",
    "\n",
    "            # Sync artifacts to S3\n",
    "            # from src.utils.mlflow_s3_utils import MLflowS3Manager\n",
    "\n",
    "            # logger.info(\"‚òÅÔ∏è Syncing artifacts to S3...\")\n",
    "            # try:\n",
    "            #     s3_manager = MLflowS3Manager()\n",
    "            #     s3_manager.sync_mlflow_artifacts_to_s3(current_run_id)\n",
    "            #     logger.info(\"‚úì Successfully synced artifacts to S3\")\n",
    "\n",
    "            #     # Verify S3 artifacts after sync\n",
    "            #     from src.utils.s3_verification import verify_s3_artifacts, log_s3_verification_results\n",
    "\n",
    "            #     logger.info(\"üîç Verifying S3 artifact storage...\")\n",
    "            #     verification_results = verify_s3_artifacts(\n",
    "            #         run_id=current_run_id,\n",
    "            #         expected_artifacts=[\n",
    "            #             'models/',\n",
    "            #             'scalers.pkl',\n",
    "            #             'encoders.pkl',\n",
    "            #             'feature_cols.pkl',\n",
    "            #             'visualizations/',\n",
    "            #             'reports/'\n",
    "            #         ]\n",
    "            #     )\n",
    "            #     log_s3_verification_results(verification_results)\n",
    "\n",
    "            #     if not verification_results[\"success\"]:\n",
    "            #         logger.warning(\"‚ö†Ô∏è S3 artifact verification failed after sync\")\n",
    "            # except Exception as e:\n",
    "            #     logger.error(f\"‚ùå Failed to sync artifacts to S3: {e}\", exc_info=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            #self.mlflow_manager.end_run(status=\"FAILED\")\n",
    "            logger.error(f\"üí• Training pipeline failed: {e}\", exc_info=True)\n",
    "            raise e\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "    def _generate_and_log_visualizations(self, results: Dict[str, Any], \n",
    "                                    test_df: pd.DataFrame, \n",
    "                                    target_col: str = 'sales') -> None:\n",
    "        \"\"\"Generate and log model comparison visualizations to MLflow\"\"\"\n",
    "        try:\n",
    "            from src.visualizations.model_visualizations import ModelVisualizer\n",
    "            import tempfile, os, json, mlflow\n",
    "            \n",
    "            logger.info(\"üé® Starting visualization generation...\")\n",
    "\n",
    "            visualizer = ModelVisualizer()\n",
    "            \n",
    "            # Ensure date column exists\n",
    "            if 'date' not in test_df.columns:\n",
    "                test_df = test_df.reset_index().rename(columns={'index': 'date'})\n",
    "                logger.warning(\"‚ö†Ô∏è No 'date' column found in test data. Using index as date.\")\n",
    "\n",
    "            # Extract metrics\n",
    "            metrics_dict = {\n",
    "                model_name: model_results['metrics']\n",
    "                for model_name, model_results in results.items()\n",
    "                if 'metrics' in model_results\n",
    "            }\n",
    "            logger.debug(f\"üìä Collected metrics: {metrics_dict}\")\n",
    "\n",
    "            # Prepare predictions\n",
    "            predictions_dict = {}\n",
    "            for model_name, model_results in results.items():\n",
    "                preds = model_results.get('predictions')\n",
    "                if preds is not None:\n",
    "                    pred_df = test_df[['date']].copy()\n",
    "                    pred_df['prediction'] = preds\n",
    "                    predictions_dict[model_name] = pred_df\n",
    "            logger.debug(f\"üîç Prepared predictions for models: {list(predictions_dict.keys())}\")\n",
    "\n",
    "            # Feature importances\n",
    "            feature_importance_dict = {}\n",
    "            for model_name, model_results in results.items():\n",
    "                model = model_results.get('model')\n",
    "                if model and hasattr(model, 'feature_importances_'):\n",
    "                    importance_df = pd.DataFrame({\n",
    "                        'feature': self.feature_cols,\n",
    "                        'importance': model.feature_importances_\n",
    "                    }).sort_values('importance', ascending=False)\n",
    "                    feature_importance_dict[model_name] = importance_df\n",
    "            logger.debug(\"‚≠ê Extracted feature importances\")\n",
    "\n",
    "            # Generate and log visualizations\n",
    "            with tempfile.TemporaryDirectory() as temp_dir:\n",
    "                logger.info(f\"üìÅ Creating visualizations in temporary directory: {temp_dir}\")\n",
    "                \n",
    "                saved_files = visualizer.create_comprehensive_report(\n",
    "                    metrics_dict=metrics_dict,\n",
    "                    predictions_dict=predictions_dict,\n",
    "                    actual_data=test_df,\n",
    "                    feature_importance_dict=feature_importance_dict if feature_importance_dict else None,\n",
    "                    save_dir=temp_dir\n",
    "                )\n",
    "\n",
    "                if not isinstance(saved_files, dict):\n",
    "                    logger.error(f\"‚ùå Expected dict from create_comprehensive_report, got {type(saved_files)}\")\n",
    "                    raise TypeError(\"Invalid return type from create_comprehensive_report\")\n",
    "\n",
    "                logger.info(f\"‚úÖ Generated {len(saved_files)} visualization files\")\n",
    "\n",
    "                for viz_name, file_path in saved_files.items():\n",
    "                    if os.path.exists(file_path):\n",
    "                        mlflow.log_artifact(file_path, \"visualizations\")\n",
    "                        logger.info(f\"üì§ Logged visualization: {viz_name}\")\n",
    "                    else:\n",
    "                        logger.warning(f\"‚ö†Ô∏è Visualization file missing: {file_path} for {viz_name}\")\n",
    "\n",
    "                # Save metrics summary JSON\n",
    "                summary_file = os.path.join(temp_dir, \"metrics_summary.json\")\n",
    "                with open(summary_file, \"w\") as f:\n",
    "                    json.dump(metrics_dict, f, indent=4)\n",
    "                mlflow.log_artifact(summary_file, \"reports\")\n",
    "                logger.info(\"üìÑ Logged metrics summary JSON\")\n",
    "\n",
    "                # Combined HTML report\n",
    "                self._create_combined_html_report(results, temp_dir)\n",
    "                combined_report = os.path.join(temp_dir, 'model_comparison_report.html')\n",
    "                if os.path.exists(combined_report):\n",
    "                    mlflow.log_artifact(combined_report, \"reports\")\n",
    "                    logger.info(\"üìà Logged combined HTML report\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"üí• Failed to generate visualizations: {e}\", exc_info=True)\n",
    "\n",
    "\n",
    "    def _create_combined_html_report(self, saved_files: Dict[str, str], save_dir: str) -> None:\n",
    "        \"\"\"Create a combined HTML report with all visualizations\"\"\"\n",
    "        import os\n",
    "        from datetime import datetime\n",
    "        import base64\n",
    "\n",
    "        try:\n",
    "            logger.info(\"üìù Creating combined HTML report for visualizations...\")\n",
    "\n",
    "            html_content = \"\"\"\n",
    "            <!DOCTYPE html>\n",
    "            <html>\n",
    "            <head>\n",
    "                <title>Model Comparison Report</title>\n",
    "                <style>\n",
    "                    body {\n",
    "                        font-family: Arial, sans-serif;\n",
    "                        margin: 20px;\n",
    "                        background-color: #f5f5f5;\n",
    "                    }\n",
    "                    h1, h2 {\n",
    "                        color: #333;\n",
    "                    }\n",
    "                    .section {\n",
    "                        background-color: white;\n",
    "                        padding: 20px;\n",
    "                        margin-bottom: 20px;\n",
    "                        border-radius: 8px;\n",
    "                        box-shadow: 0 2px 4px rgba(0,0,0,0.1);\n",
    "                    }\n",
    "                    .timestamp {\n",
    "                        color: #666;\n",
    "                        font-size: 14px;\n",
    "                    }\n",
    "                    iframe {\n",
    "                        width: 100%;\n",
    "                        height: 800px;\n",
    "                        border: 1px solid #ddd;\n",
    "                        border-radius: 4px;\n",
    "                        margin-top: 10px;\n",
    "                    }\n",
    "                    img {\n",
    "                        max-width: 100%;\n",
    "                        height: auto;\n",
    "                        border-radius: 4px;\n",
    "                        margin-top: 10px;\n",
    "                    }\n",
    "                </style>\n",
    "            </head>\n",
    "            <body>\n",
    "                <h1>Sales Forecast Model Comparison Report</h1>\n",
    "                <p class=\"timestamp\">Generated on: {timestamp}</p>\n",
    "            \"\"\"\n",
    "\n",
    "            html_content = html_content.format(timestamp=datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "            sections = [\n",
    "                ('metrics_comparison', 'Model Performance Metrics'),\n",
    "                ('predictions_comparison', 'Predictions Comparison'),\n",
    "                ('residuals_analysis', 'Residuals Analysis'),\n",
    "                ('error_distribution', 'Error Distribution'),\n",
    "                ('feature_importance', 'Feature Importance'),\n",
    "                ('summary', 'Summary Statistics')\n",
    "            ]\n",
    "\n",
    "            for key, title in sections:\n",
    "                if key in saved_files:\n",
    "                    html_content += f'<div class=\"section\"><h2>{title}</h2>'\n",
    "\n",
    "                    try:\n",
    "                        with open(saved_files[key], 'rb') as f:\n",
    "                            img_data = base64.b64encode(f.read()).decode()\n",
    "                        html_content += f'<img src=\"data:image/png;base64,{img_data}\" alt=\"{title}\">'\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"‚ö†Ô∏è Failed to embed image for section '{title}': {e}\")\n",
    "\n",
    "                    html_content += '</div>'\n",
    "\n",
    "            html_content += \"\"\"\n",
    "            </body>\n",
    "            </html>\n",
    "            \"\"\"\n",
    "\n",
    "            report_path = os.path.join(save_dir, 'model_comparison_report.html')\n",
    "            with open(report_path, 'w') as f:\n",
    "                f.write(html_content)\n",
    "\n",
    "            logger.info(f\"‚úÖ Combined HTML report created at: {report_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"üí• Failed to create combined HTML report: {e}\", exc_info=True)\n",
    "\n",
    "    def save_artifacts(self, version: str = None):\n",
    "        \"\"\"\n",
    "        Save scalers, encoders, feature columns, and trained models.\n",
    "        Also logs everything to MLflow for version tracking.\n",
    "        \"\"\"\n",
    "        import os\n",
    "        import joblib\n",
    "        from datetime import datetime\n",
    "\n",
    "        version = version or datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        base_dir = f'/tmp/artifacts/{version}'\n",
    "\n",
    "        try:\n",
    "            os.makedirs(base_dir, exist_ok=True)\n",
    "            logger.info(f\"üìÅ Created artifact directory: {base_dir}\")\n",
    "\n",
    "            # Save preprocessing objects\n",
    "            joblib.dump(self.scalers, os.path.join(base_dir, 'scalers.pkl'))\n",
    "            joblib.dump(self.encoders, os.path.join(base_dir, 'encoders.pkl'))\n",
    "            joblib.dump(self.feature_cols, os.path.join(base_dir, 'feature_cols.pkl'))\n",
    "            logger.info(\"üíæ Saved scalers, encoders, and feature columns.\")\n",
    "\n",
    "            # Save model directories\n",
    "            model_dirs = {\n",
    "                'xgboost': os.path.join(base_dir, 'models/xgboost'),\n",
    "                'lightgbm': os.path.join(base_dir, 'models/lightgbm'),\n",
    "                'ensemble': os.path.join(base_dir, 'models/ensemble')\n",
    "            }\n",
    "\n",
    "            for mname, mdir in model_dirs.items():\n",
    "                os.makedirs(mdir, exist_ok=True)\n",
    "                if mname in self.models:\n",
    "                    model = self.models[mname]\n",
    "                    joblib.dump(model, os.path.join(mdir, f\"{mname}_model.pkl\"))\n",
    "                    logger.info(f\"üõ†Ô∏è Saved model: {mname} -> {mdir}\")\n",
    "\n",
    "            # Save metadata for reproducibility\n",
    "            metadata = {\n",
    "                \"version\": version,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"models_saved\": list(self.models.keys()),\n",
    "                \"feature_count\": len(self.feature_cols),\n",
    "            }\n",
    "            joblib.dump(metadata, os.path.join(base_dir, 'metadata.pkl'))\n",
    "            logger.info(\"üìú Saved metadata.\")\n",
    "\n",
    "            # Log artifacts to MLflow\n",
    "            if hasattr(self, \"mlflow_manager\") and self.mlflow_manager:\n",
    "                self.mlflow_manager.log_artifacts(base_dir)\n",
    "                logger.info(\"üöÄ Artifacts logged to MLflow.\")\n",
    "            else:\n",
    "                logger.warning(\"‚ö†Ô∏è mlflow_manager not found or None. Skipping MLflow logging.\")\n",
    "\n",
    "            logger.info(f\"‚úÖ Artifacts saved successfully in {base_dir}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Failed to save artifacts: {e}\", exc_info=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-09-30 00:29:02,984 ] src.utils.config_loader - \u001b[32mINFO\u001b[0m - \u001b[32mLoaded configuration from: C:\\Users\\Administrator\\OneDrive\\Desktop\\SalesAI\\backend\\configs\\ml_config.yaml\u001b[0m\n",
      "[ 2025-09-30 00:29:02,991 ] src.data_pipelines.validators - \u001b[32mINFO\u001b[0m - \u001b[32müîç Initializing DataValidator...\u001b[0m\n",
      "[ 2025-09-30 00:29:03,017 ] src.utils.config_loader - \u001b[32mINFO\u001b[0m - \u001b[32mLoaded configuration from: C:\\Users\\Administrator\\OneDrive\\Desktop\\SalesAI\\backend\\configs\\ml_config.yaml\u001b[0m\n",
      "[ 2025-09-30 00:29:03,017 ] src.data_pipelines.validators - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Loaded validation config from: ml_config.yaml\u001b[0m\n",
      "[ 2025-09-30 00:29:03,023 ] src.data_pipelines.validators - \u001b[32mINFO\u001b[0m - \u001b[32müìã Validation Config Loaded:\u001b[0m\n",
      "[ 2025-09-30 00:29:03,025 ] src.data_pipelines.validators - \u001b[32mINFO\u001b[0m - \u001b[32m - Required Columns: ['date', 'sales', 'store_id', 'product_id']\u001b[0m\n",
      "[ 2025-09-30 00:29:03,025 ] src.data_pipelines.validators - \u001b[32mINFO\u001b[0m - \u001b[32m - Data Types: {'date': 'datetime64[ns]', 'sales': 'float64', 'store_id': 'object', 'product_id': 'object'}\u001b[0m\n",
      "[ 2025-09-30 00:29:03,035 ] src.data_pipelines.validators - \u001b[32mINFO\u001b[0m - \u001b[32m - Value Ranges: {'sales': {'min': 0, 'max': 1000000}}\u001b[0m\n",
      "[ 2025-09-30 00:29:03,037 ] src.data_pipelines.validators - \u001b[32mINFO\u001b[0m - \u001b[32m - Check Duplicates: True\u001b[0m\n",
      "[ 2025-09-30 00:29:03,040 ] src.data_pipelines.validators - \u001b[32mINFO\u001b[0m - \u001b[32m - Outlier Detection Method: zscore\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-09-30 00:29:03,042 ] src.data_pipelines.validators - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ DataValidator initialized successfully.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "trainer = ModelTrainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(r\"C:\\Users\\Administrator\\OneDrive\\Desktop\\SalesAI\\backend\\data\\features\\m5\\m5_features.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94d44105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2958084, 82)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3a582b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(df)\n",
    "new_df = df.iloc[:n//9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "de174f95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(328676, 82)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4508dc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-09-30 00:07:20,556 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müõ† Preparing data for training\u001b[0m\n",
      "[ 2025-09-30 00:07:20,991 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müöÄ Initialized FeaturePipeline with target: sales, groups: ['store_id'], country: US\u001b[0m\n",
      "[ 2025-09-30 00:07:20,993 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müö¶ Starting Feature Pipeline...\u001b[0m\n",
      "[ 2025-09-30 00:07:20,995 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müìÖ Adding date features\u001b[0m\n",
      "[ 2025-09-30 00:07:21,331 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Added date features: ['year', 'month', 'day', 'dayofweek', 'quarter', 'weekofyear', 'is_weekend', 'is_holiday']\u001b[0m\n",
      "[ 2025-09-30 00:07:21,336 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müìÖ Date features added.\u001b[0m\n",
      "[ 2025-09-30 00:07:21,339 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müï∞ Adding lag features\u001b[0m\n",
      "[ 2025-09-30 00:07:21,797 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created lag feature: sales_lag_1\u001b[0m\n",
      "[ 2025-09-30 00:07:21,811 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created lag feature: sales_lag_2\u001b[0m\n",
      "[ 2025-09-30 00:07:21,828 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created lag feature: sales_lag_3\u001b[0m\n",
      "[ 2025-09-30 00:07:21,846 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created lag feature: sales_lag_7\u001b[0m\n",
      "[ 2025-09-30 00:07:21,864 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created lag feature: sales_lag_14\u001b[0m\n",
      "[ 2025-09-30 00:07:21,880 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created lag feature: sales_lag_21\u001b[0m\n",
      "[ 2025-09-30 00:07:21,906 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created lag feature: sales_lag_30\u001b[0m\n",
      "[ 2025-09-30 00:07:21,910 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müï∞ Lag features added.\u001b[0m\n",
      "[ 2025-09-30 00:07:21,914 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müìà Adding rolling features\u001b[0m\n",
      "[ 2025-09-30 00:07:22,436 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created rolling feature: sales_roll_3_mean\u001b[0m\n",
      "[ 2025-09-30 00:07:22,505 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created rolling feature: sales_roll_3_std\u001b[0m\n",
      "[ 2025-09-30 00:07:22,574 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created rolling feature: sales_roll_3_min\u001b[0m\n",
      "[ 2025-09-30 00:07:22,637 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created rolling feature: sales_roll_3_max\u001b[0m\n",
      "[ 2025-09-30 00:07:22,773 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created rolling feature: sales_roll_3_median\u001b[0m\n",
      "[ 2025-09-30 00:07:22,831 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created rolling feature: sales_roll_7_mean\u001b[0m\n",
      "[ 2025-09-30 00:07:22,895 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created rolling feature: sales_roll_7_std\u001b[0m\n",
      "[ 2025-09-30 00:07:22,962 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created rolling feature: sales_roll_7_min\u001b[0m\n",
      "[ 2025-09-30 00:07:23,031 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created rolling feature: sales_roll_7_max\u001b[0m\n",
      "[ 2025-09-30 00:07:23,252 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created rolling feature: sales_roll_7_median\u001b[0m\n",
      "[ 2025-09-30 00:07:23,308 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created rolling feature: sales_roll_14_mean\u001b[0m\n",
      "[ 2025-09-30 00:07:23,371 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created rolling feature: sales_roll_14_std\u001b[0m\n",
      "[ 2025-09-30 00:07:23,415 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created rolling feature: sales_roll_14_min\u001b[0m\n",
      "[ 2025-09-30 00:07:23,463 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created rolling feature: sales_roll_14_max\u001b[0m\n",
      "[ 2025-09-30 00:07:23,643 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created rolling feature: sales_roll_14_median\u001b[0m\n",
      "[ 2025-09-30 00:07:23,693 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created rolling feature: sales_roll_21_mean\u001b[0m\n",
      "[ 2025-09-30 00:07:23,762 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created rolling feature: sales_roll_21_std\u001b[0m\n",
      "[ 2025-09-30 00:07:23,844 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created rolling feature: sales_roll_21_min\u001b[0m\n",
      "[ 2025-09-30 00:07:23,920 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created rolling feature: sales_roll_21_max\u001b[0m\n",
      "[ 2025-09-30 00:07:24,139 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created rolling feature: sales_roll_21_median\u001b[0m\n",
      "[ 2025-09-30 00:07:24,188 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created rolling feature: sales_roll_30_mean\u001b[0m\n",
      "[ 2025-09-30 00:07:24,244 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created rolling feature: sales_roll_30_std\u001b[0m\n",
      "[ 2025-09-30 00:07:24,307 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created rolling feature: sales_roll_30_min\u001b[0m\n",
      "[ 2025-09-30 00:07:24,380 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created rolling feature: sales_roll_30_max\u001b[0m\n",
      "[ 2025-09-30 00:07:24,593 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created rolling feature: sales_roll_30_median\u001b[0m\n",
      "[ 2025-09-30 00:07:24,595 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müìà Rolling features added.\u001b[0m\n",
      "[ 2025-09-30 00:07:24,598 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müîÑ Adding cyclical features (month and dayofweek)...\u001b[0m\n",
      "[ 2025-09-30 00:07:24,654 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Cyclical features added successfully.\u001b[0m\n",
      "[ 2025-09-30 00:07:24,654 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müîÑ Cyclical features added.\u001b[0m\n",
      "[ 2025-09-30 00:07:25,146 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müßπ Handling missing values for 98 numeric columns.\u001b[0m\n",
      "[ 2025-09-30 00:07:25,233 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müîÑ Filled 1 missing values in time-based feature 'sales_lag_1' using forward/backward fill.\u001b[0m\n",
      "[ 2025-09-30 00:07:25,253 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müîÑ Filled 2 missing values in time-based feature 'sales_lag_2' using forward/backward fill.\u001b[0m\n",
      "[ 2025-09-30 00:07:25,281 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müîÑ Filled 3 missing values in time-based feature 'sales_lag_3' using forward/backward fill.\u001b[0m\n",
      "[ 2025-09-30 00:07:25,303 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müîÑ Filled 7 missing values in time-based feature 'sales_lag_7' using forward/backward fill.\u001b[0m\n",
      "[ 2025-09-30 00:07:25,325 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müîÑ Filled 14 missing values in time-based feature 'sales_lag_14' using forward/backward fill.\u001b[0m\n",
      "[ 2025-09-30 00:07:25,346 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müîÑ Filled 21 missing values in time-based feature 'sales_lag_21' using forward/backward fill.\u001b[0m\n",
      "[ 2025-09-30 00:07:25,389 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müîÑ Filled 1 missing values in time-based feature 'sales_roll_7_std' using forward/backward fill.\u001b[0m\n",
      "[ 2025-09-30 00:07:25,417 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müîÑ Filled 1 missing values in time-based feature 'sales_roll_14_std' using forward/backward fill.\u001b[0m\n",
      "[ 2025-09-30 00:07:25,447 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müîÑ Filled 30 missing values in time-based feature 'sales_lag_30' using forward/backward fill.\u001b[0m\n",
      "[ 2025-09-30 00:07:25,475 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müîÑ Filled 1 missing values in time-based feature 'sales_roll_3_std' using forward/backward fill.\u001b[0m\n",
      "[ 2025-09-30 00:07:25,521 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müîÑ Filled 1 missing values in time-based feature 'sales_roll_21_std' using forward/backward fill.\u001b[0m\n",
      "[ 2025-09-30 00:07:25,552 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müîÑ Filled 1 missing values in time-based feature 'sales_roll_30_std' using forward/backward fill.\u001b[0m\n",
      "[ 2025-09-30 00:07:25,567 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Completed handling missing values.\u001b[0m\n",
      "[ 2025-09-30 00:07:25,571 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müßπ Handled missing values.\u001b[0m\n",
      "[ 2025-09-30 00:07:25,574 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müèÅ Feature Pipeline completed successfully.\u001b[0m\n",
      "[ 2025-09-30 00:07:25,578 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Feature pipeline executed successfully.\u001b[0m\n",
      "[ 2025-09-30 00:07:25,582 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müéØ Creating target encoding for categorical columns\u001b[0m\n",
      "[ 2025-09-30 00:07:25,905 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müìä Global mean of target 'sales': 0.9237\u001b[0m\n",
      "[ 2025-09-30 00:07:25,907 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müî§ Applying target encoding for column: store_id\u001b[0m\n",
      "[ 2025-09-30 00:07:25,977 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Created target-encoded feature: store_id_target_encoded\u001b[0m\n",
      "[ 2025-09-30 00:07:25,979 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32müèÅ Target encoding completed for 1 columns.\u001b[0m\n",
      "[ 2025-09-30 00:07:25,985 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müéØ Applied target encoding to categorical columns.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\OneDrive\\Desktop\\SalesAI\\backend\\src\\features\\feature_pipeline.py:342: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  counts = df[col].value_counts()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-09-30 00:07:26,444 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müìä Data split ‚Üí Train: 230073, Val: 32867, Test: 65736\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train_df,value_df,test_df = trainer.prepare_data(\n",
    "    df=new_df,\n",
    "    target_col=\"sales\",\n",
    "    date_col=\"date\",\n",
    "    group_cols=['store_id'],\n",
    "    categorical_cols=['store_id']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "143aafd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                         0\n",
       "item_id                    0\n",
       "dept_id                    0\n",
       "cat_id                     0\n",
       "store_id                   0\n",
       "                          ..\n",
       "month_sin                  0\n",
       "month_cos                  0\n",
       "dow_sin                    0\n",
       "dow_cos                    0\n",
       "store_id_target_encoded    0\n",
       "Length: 111, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "96979e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-09-29 23:47:37,798 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müîÑ Starting feature preprocessing...\u001b[0m\n",
      "[ 2025-09-29 23:47:38,215 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'id' in Train set.\u001b[0m\n",
      "[ 2025-09-29 23:47:38,252 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'id' in Val set.\u001b[0m\n",
      "[ 2025-09-29 23:47:38,297 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'id' in Test set.\u001b[0m\n",
      "[ 2025-09-29 23:47:38,477 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'item_id' in Train set.\u001b[0m\n",
      "[ 2025-09-29 23:47:38,530 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'item_id' in Val set.\u001b[0m\n",
      "[ 2025-09-29 23:47:38,584 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'item_id' in Test set.\u001b[0m\n",
      "[ 2025-09-29 23:47:38,730 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'dept_id' in Train set.\u001b[0m\n",
      "[ 2025-09-29 23:47:38,759 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'dept_id' in Val set.\u001b[0m\n",
      "[ 2025-09-29 23:47:38,793 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'dept_id' in Test set.\u001b[0m\n",
      "[ 2025-09-29 23:47:38,906 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'cat_id' in Train set.\u001b[0m\n",
      "[ 2025-09-29 23:47:38,923 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'cat_id' in Val set.\u001b[0m\n",
      "[ 2025-09-29 23:47:38,953 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'cat_id' in Test set.\u001b[0m\n",
      "[ 2025-09-29 23:47:39,058 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'store_id' in Train set.\u001b[0m\n",
      "[ 2025-09-29 23:47:39,073 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'store_id' in Val set.\u001b[0m\n",
      "[ 2025-09-29 23:47:39,099 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'store_id' in Test set.\u001b[0m\n",
      "[ 2025-09-29 23:47:39,186 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'state_id' in Train set.\u001b[0m\n",
      "[ 2025-09-29 23:47:39,205 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'state_id' in Val set.\u001b[0m\n",
      "[ 2025-09-29 23:47:39,230 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'state_id' in Test set.\u001b[0m\n",
      "[ 2025-09-29 23:47:39,464 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'd' in Train set.\u001b[0m\n",
      "[ 2025-09-29 23:47:39,501 ] __main__ - \u001b[33mWARNING\u001b[0m - \u001b[33m‚ö†Ô∏è Unseen labels in column 'd': ['d_1360' 'd_1361' 'd_1362' 'd_1363' 'd_1364' 'd_1365' 'd_1366' 'd_1367'\n",
      " 'd_1368' 'd_1369' 'd_1370' 'd_1371' 'd_1372' 'd_1373' 'd_1374' 'd_1375'\n",
      " 'd_1376' 'd_1377' 'd_1378' 'd_1379' 'd_1380' 'd_1381' 'd_1382' 'd_1383'\n",
      " 'd_1384' 'd_1385' 'd_1386' 'd_1387' 'd_1388' 'd_1389' 'd_1390' 'd_1391'\n",
      " 'd_1392' 'd_1393' 'd_1394' 'd_1395' 'd_1396' 'd_1397' 'd_1398' 'd_1399'\n",
      " 'd_1400' 'd_1401' 'd_1402' 'd_1403' 'd_1404' 'd_1405' 'd_1406' 'd_1407'\n",
      " 'd_1408' 'd_1409' 'd_1410' 'd_1411' 'd_1412' 'd_1413' 'd_1414' 'd_1415'\n",
      " 'd_1416' 'd_1417' 'd_1418' 'd_1419' 'd_1420' 'd_1421' 'd_1422' 'd_1423'\n",
      " 'd_1424' 'd_1425' 'd_1426' 'd_1427' 'd_1428' 'd_1429' 'd_1430' 'd_1431'\n",
      " 'd_1432' 'd_1433' 'd_1434' 'd_1435' 'd_1436' 'd_1437' 'd_1438' 'd_1439'\n",
      " 'd_1440' 'd_1441' 'd_1442' 'd_1443' 'd_1444' 'd_1445' 'd_1446' 'd_1447'\n",
      " 'd_1448' 'd_1449' 'd_1450' 'd_1451' 'd_1452' 'd_1453' 'd_1454' 'd_1455'\n",
      " 'd_1456' 'd_1457' 'd_1458' 'd_1459' 'd_1460' 'd_1461' 'd_1462' 'd_1463'\n",
      " 'd_1464' 'd_1465' 'd_1466' 'd_1467' 'd_1468' 'd_1469' 'd_1470' 'd_1471'\n",
      " 'd_1472' 'd_1473' 'd_1474' 'd_1475' 'd_1476' 'd_1477' 'd_1478' 'd_1479'\n",
      " 'd_1480' 'd_1481' 'd_1482' 'd_1483' 'd_1484' 'd_1485' 'd_1486' 'd_1487'\n",
      " 'd_1488' 'd_1489' 'd_1490' 'd_1491' 'd_1492' 'd_1493' 'd_1494' 'd_1495'\n",
      " 'd_1496' 'd_1497' 'd_1498' 'd_1499' 'd_1500' 'd_1501' 'd_1502' 'd_1503'\n",
      " 'd_1504' 'd_1505' 'd_1506' 'd_1507' 'd_1508' 'd_1509' 'd_1510' 'd_1511'\n",
      " 'd_1512' 'd_1513' 'd_1514' 'd_1515' 'd_1516' 'd_1517' 'd_1518' 'd_1519'\n",
      " 'd_1520' 'd_1521' 'd_1522' 'd_1523' 'd_1524' 'd_1525' 'd_1526' 'd_1527'\n",
      " 'd_1528' 'd_1529' 'd_1530' 'd_1531' 'd_1532' 'd_1533' 'd_1534' 'd_1535'\n",
      " 'd_1536' 'd_1537' 'd_1538' 'd_1539' 'd_1540' 'd_1541' 'd_1542' 'd_1543'\n",
      " 'd_1544' 'd_1545' 'd_1546' 'd_1547' 'd_1548' 'd_1549' 'd_1550' 'd_1551'\n",
      " 'd_1552' 'd_1553']\u001b[0m\n",
      "[ 2025-09-29 23:47:39,525 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'd' in Val set.\u001b[0m\n",
      "[ 2025-09-29 23:47:39,561 ] __main__ - \u001b[33mWARNING\u001b[0m - \u001b[33m‚ö†Ô∏è Unseen labels in column 'd': ['d_1553' 'd_1554' 'd_1555' 'd_1556' 'd_1557' 'd_1558' 'd_1559' 'd_1560'\n",
      " 'd_1561' 'd_1562' 'd_1563' 'd_1564' 'd_1565' 'd_1566' 'd_1567' 'd_1568'\n",
      " 'd_1569' 'd_1570' 'd_1571' 'd_1572' 'd_1573' 'd_1574' 'd_1575' 'd_1576'\n",
      " 'd_1577' 'd_1578' 'd_1579' 'd_1580' 'd_1581' 'd_1582' 'd_1583' 'd_1584'\n",
      " 'd_1585' 'd_1586' 'd_1587' 'd_1588' 'd_1589' 'd_1590' 'd_1591' 'd_1592'\n",
      " 'd_1593' 'd_1594' 'd_1595' 'd_1596' 'd_1597' 'd_1598' 'd_1599' 'd_1600'\n",
      " 'd_1601' 'd_1602' 'd_1603' 'd_1604' 'd_1605' 'd_1606' 'd_1607' 'd_1608'\n",
      " 'd_1609' 'd_1610' 'd_1611' 'd_1612' 'd_1613' 'd_1614' 'd_1615' 'd_1616'\n",
      " 'd_1617' 'd_1618' 'd_1619' 'd_1620' 'd_1621' 'd_1622' 'd_1623' 'd_1624'\n",
      " 'd_1625' 'd_1626' 'd_1627' 'd_1628' 'd_1629' 'd_1630' 'd_1631' 'd_1632'\n",
      " 'd_1633' 'd_1634' 'd_1635' 'd_1636' 'd_1637' 'd_1638' 'd_1639' 'd_1640'\n",
      " 'd_1641' 'd_1642' 'd_1643' 'd_1644' 'd_1645' 'd_1646' 'd_1647' 'd_1648'\n",
      " 'd_1649' 'd_1650' 'd_1651' 'd_1652' 'd_1653' 'd_1654' 'd_1655' 'd_1656'\n",
      " 'd_1657' 'd_1658' 'd_1659' 'd_1660' 'd_1661' 'd_1662' 'd_1663' 'd_1664'\n",
      " 'd_1665' 'd_1666' 'd_1667' 'd_1668' 'd_1669' 'd_1670' 'd_1671' 'd_1672'\n",
      " 'd_1673' 'd_1674' 'd_1675' 'd_1676' 'd_1677' 'd_1678' 'd_1679' 'd_1680'\n",
      " 'd_1681' 'd_1682' 'd_1683' 'd_1684' 'd_1685' 'd_1686' 'd_1687' 'd_1688'\n",
      " 'd_1689' 'd_1690' 'd_1691' 'd_1692' 'd_1693' 'd_1694' 'd_1695' 'd_1696'\n",
      " 'd_1697' 'd_1698' 'd_1699' 'd_1700' 'd_1701' 'd_1702' 'd_1703' 'd_1704'\n",
      " 'd_1705' 'd_1706' 'd_1707' 'd_1708' 'd_1709' 'd_1710' 'd_1711' 'd_1712'\n",
      " 'd_1713' 'd_1714' 'd_1715' 'd_1716' 'd_1717' 'd_1718' 'd_1719' 'd_1720'\n",
      " 'd_1721' 'd_1722' 'd_1723' 'd_1724' 'd_1725' 'd_1726' 'd_1727' 'd_1728'\n",
      " 'd_1729' 'd_1730' 'd_1731' 'd_1732' 'd_1733' 'd_1734' 'd_1735' 'd_1736'\n",
      " 'd_1737' 'd_1738' 'd_1739' 'd_1740' 'd_1741' 'd_1742' 'd_1743' 'd_1744'\n",
      " 'd_1745' 'd_1746' 'd_1747' 'd_1748' 'd_1749' 'd_1750' 'd_1751' 'd_1752'\n",
      " 'd_1753' 'd_1754' 'd_1755' 'd_1756' 'd_1757' 'd_1758' 'd_1759' 'd_1760'\n",
      " 'd_1761' 'd_1762' 'd_1763' 'd_1764' 'd_1765' 'd_1766' 'd_1767' 'd_1768'\n",
      " 'd_1769' 'd_1770' 'd_1771' 'd_1772' 'd_1773' 'd_1774' 'd_1775' 'd_1776'\n",
      " 'd_1777' 'd_1778' 'd_1779' 'd_1780' 'd_1781' 'd_1782' 'd_1783' 'd_1784'\n",
      " 'd_1785' 'd_1786' 'd_1787' 'd_1788' 'd_1789' 'd_1790' 'd_1791' 'd_1792'\n",
      " 'd_1793' 'd_1794' 'd_1795' 'd_1796' 'd_1797' 'd_1798' 'd_1799' 'd_1800'\n",
      " 'd_1801' 'd_1802' 'd_1803' 'd_1804' 'd_1805' 'd_1806' 'd_1807' 'd_1808'\n",
      " 'd_1809' 'd_1810' 'd_1811' 'd_1812' 'd_1813' 'd_1814' 'd_1815' 'd_1816'\n",
      " 'd_1817' 'd_1818' 'd_1819' 'd_1820' 'd_1821' 'd_1822' 'd_1823' 'd_1824'\n",
      " 'd_1825' 'd_1826' 'd_1827' 'd_1828' 'd_1829' 'd_1830' 'd_1831' 'd_1832'\n",
      " 'd_1833' 'd_1834' 'd_1835' 'd_1836' 'd_1837' 'd_1838' 'd_1839' 'd_1840'\n",
      " 'd_1841' 'd_1842' 'd_1843' 'd_1844' 'd_1845' 'd_1846' 'd_1847' 'd_1848'\n",
      " 'd_1849' 'd_1850' 'd_1851' 'd_1852' 'd_1853' 'd_1854' 'd_1855' 'd_1856'\n",
      " 'd_1857' 'd_1858' 'd_1859' 'd_1860' 'd_1861' 'd_1862' 'd_1863' 'd_1864'\n",
      " 'd_1865' 'd_1866' 'd_1867' 'd_1868' 'd_1869' 'd_1870' 'd_1871' 'd_1872'\n",
      " 'd_1873' 'd_1874' 'd_1875' 'd_1876' 'd_1877' 'd_1878' 'd_1879' 'd_1880'\n",
      " 'd_1881' 'd_1882' 'd_1883' 'd_1884' 'd_1885' 'd_1886' 'd_1887' 'd_1888'\n",
      " 'd_1889' 'd_1890' 'd_1891' 'd_1892' 'd_1893' 'd_1894' 'd_1895' 'd_1896'\n",
      " 'd_1897' 'd_1898' 'd_1899' 'd_1900' 'd_1901' 'd_1902' 'd_1903' 'd_1904'\n",
      " 'd_1905' 'd_1906' 'd_1907' 'd_1908' 'd_1909' 'd_1910' 'd_1911' 'd_1912'\n",
      " 'd_1913' 'd_1914' 'd_1915' 'd_1916' 'd_1917' 'd_1918' 'd_1919' 'd_1920'\n",
      " 'd_1921' 'd_1922' 'd_1923' 'd_1924' 'd_1925' 'd_1926' 'd_1927' 'd_1928'\n",
      " 'd_1929' 'd_1930' 'd_1931' 'd_1932' 'd_1933' 'd_1934' 'd_1935' 'd_1936'\n",
      " 'd_1937' 'd_1938' 'd_1939' 'd_1940' 'd_1941']\u001b[0m\n",
      "[ 2025-09-29 23:47:39,588 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'd' in Test set.\u001b[0m\n",
      "[ 2025-09-29 23:47:39,704 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'event_name_1' in Train set.\u001b[0m\n",
      "[ 2025-09-29 23:47:39,728 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'event_name_1' in Val set.\u001b[0m\n",
      "[ 2025-09-29 23:47:39,763 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'event_name_1' in Test set.\u001b[0m\n",
      "[ 2025-09-29 23:47:39,866 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'event_type_1' in Train set.\u001b[0m\n",
      "[ 2025-09-29 23:47:39,885 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'event_type_1' in Val set.\u001b[0m\n",
      "[ 2025-09-29 23:47:39,917 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'event_type_1' in Test set.\u001b[0m\n",
      "[ 2025-09-29 23:47:40,035 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'event_name_2' in Train set.\u001b[0m\n",
      "[ 2025-09-29 23:47:40,054 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'event_name_2' in Val set.\u001b[0m\n",
      "[ 2025-09-29 23:47:40,089 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'event_name_2' in Test set.\u001b[0m\n",
      "[ 2025-09-29 23:47:40,183 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'event_type_2' in Train set.\u001b[0m\n",
      "[ 2025-09-29 23:47:40,204 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'event_type_2' in Val set.\u001b[0m\n",
      "[ 2025-09-29 23:47:40,235 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'event_type_2' in Test set.\u001b[0m\n",
      "[ 2025-09-29 23:47:40,358 ] __main__ - \u001b[33mWARNING\u001b[0m - \u001b[33m‚ö†Ô∏è Unseen labels in column 'store_id_target_encoded': ['0.9237273335456848']\u001b[0m\n",
      "[ 2025-09-29 23:47:40,413 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'store_id_target_encoded' in Train set.\u001b[0m\n",
      "[ 2025-09-29 23:47:40,443 ] __main__ - \u001b[33mWARNING\u001b[0m - \u001b[33m‚ö†Ô∏è Unseen labels in column 'store_id_target_encoded': ['0.9237273335456848']\u001b[0m\n",
      "[ 2025-09-29 23:47:40,460 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'store_id_target_encoded' in Val set.\u001b[0m\n",
      "[ 2025-09-29 23:47:40,504 ] __main__ - \u001b[33mWARNING\u001b[0m - \u001b[33m‚ö†Ô∏è Unseen labels in column 'store_id_target_encoded': ['0.9237273335456848']\u001b[0m\n",
      "[ 2025-09-29 23:47:40,523 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'store_id_target_encoded' in Test set.\u001b[0m\n",
      "[ 2025-09-29 23:47:43,658 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Preprocessing complete. Total features used: 109 üß†\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = trainer.preprocess_features(\n",
    "                train_df, value_df, test_df, target_col='sales'\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "003ffa56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-09-29 23:56:54,026 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚öôÔ∏è Starting LightGBM training...\u001b[0m\n",
      "[ 2025-09-29 23:56:54,030 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müì¶ Using static config parameters for LightGBM...\u001b[0m\n",
      "[ 2025-09-29 23:56:54,037 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Loaded config params: {'num_leaves': 31, 'learning_rate': 0.05, 'n_estimators': 100, 'objective': 'regression', 'random_state': 42}\u001b[0m\n",
      "[ 2025-09-29 23:56:54,040 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müõ†Ô∏è Training final LightGBM model...\u001b[0m\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's l2: 0.0580903\n",
      "[100]\tvalid_0's l2: 0.0087255\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l2: 0.0087255\n",
      "[ 2025-09-29 23:57:00,590 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ LightGBM model trained and stored.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model_light = trainer.train_lightgbm(X_train,y_train,X_val,y_val,use_optuna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "29c11b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_light.predict(X_val)\n",
    "rmse = np.sqrt(mean_squared_error(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e5174eea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09341039322822894"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "055cdb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = 'sales'\n",
    "exclude_cols = 'date'\n",
    "feature_cols = [col for col in train_df.columns ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2f53256b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature': ['id',\n",
       "  'item_id',\n",
       "  'dept_id',\n",
       "  'cat_id',\n",
       "  'store_id',\n",
       "  'state_id',\n",
       "  'd',\n",
       "  'sales',\n",
       "  'date',\n",
       "  'wm_yr_wk',\n",
       "  'weekday',\n",
       "  'wday',\n",
       "  'month',\n",
       "  'year',\n",
       "  'event_name_1',\n",
       "  'event_type_1',\n",
       "  'event_name_2',\n",
       "  'event_type_2',\n",
       "  'snap_CA',\n",
       "  'snap_TX',\n",
       "  'snap_WI',\n",
       "  'day',\n",
       "  'quarter',\n",
       "  'week_of_year',\n",
       "  'is_weekend',\n",
       "  'has_event',\n",
       "  'snap_any',\n",
       "  'sell_price',\n",
       "  'revenue',\n",
       "  'is_holiday',\n",
       "  'snap_benefit_period',\n",
       "  'price_lag_1',\n",
       "  'price_change_1d',\n",
       "  'price_increased_1d',\n",
       "  'price_decreased_1d',\n",
       "  'price_lag_7',\n",
       "  'price_change_7d',\n",
       "  'price_increased_7d',\n",
       "  'price_decreased_7d',\n",
       "  'price_lag_14',\n",
       "  'price_change_14d',\n",
       "  'price_increased_14d',\n",
       "  'price_decreased_14d',\n",
       "  'price_lag_28',\n",
       "  'price_change_28d',\n",
       "  'price_increased_28d',\n",
       "  'price_decreased_28d',\n",
       "  'price_volatility_7d',\n",
       "  'price_volatility_28d',\n",
       "  'event_sporting',\n",
       "  'event_cultural',\n",
       "  'event_national',\n",
       "  'event_religious',\n",
       "  'sales_lag_1',\n",
       "  'sales_lag_2',\n",
       "  'sales_lag_3',\n",
       "  'sales_lag_7',\n",
       "  'sales_lag_14',\n",
       "  'sales_lag_21',\n",
       "  'sales_lag_28',\n",
       "  'revenue_lag_7',\n",
       "  'revenue_lag_14',\n",
       "  'revenue_lag_28',\n",
       "  'sales_roll_7_mean',\n",
       "  'sales_roll_7_std',\n",
       "  'sales_roll_14_mean',\n",
       "  'sales_roll_14_std',\n",
       "  'sales_roll_28_mean',\n",
       "  'sales_roll_28_std',\n",
       "  'sales_roll_56_mean',\n",
       "  'sales_roll_56_std',\n",
       "  'sales_ewm_7',\n",
       "  'sales_ewm_14',\n",
       "  'sales_ewm_28',\n",
       "  'time_index',\n",
       "  'sales_velocity',\n",
       "  'sales_acceleration',\n",
       "  'sales_ratio_to_7d_avg',\n",
       "  'sales_ratio_to_28d_avg',\n",
       "  'days_since_first_sale',\n",
       "  'zero_sales_flag',\n",
       "  'consecutive_zero_days',\n",
       "  'dayofweek',\n",
       "  'weekofyear',\n",
       "  'sales_lag_30',\n",
       "  'sales_roll_3_mean',\n",
       "  'sales_roll_3_std',\n",
       "  'sales_roll_3_min',\n",
       "  'sales_roll_3_max',\n",
       "  'sales_roll_3_median',\n",
       "  'sales_roll_7_min',\n",
       "  'sales_roll_7_max',\n",
       "  'sales_roll_7_median',\n",
       "  'sales_roll_14_min',\n",
       "  'sales_roll_14_max',\n",
       "  'sales_roll_14_median',\n",
       "  'sales_roll_21_mean',\n",
       "  'sales_roll_21_std',\n",
       "  'sales_roll_21_min',\n",
       "  'sales_roll_21_max',\n",
       "  'sales_roll_21_median',\n",
       "  'sales_roll_30_mean',\n",
       "  'sales_roll_30_std',\n",
       "  'sales_roll_30_min',\n",
       "  'sales_roll_30_max',\n",
       "  'sales_roll_30_median',\n",
       "  'month_sin',\n",
       "  'month_cos',\n",
       "  'dow_sin',\n",
       "  'dow_cos',\n",
       "  'store_id_target_encoded'],\n",
       " 'importance': array([  0,   0,   0,   0,   0,   0,   1,   7,   0,   1,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   2,   0,   0,   0,   0,   0,  11,\n",
       "        116,   0,   0,   0,   0,   0,   0,   1,   0,   0,   0,   3,   0,\n",
       "          0,   0,  11,   0,   0,   0,   0,   0,   0,   0,   0,   0, 148,\n",
       "         74,   0,   1,   0,   0,   0,   2,   4,   0,  44,   8,  19,   6,\n",
       "        244,  17,  11,   7, 335,   8,   5,  10, 180, 143, 254, 262,   0,\n",
       "          0,   0,   0,   0,   0, 162, 108,   4, 706,  38,   0,   6,   0,\n",
       "          0,   3,   0,  13,   1,   0,  12,   0,   6,   0,   0,   4,   0,\n",
       "          0,   2,   0,   0,   0])}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_importance =({\n",
    "                'feature': feature_cols,\n",
    "                'importance': model_light.feature_importances_\n",
    "            })\n",
    "lgb_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "81a58642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-09-30 00:01:11,993 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müöÄ Starting training for XGBoost model...\u001b[0m\n",
      "[ 2025-09-30 00:01:12,043 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚öôÔ∏è Using tree_method: `hist`\u001b[0m\n",
      "[ 2025-09-30 00:01:12,044 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müì¶ Using config-defined hyperparameters for XGBoost...\u001b[0m\n",
      "[ 2025-09-30 00:01:12,047 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Loaded XGBoost params: {'n_estimators': 100, 'max_depth': 6, 'learning_rate': 0.1, 'objective': 'reg:squarederror', 'random_state': 42}\u001b[0m\n",
      "[ 2025-09-30 00:01:12,050 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müõ†Ô∏è Training final XGBoost model...\u001b[0m\n",
      "[0]\tvalidation_0-rmse:2.29048\n",
      "[1]\tvalidation_0-rmse:2.06796\n",
      "[2]\tvalidation_0-rmse:1.86772\n",
      "[3]\tvalidation_0-rmse:1.68659\n",
      "[4]\tvalidation_0-rmse:1.52195\n",
      "[5]\tvalidation_0-rmse:1.37407\n",
      "[6]\tvalidation_0-rmse:1.24226\n",
      "[7]\tvalidation_0-rmse:1.12233\n",
      "[8]\tvalidation_0-rmse:1.01449\n",
      "[9]\tvalidation_0-rmse:0.91729\n",
      "[10]\tvalidation_0-rmse:0.82957\n",
      "[11]\tvalidation_0-rmse:0.75128\n",
      "[12]\tvalidation_0-rmse:0.68052\n",
      "[13]\tvalidation_0-rmse:0.61559\n",
      "[14]\tvalidation_0-rmse:0.55765\n",
      "[15]\tvalidation_0-rmse:0.50591\n",
      "[16]\tvalidation_0-rmse:0.45936\n",
      "[17]\tvalidation_0-rmse:0.41790\n",
      "[18]\tvalidation_0-rmse:0.38034\n",
      "[19]\tvalidation_0-rmse:0.34668\n",
      "[20]\tvalidation_0-rmse:0.31650\n",
      "[21]\tvalidation_0-rmse:0.28984\n",
      "[22]\tvalidation_0-rmse:0.26591\n",
      "[23]\tvalidation_0-rmse:0.24470\n",
      "[24]\tvalidation_0-rmse:0.22589\n",
      "[25]\tvalidation_0-rmse:0.20929\n",
      "[26]\tvalidation_0-rmse:0.19401\n",
      "[27]\tvalidation_0-rmse:0.18054\n",
      "[28]\tvalidation_0-rmse:0.16857\n",
      "[29]\tvalidation_0-rmse:0.15819\n",
      "[30]\tvalidation_0-rmse:0.14904\n",
      "[31]\tvalidation_0-rmse:0.14116\n",
      "[32]\tvalidation_0-rmse:0.13401\n",
      "[33]\tvalidation_0-rmse:0.12806\n",
      "[34]\tvalidation_0-rmse:0.12265\n",
      "[35]\tvalidation_0-rmse:0.11808\n",
      "[36]\tvalidation_0-rmse:0.11382\n",
      "[37]\tvalidation_0-rmse:0.11023\n",
      "[38]\tvalidation_0-rmse:0.10713\n",
      "[39]\tvalidation_0-rmse:0.10452\n",
      "[40]\tvalidation_0-rmse:0.10214\n",
      "[41]\tvalidation_0-rmse:0.10009\n",
      "[42]\tvalidation_0-rmse:0.09812\n",
      "[43]\tvalidation_0-rmse:0.09635\n",
      "[44]\tvalidation_0-rmse:0.09477\n",
      "[45]\tvalidation_0-rmse:0.09318\n",
      "[46]\tvalidation_0-rmse:0.09220\n",
      "[47]\tvalidation_0-rmse:0.09108\n",
      "[48]\tvalidation_0-rmse:0.09018\n",
      "[49]\tvalidation_0-rmse:0.08943\n",
      "[50]\tvalidation_0-rmse:0.08871\n",
      "[51]\tvalidation_0-rmse:0.08792\n",
      "[52]\tvalidation_0-rmse:0.08737\n",
      "[53]\tvalidation_0-rmse:0.08662\n",
      "[54]\tvalidation_0-rmse:0.08583\n",
      "[55]\tvalidation_0-rmse:0.08543\n",
      "[56]\tvalidation_0-rmse:0.08488\n",
      "[57]\tvalidation_0-rmse:0.08431\n",
      "[58]\tvalidation_0-rmse:0.08393\n",
      "[59]\tvalidation_0-rmse:0.08353\n",
      "[60]\tvalidation_0-rmse:0.08289\n",
      "[61]\tvalidation_0-rmse:0.08264\n",
      "[62]\tvalidation_0-rmse:0.08245\n",
      "[63]\tvalidation_0-rmse:0.08200\n",
      "[64]\tvalidation_0-rmse:0.08173\n",
      "[65]\tvalidation_0-rmse:0.08152\n",
      "[66]\tvalidation_0-rmse:0.08096\n",
      "[67]\tvalidation_0-rmse:0.08099\n",
      "[68]\tvalidation_0-rmse:0.08073\n",
      "[69]\tvalidation_0-rmse:0.08022\n",
      "[70]\tvalidation_0-rmse:0.08001\n",
      "[71]\tvalidation_0-rmse:0.07953\n",
      "[72]\tvalidation_0-rmse:0.07945\n",
      "[73]\tvalidation_0-rmse:0.07929\n",
      "[74]\tvalidation_0-rmse:0.07906\n",
      "[75]\tvalidation_0-rmse:0.07888\n",
      "[76]\tvalidation_0-rmse:0.07872\n",
      "[77]\tvalidation_0-rmse:0.07843\n",
      "[78]\tvalidation_0-rmse:0.07823\n",
      "[79]\tvalidation_0-rmse:0.07796\n",
      "[80]\tvalidation_0-rmse:0.07787\n",
      "[81]\tvalidation_0-rmse:0.07752\n",
      "[82]\tvalidation_0-rmse:0.07742\n",
      "[83]\tvalidation_0-rmse:0.07731\n",
      "[84]\tvalidation_0-rmse:0.07699\n",
      "[85]\tvalidation_0-rmse:0.07689\n",
      "[86]\tvalidation_0-rmse:0.07683\n",
      "[87]\tvalidation_0-rmse:0.07659\n",
      "[88]\tvalidation_0-rmse:0.07647\n",
      "[89]\tvalidation_0-rmse:0.07644\n",
      "[90]\tvalidation_0-rmse:0.07638\n",
      "[91]\tvalidation_0-rmse:0.07615\n",
      "[92]\tvalidation_0-rmse:0.07580\n",
      "[93]\tvalidation_0-rmse:0.07568\n",
      "[94]\tvalidation_0-rmse:0.07567\n",
      "[95]\tvalidation_0-rmse:0.07554\n",
      "[96]\tvalidation_0-rmse:0.07549\n",
      "[97]\tvalidation_0-rmse:0.07528\n",
      "[98]\tvalidation_0-rmse:0.07500\n",
      "[99]\tvalidation_0-rmse:0.07480\n",
      "[ 2025-09-30 00:01:18,789 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Model trained successfully. Best iteration: 99\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model_xgb = trainer.train_xgboost(X_train, y_train, X_val, y_val, use_optuna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4e9ef998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-09-30 00:02:23,872 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müìä Calculating model evaluation metrics...\u001b[0m\n",
      "[ 2025-09-30 00:02:23,951 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Metrics calculated: {'rmse': 0.1025, 'mae': 0.0249, 'mape': 1.9689, 'r2': 0.9987}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "xgp_pred = model_xgb.predict(X_test)\n",
    "xgb_metric = trainer.calculate_metrics(y_test,xgp_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dde6c0da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rmse': 0.1025, 'mae': 0.0249, 'mape': 1.9689, 'r2': 0.9987}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a2682194",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = ({\n",
    "                'feature': feature_cols,\n",
    "                'importance': model_xgb.feature_importances_\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "13500bfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature': ['id',\n",
       "  'item_id',\n",
       "  'dept_id',\n",
       "  'cat_id',\n",
       "  'store_id',\n",
       "  'state_id',\n",
       "  'd',\n",
       "  'sales',\n",
       "  'date',\n",
       "  'wm_yr_wk',\n",
       "  'weekday',\n",
       "  'wday',\n",
       "  'month',\n",
       "  'year',\n",
       "  'event_name_1',\n",
       "  'event_type_1',\n",
       "  'event_name_2',\n",
       "  'event_type_2',\n",
       "  'snap_CA',\n",
       "  'snap_TX',\n",
       "  'snap_WI',\n",
       "  'day',\n",
       "  'quarter',\n",
       "  'week_of_year',\n",
       "  'is_weekend',\n",
       "  'has_event',\n",
       "  'snap_any',\n",
       "  'sell_price',\n",
       "  'revenue',\n",
       "  'is_holiday',\n",
       "  'snap_benefit_period',\n",
       "  'price_lag_1',\n",
       "  'price_change_1d',\n",
       "  'price_increased_1d',\n",
       "  'price_decreased_1d',\n",
       "  'price_lag_7',\n",
       "  'price_change_7d',\n",
       "  'price_increased_7d',\n",
       "  'price_decreased_7d',\n",
       "  'price_lag_14',\n",
       "  'price_change_14d',\n",
       "  'price_increased_14d',\n",
       "  'price_decreased_14d',\n",
       "  'price_lag_28',\n",
       "  'price_change_28d',\n",
       "  'price_increased_28d',\n",
       "  'price_decreased_28d',\n",
       "  'price_volatility_7d',\n",
       "  'price_volatility_28d',\n",
       "  'event_sporting',\n",
       "  'event_cultural',\n",
       "  'event_national',\n",
       "  'event_religious',\n",
       "  'sales_lag_1',\n",
       "  'sales_lag_2',\n",
       "  'sales_lag_3',\n",
       "  'sales_lag_7',\n",
       "  'sales_lag_14',\n",
       "  'sales_lag_21',\n",
       "  'sales_lag_28',\n",
       "  'revenue_lag_7',\n",
       "  'revenue_lag_14',\n",
       "  'revenue_lag_28',\n",
       "  'sales_roll_7_mean',\n",
       "  'sales_roll_7_std',\n",
       "  'sales_roll_14_mean',\n",
       "  'sales_roll_14_std',\n",
       "  'sales_roll_28_mean',\n",
       "  'sales_roll_28_std',\n",
       "  'sales_roll_56_mean',\n",
       "  'sales_roll_56_std',\n",
       "  'sales_ewm_7',\n",
       "  'sales_ewm_14',\n",
       "  'sales_ewm_28',\n",
       "  'time_index',\n",
       "  'sales_velocity',\n",
       "  'sales_acceleration',\n",
       "  'sales_ratio_to_7d_avg',\n",
       "  'sales_ratio_to_28d_avg',\n",
       "  'days_since_first_sale',\n",
       "  'zero_sales_flag',\n",
       "  'consecutive_zero_days',\n",
       "  'dayofweek',\n",
       "  'weekofyear',\n",
       "  'sales_lag_30',\n",
       "  'sales_roll_3_mean',\n",
       "  'sales_roll_3_std',\n",
       "  'sales_roll_3_min',\n",
       "  'sales_roll_3_max',\n",
       "  'sales_roll_3_median',\n",
       "  'sales_roll_7_min',\n",
       "  'sales_roll_7_max',\n",
       "  'sales_roll_7_median',\n",
       "  'sales_roll_14_min',\n",
       "  'sales_roll_14_max',\n",
       "  'sales_roll_14_median',\n",
       "  'sales_roll_21_mean',\n",
       "  'sales_roll_21_std',\n",
       "  'sales_roll_21_min',\n",
       "  'sales_roll_21_max',\n",
       "  'sales_roll_21_median',\n",
       "  'sales_roll_30_mean',\n",
       "  'sales_roll_30_std',\n",
       "  'sales_roll_30_min',\n",
       "  'sales_roll_30_max',\n",
       "  'sales_roll_30_median',\n",
       "  'month_sin',\n",
       "  'month_cos',\n",
       "  'dow_sin',\n",
       "  'dow_cos',\n",
       "  'store_id_target_encoded'],\n",
       " 'importance': array([6.2804884e-06, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 1.0735953e-05, 5.9981408e-05,\n",
       "        6.3641496e-06, 9.7796019e-06, 1.4110699e-05, 0.0000000e+00,\n",
       "        9.2104774e-06, 0.0000000e+00, 1.6822252e-05, 0.0000000e+00,\n",
       "        5.0963732e-05, 5.1373052e-05, 1.8432402e-05, 3.9379782e-05,\n",
       "        0.0000000e+00, 3.8742321e-05, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 1.5864887e-04, 4.2436374e-03, 0.0000000e+00,\n",
       "        0.0000000e+00, 2.0784606e-05, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 4.3834181e-04, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 3.4614114e-04, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 1.1670704e-04, 1.6439972e-05, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 3.1971787e-05, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 9.3542266e-04,\n",
       "        1.4114864e-04, 9.1415577e-06, 1.3629985e-05, 0.0000000e+00,\n",
       "        1.6353344e-05, 7.8855874e-06, 3.6171397e-05, 1.3362046e-05,\n",
       "        5.6023746e-05, 2.9127049e-04, 1.1573720e-04, 3.6101392e-05,\n",
       "        3.5080815e-05, 4.6629859e-03, 1.0575110e-04, 1.2890180e-04,\n",
       "        2.9398224e-04, 5.2684747e-02, 3.0473645e-03, 1.7276892e-04,\n",
       "        1.3380450e-04, 3.8563120e-01, 1.0246681e-04, 6.8550527e-02,\n",
       "        5.6559093e-02, 0.0000000e+00, 2.3321186e-01, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 4.0297586e-04,\n",
       "        1.3048577e-01, 4.3051496e-05, 5.5787668e-02, 1.2135854e-04,\n",
       "        0.0000000e+00, 5.9497866e-05, 1.5419293e-05, 0.0000000e+00,\n",
       "        1.2488739e-04, 6.8344584e-06, 5.4110616e-05, 2.8739574e-05,\n",
       "        0.0000000e+00, 4.2190470e-05, 1.6082635e-05, 2.6002079e-05,\n",
       "        6.2232693e-06, 0.0000000e+00, 2.0211917e-05, 0.0000000e+00,\n",
       "        1.7378003e-05, 2.2147611e-05, 1.1485146e-05, 1.0240833e-05,\n",
       "        0.0000000e+00], dtype=float32)}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d69d4542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-09-30 00:07:34,982 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32mTraining Prophet model\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:08:18 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-09-30 00:08:18,272 ] cmdstanpy - \u001b[32mINFO\u001b[0m - \u001b[32mChain [1] start processing\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:13:18 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-09-30 00:13:18,668 ] cmdstanpy - \u001b[32mINFO\u001b[0m - \u001b[32mChain [1] done processing\u001b[0m\n",
      "[ 2025-09-30 00:13:23,636 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32mProphet Val RMSE: 2.5317, MAE: 1.1956, R2: 0.0011\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model_prophet = trainer.train_prophet(train_df,value_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "489e9642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-09-30 00:29:11,240 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müöÄ Starting full model training pipeline...\u001b[0m\n",
      "[ 2025-09-30 00:29:11,243 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müßπ Preprocessing features and target variables...\u001b[0m\n",
      "[ 2025-09-30 00:29:11,247 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müîÑ Starting feature preprocessing...\u001b[0m\n",
      "[ 2025-09-30 00:29:12,324 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'id' in Train set.\u001b[0m\n",
      "[ 2025-09-30 00:29:12,371 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'id' in Val set.\u001b[0m\n",
      "[ 2025-09-30 00:29:12,424 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'id' in Test set.\u001b[0m\n",
      "[ 2025-09-30 00:29:12,662 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'item_id' in Train set.\u001b[0m\n",
      "[ 2025-09-30 00:29:12,693 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'item_id' in Val set.\u001b[0m\n",
      "[ 2025-09-30 00:29:12,736 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'item_id' in Test set.\u001b[0m\n",
      "[ 2025-09-30 00:29:12,922 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'dept_id' in Train set.\u001b[0m\n",
      "[ 2025-09-30 00:29:12,947 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'dept_id' in Val set.\u001b[0m\n",
      "[ 2025-09-30 00:29:13,000 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'dept_id' in Test set.\u001b[0m\n",
      "[ 2025-09-30 00:29:13,231 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'cat_id' in Train set.\u001b[0m\n",
      "[ 2025-09-30 00:29:13,260 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'cat_id' in Val set.\u001b[0m\n",
      "[ 2025-09-30 00:29:13,307 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'cat_id' in Test set.\u001b[0m\n",
      "[ 2025-09-30 00:29:13,482 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'store_id' in Train set.\u001b[0m\n",
      "[ 2025-09-30 00:29:13,503 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'store_id' in Val set.\u001b[0m\n",
      "[ 2025-09-30 00:29:13,534 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'store_id' in Test set.\u001b[0m\n",
      "[ 2025-09-30 00:29:13,680 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'state_id' in Train set.\u001b[0m\n",
      "[ 2025-09-30 00:29:13,697 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'state_id' in Val set.\u001b[0m\n",
      "[ 2025-09-30 00:29:13,722 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'state_id' in Test set.\u001b[0m\n",
      "[ 2025-09-30 00:29:13,902 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'd' in Train set.\u001b[0m\n",
      "[ 2025-09-30 00:29:13,937 ] __main__ - \u001b[33mWARNING\u001b[0m - \u001b[33m‚ö†Ô∏è Unseen labels in column 'd': ['d_1359' 'd_1360' 'd_1361' 'd_1362' 'd_1363' 'd_1364' 'd_1365' 'd_1366'\n",
      " 'd_1367' 'd_1368' 'd_1369' 'd_1370' 'd_1371' 'd_1372' 'd_1373' 'd_1374'\n",
      " 'd_1375' 'd_1376' 'd_1377' 'd_1378' 'd_1379' 'd_1380' 'd_1381' 'd_1382'\n",
      " 'd_1383' 'd_1384' 'd_1385' 'd_1386' 'd_1387' 'd_1388' 'd_1389' 'd_1390'\n",
      " 'd_1391' 'd_1392' 'd_1393' 'd_1394' 'd_1395' 'd_1396' 'd_1397' 'd_1398'\n",
      " 'd_1399' 'd_1400' 'd_1401' 'd_1402' 'd_1403' 'd_1404' 'd_1405' 'd_1406'\n",
      " 'd_1407' 'd_1408' 'd_1409' 'd_1410' 'd_1411' 'd_1412' 'd_1413' 'd_1414'\n",
      " 'd_1415' 'd_1416' 'd_1417' 'd_1418' 'd_1419' 'd_1420' 'd_1421' 'd_1422'\n",
      " 'd_1423' 'd_1424' 'd_1425' 'd_1426' 'd_1427' 'd_1428' 'd_1429' 'd_1430'\n",
      " 'd_1431' 'd_1432' 'd_1433' 'd_1434' 'd_1435' 'd_1436' 'd_1437' 'd_1438'\n",
      " 'd_1439' 'd_1440' 'd_1441' 'd_1442' 'd_1443' 'd_1444' 'd_1445' 'd_1446'\n",
      " 'd_1447' 'd_1448' 'd_1449' 'd_1450' 'd_1451' 'd_1452' 'd_1453' 'd_1454'\n",
      " 'd_1455' 'd_1456' 'd_1457' 'd_1458' 'd_1459' 'd_1460' 'd_1461' 'd_1462'\n",
      " 'd_1463' 'd_1464' 'd_1465' 'd_1466' 'd_1467' 'd_1468' 'd_1469' 'd_1470'\n",
      " 'd_1471' 'd_1472' 'd_1473' 'd_1474' 'd_1475' 'd_1476' 'd_1477' 'd_1478'\n",
      " 'd_1479' 'd_1480' 'd_1481' 'd_1482' 'd_1483' 'd_1484' 'd_1485' 'd_1486'\n",
      " 'd_1487' 'd_1488' 'd_1489' 'd_1490' 'd_1491' 'd_1492' 'd_1493' 'd_1494'\n",
      " 'd_1495' 'd_1496' 'd_1497' 'd_1498' 'd_1499' 'd_1500' 'd_1501' 'd_1502'\n",
      " 'd_1503' 'd_1504' 'd_1505' 'd_1506' 'd_1507' 'd_1508' 'd_1509' 'd_1510'\n",
      " 'd_1511' 'd_1512' 'd_1513' 'd_1514' 'd_1515' 'd_1516' 'd_1517' 'd_1518'\n",
      " 'd_1519' 'd_1520' 'd_1521' 'd_1522' 'd_1523' 'd_1524' 'd_1525' 'd_1526'\n",
      " 'd_1527' 'd_1528' 'd_1529' 'd_1530' 'd_1531' 'd_1532' 'd_1533' 'd_1534'\n",
      " 'd_1535' 'd_1536' 'd_1537' 'd_1538' 'd_1539' 'd_1540' 'd_1541' 'd_1542'\n",
      " 'd_1543' 'd_1544' 'd_1545' 'd_1546' 'd_1547' 'd_1548' 'd_1549' 'd_1550'\n",
      " 'd_1551' 'd_1552' 'd_1553']\u001b[0m\n",
      "[ 2025-09-30 00:29:13,957 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'd' in Val set.\u001b[0m\n",
      "[ 2025-09-30 00:29:13,991 ] __main__ - \u001b[33mWARNING\u001b[0m - \u001b[33m‚ö†Ô∏è Unseen labels in column 'd': ['d_1553' 'd_1554' 'd_1555' 'd_1556' 'd_1557' 'd_1558' 'd_1559' 'd_1560'\n",
      " 'd_1561' 'd_1562' 'd_1563' 'd_1564' 'd_1565' 'd_1566' 'd_1567' 'd_1568'\n",
      " 'd_1569' 'd_1570' 'd_1571' 'd_1572' 'd_1573' 'd_1574' 'd_1575' 'd_1576'\n",
      " 'd_1577' 'd_1578' 'd_1579' 'd_1580' 'd_1581' 'd_1582' 'd_1583' 'd_1584'\n",
      " 'd_1585' 'd_1586' 'd_1587' 'd_1588' 'd_1589' 'd_1590' 'd_1591' 'd_1592'\n",
      " 'd_1593' 'd_1594' 'd_1595' 'd_1596' 'd_1597' 'd_1598' 'd_1599' 'd_1600'\n",
      " 'd_1601' 'd_1602' 'd_1603' 'd_1604' 'd_1605' 'd_1606' 'd_1607' 'd_1608'\n",
      " 'd_1609' 'd_1610' 'd_1611' 'd_1612' 'd_1613' 'd_1614' 'd_1615' 'd_1616'\n",
      " 'd_1617' 'd_1618' 'd_1619' 'd_1620' 'd_1621' 'd_1622' 'd_1623' 'd_1624'\n",
      " 'd_1625' 'd_1626' 'd_1627' 'd_1628' 'd_1629' 'd_1630' 'd_1631' 'd_1632'\n",
      " 'd_1633' 'd_1634' 'd_1635' 'd_1636' 'd_1637' 'd_1638' 'd_1639' 'd_1640'\n",
      " 'd_1641' 'd_1642' 'd_1643' 'd_1644' 'd_1645' 'd_1646' 'd_1647' 'd_1648'\n",
      " 'd_1649' 'd_1650' 'd_1651' 'd_1652' 'd_1653' 'd_1654' 'd_1655' 'd_1656'\n",
      " 'd_1657' 'd_1658' 'd_1659' 'd_1660' 'd_1661' 'd_1662' 'd_1663' 'd_1664'\n",
      " 'd_1665' 'd_1666' 'd_1667' 'd_1668' 'd_1669' 'd_1670' 'd_1671' 'd_1672'\n",
      " 'd_1673' 'd_1674' 'd_1675' 'd_1676' 'd_1677' 'd_1678' 'd_1679' 'd_1680'\n",
      " 'd_1681' 'd_1682' 'd_1683' 'd_1684' 'd_1685' 'd_1686' 'd_1687' 'd_1688'\n",
      " 'd_1689' 'd_1690' 'd_1691' 'd_1692' 'd_1693' 'd_1694' 'd_1695' 'd_1696'\n",
      " 'd_1697' 'd_1698' 'd_1699' 'd_1700' 'd_1701' 'd_1702' 'd_1703' 'd_1704'\n",
      " 'd_1705' 'd_1706' 'd_1707' 'd_1708' 'd_1709' 'd_1710' 'd_1711' 'd_1712'\n",
      " 'd_1713' 'd_1714' 'd_1715' 'd_1716' 'd_1717' 'd_1718' 'd_1719' 'd_1720'\n",
      " 'd_1721' 'd_1722' 'd_1723' 'd_1724' 'd_1725' 'd_1726' 'd_1727' 'd_1728'\n",
      " 'd_1729' 'd_1730' 'd_1731' 'd_1732' 'd_1733' 'd_1734' 'd_1735' 'd_1736'\n",
      " 'd_1737' 'd_1738' 'd_1739' 'd_1740' 'd_1741' 'd_1742' 'd_1743' 'd_1744'\n",
      " 'd_1745' 'd_1746' 'd_1747' 'd_1748' 'd_1749' 'd_1750' 'd_1751' 'd_1752'\n",
      " 'd_1753' 'd_1754' 'd_1755' 'd_1756' 'd_1757' 'd_1758' 'd_1759' 'd_1760'\n",
      " 'd_1761' 'd_1762' 'd_1763' 'd_1764' 'd_1765' 'd_1766' 'd_1767' 'd_1768'\n",
      " 'd_1769' 'd_1770' 'd_1771' 'd_1772' 'd_1773' 'd_1774' 'd_1775' 'd_1776'\n",
      " 'd_1777' 'd_1778' 'd_1779' 'd_1780' 'd_1781' 'd_1782' 'd_1783' 'd_1784'\n",
      " 'd_1785' 'd_1786' 'd_1787' 'd_1788' 'd_1789' 'd_1790' 'd_1791' 'd_1792'\n",
      " 'd_1793' 'd_1794' 'd_1795' 'd_1796' 'd_1797' 'd_1798' 'd_1799' 'd_1800'\n",
      " 'd_1801' 'd_1802' 'd_1803' 'd_1804' 'd_1805' 'd_1806' 'd_1807' 'd_1808'\n",
      " 'd_1809' 'd_1810' 'd_1811' 'd_1812' 'd_1813' 'd_1814' 'd_1815' 'd_1816'\n",
      " 'd_1817' 'd_1818' 'd_1819' 'd_1820' 'd_1821' 'd_1822' 'd_1823' 'd_1824'\n",
      " 'd_1825' 'd_1826' 'd_1827' 'd_1828' 'd_1829' 'd_1830' 'd_1831' 'd_1832'\n",
      " 'd_1833' 'd_1834' 'd_1835' 'd_1836' 'd_1837' 'd_1838' 'd_1839' 'd_1840'\n",
      " 'd_1841' 'd_1842' 'd_1843' 'd_1844' 'd_1845' 'd_1846' 'd_1847' 'd_1848'\n",
      " 'd_1849' 'd_1850' 'd_1851' 'd_1852' 'd_1853' 'd_1854' 'd_1855' 'd_1856'\n",
      " 'd_1857' 'd_1858' 'd_1859' 'd_1860' 'd_1861' 'd_1862' 'd_1863' 'd_1864'\n",
      " 'd_1865' 'd_1866' 'd_1867' 'd_1868' 'd_1869' 'd_1870' 'd_1871' 'd_1872'\n",
      " 'd_1873' 'd_1874' 'd_1875' 'd_1876' 'd_1877' 'd_1878' 'd_1879' 'd_1880'\n",
      " 'd_1881' 'd_1882' 'd_1883' 'd_1884' 'd_1885' 'd_1886' 'd_1887' 'd_1888'\n",
      " 'd_1889' 'd_1890' 'd_1891' 'd_1892' 'd_1893' 'd_1894' 'd_1895' 'd_1896'\n",
      " 'd_1897' 'd_1898' 'd_1899' 'd_1900' 'd_1901' 'd_1902' 'd_1903' 'd_1904'\n",
      " 'd_1905' 'd_1906' 'd_1907' 'd_1908' 'd_1909' 'd_1910' 'd_1911' 'd_1912'\n",
      " 'd_1913' 'd_1914' 'd_1915' 'd_1916' 'd_1917' 'd_1918' 'd_1919' 'd_1920'\n",
      " 'd_1921' 'd_1922' 'd_1923' 'd_1924' 'd_1925' 'd_1926' 'd_1927' 'd_1928'\n",
      " 'd_1929' 'd_1930' 'd_1931' 'd_1932' 'd_1933' 'd_1934' 'd_1935' 'd_1936'\n",
      " 'd_1937' 'd_1938' 'd_1939' 'd_1940' 'd_1941']\u001b[0m\n",
      "[ 2025-09-30 00:29:14,024 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'd' in Test set.\u001b[0m\n",
      "[ 2025-09-30 00:29:14,242 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'event_name_1' in Train set.\u001b[0m\n",
      "[ 2025-09-30 00:29:14,273 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'event_name_1' in Val set.\u001b[0m\n",
      "[ 2025-09-30 00:29:14,315 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'event_name_1' in Test set.\u001b[0m\n",
      "[ 2025-09-30 00:29:14,540 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'event_type_1' in Train set.\u001b[0m\n",
      "[ 2025-09-30 00:29:14,561 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'event_type_1' in Val set.\u001b[0m\n",
      "[ 2025-09-30 00:29:14,601 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'event_type_1' in Test set.\u001b[0m\n",
      "[ 2025-09-30 00:29:14,781 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'event_name_2' in Train set.\u001b[0m\n",
      "[ 2025-09-30 00:29:14,802 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'event_name_2' in Val set.\u001b[0m\n",
      "[ 2025-09-30 00:29:14,839 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'event_name_2' in Test set.\u001b[0m\n",
      "[ 2025-09-30 00:29:15,002 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'event_type_2' in Train set.\u001b[0m\n",
      "[ 2025-09-30 00:29:15,024 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'event_type_2' in Val set.\u001b[0m\n",
      "[ 2025-09-30 00:29:15,056 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'event_type_2' in Test set.\u001b[0m\n",
      "[ 2025-09-30 00:29:15,306 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'store_id_target_encoded' in Train set.\u001b[0m\n",
      "[ 2025-09-30 00:29:15,332 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'store_id_target_encoded' in Val set.\u001b[0m\n",
      "[ 2025-09-30 00:29:15,374 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Label encoded 'store_id_target_encoded' in Test set.\u001b[0m\n",
      "[ 2025-09-30 00:29:18,488 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Preprocessing complete. Total features used: 109 üß†\u001b[0m\n",
      "[ 2025-09-30 00:29:18,516 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müìä Data sizes - Train: 230073, Val: 32867, Test: 65736\u001b[0m\n",
      "[ 2025-09-30 00:29:18,522 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müî• Training XGBoost model...\u001b[0m\n",
      "[ 2025-09-30 00:29:18,522 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müöÄ Starting training for XGBoost model...\u001b[0m\n",
      "[ 2025-09-30 00:29:18,546 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚öôÔ∏è Using tree_method: `hist`\u001b[0m\n",
      "[ 2025-09-30 00:29:18,549 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müì¶ Using config-defined hyperparameters for XGBoost...\u001b[0m\n",
      "[ 2025-09-30 00:29:18,551 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Loaded XGBoost params: {'n_estimators': 100, 'max_depth': 6, 'learning_rate': 0.1, 'objective': 'reg:squarederror', 'random_state': 42}\u001b[0m\n",
      "[ 2025-09-30 00:29:18,553 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müõ†Ô∏è Training final XGBoost model...\u001b[0m\n",
      "[0]\tvalidation_0-rmse:2.29048\n",
      "[1]\tvalidation_0-rmse:2.06796\n",
      "[2]\tvalidation_0-rmse:1.86772\n",
      "[3]\tvalidation_0-rmse:1.68659\n",
      "[4]\tvalidation_0-rmse:1.52195\n",
      "[5]\tvalidation_0-rmse:1.37407\n",
      "[6]\tvalidation_0-rmse:1.24226\n",
      "[7]\tvalidation_0-rmse:1.12233\n",
      "[8]\tvalidation_0-rmse:1.01449\n",
      "[9]\tvalidation_0-rmse:0.91729\n",
      "[10]\tvalidation_0-rmse:0.82957\n",
      "[11]\tvalidation_0-rmse:0.75128\n",
      "[12]\tvalidation_0-rmse:0.68052\n",
      "[13]\tvalidation_0-rmse:0.61559\n",
      "[14]\tvalidation_0-rmse:0.55765\n",
      "[15]\tvalidation_0-rmse:0.50591\n",
      "[16]\tvalidation_0-rmse:0.45936\n",
      "[17]\tvalidation_0-rmse:0.41790\n",
      "[18]\tvalidation_0-rmse:0.38034\n",
      "[19]\tvalidation_0-rmse:0.34668\n",
      "[20]\tvalidation_0-rmse:0.31650\n",
      "[21]\tvalidation_0-rmse:0.28984\n",
      "[22]\tvalidation_0-rmse:0.26591\n",
      "[23]\tvalidation_0-rmse:0.24470\n",
      "[24]\tvalidation_0-rmse:0.22589\n",
      "[25]\tvalidation_0-rmse:0.20929\n",
      "[26]\tvalidation_0-rmse:0.19401\n",
      "[27]\tvalidation_0-rmse:0.18054\n",
      "[28]\tvalidation_0-rmse:0.16857\n",
      "[29]\tvalidation_0-rmse:0.15819\n",
      "[30]\tvalidation_0-rmse:0.14904\n",
      "[31]\tvalidation_0-rmse:0.14116\n",
      "[32]\tvalidation_0-rmse:0.13401\n",
      "[33]\tvalidation_0-rmse:0.12806\n",
      "[34]\tvalidation_0-rmse:0.12265\n",
      "[35]\tvalidation_0-rmse:0.11808\n",
      "[36]\tvalidation_0-rmse:0.11382\n",
      "[37]\tvalidation_0-rmse:0.11023\n",
      "[38]\tvalidation_0-rmse:0.10713\n",
      "[39]\tvalidation_0-rmse:0.10452\n",
      "[40]\tvalidation_0-rmse:0.10209\n",
      "[41]\tvalidation_0-rmse:0.10004\n",
      "[42]\tvalidation_0-rmse:0.09808\n",
      "[43]\tvalidation_0-rmse:0.09631\n",
      "[44]\tvalidation_0-rmse:0.09469\n",
      "[45]\tvalidation_0-rmse:0.09310\n",
      "[46]\tvalidation_0-rmse:0.09212\n",
      "[47]\tvalidation_0-rmse:0.09101\n",
      "[48]\tvalidation_0-rmse:0.09011\n",
      "[49]\tvalidation_0-rmse:0.08935\n",
      "[50]\tvalidation_0-rmse:0.08864\n",
      "[51]\tvalidation_0-rmse:0.08785\n",
      "[52]\tvalidation_0-rmse:0.08730\n",
      "[53]\tvalidation_0-rmse:0.08655\n",
      "[54]\tvalidation_0-rmse:0.08577\n",
      "[55]\tvalidation_0-rmse:0.08537\n",
      "[56]\tvalidation_0-rmse:0.08482\n",
      "[57]\tvalidation_0-rmse:0.08424\n",
      "[58]\tvalidation_0-rmse:0.08387\n",
      "[59]\tvalidation_0-rmse:0.08346\n",
      "[60]\tvalidation_0-rmse:0.08283\n",
      "[61]\tvalidation_0-rmse:0.08258\n",
      "[62]\tvalidation_0-rmse:0.08239\n",
      "[63]\tvalidation_0-rmse:0.08194\n",
      "[64]\tvalidation_0-rmse:0.08167\n",
      "[65]\tvalidation_0-rmse:0.08146\n",
      "[66]\tvalidation_0-rmse:0.08090\n",
      "[67]\tvalidation_0-rmse:0.08093\n",
      "[68]\tvalidation_0-rmse:0.08067\n",
      "[69]\tvalidation_0-rmse:0.08017\n",
      "[70]\tvalidation_0-rmse:0.07996\n",
      "[71]\tvalidation_0-rmse:0.07948\n",
      "[72]\tvalidation_0-rmse:0.07939\n",
      "[73]\tvalidation_0-rmse:0.07923\n",
      "[74]\tvalidation_0-rmse:0.07900\n",
      "[75]\tvalidation_0-rmse:0.07882\n",
      "[76]\tvalidation_0-rmse:0.07866\n",
      "[77]\tvalidation_0-rmse:0.07837\n",
      "[78]\tvalidation_0-rmse:0.07817\n",
      "[79]\tvalidation_0-rmse:0.07790\n",
      "[80]\tvalidation_0-rmse:0.07781\n",
      "[81]\tvalidation_0-rmse:0.07746\n",
      "[82]\tvalidation_0-rmse:0.07737\n",
      "[83]\tvalidation_0-rmse:0.07725\n",
      "[84]\tvalidation_0-rmse:0.07694\n",
      "[85]\tvalidation_0-rmse:0.07683\n",
      "[86]\tvalidation_0-rmse:0.07678\n",
      "[87]\tvalidation_0-rmse:0.07654\n",
      "[88]\tvalidation_0-rmse:0.07642\n",
      "[89]\tvalidation_0-rmse:0.07639\n",
      "[90]\tvalidation_0-rmse:0.07633\n",
      "[91]\tvalidation_0-rmse:0.07609\n",
      "[92]\tvalidation_0-rmse:0.07575\n",
      "[93]\tvalidation_0-rmse:0.07563\n",
      "[94]\tvalidation_0-rmse:0.07561\n",
      "[95]\tvalidation_0-rmse:0.07548\n",
      "[96]\tvalidation_0-rmse:0.07544\n",
      "[97]\tvalidation_0-rmse:0.07522\n",
      "[98]\tvalidation_0-rmse:0.07494\n",
      "[99]\tvalidation_0-rmse:0.07475\n",
      "[ 2025-09-30 00:29:22,502 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Model trained successfully. Best iteration: 99\u001b[0m\n",
      "[ 2025-09-30 00:29:22,558 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müìä Calculating model evaluation metrics...\u001b[0m\n",
      "[ 2025-09-30 00:29:22,581 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Metrics calculated: {'rmse': 0.1025, 'mae': 0.0249, 'mape': 1.9689, 'r2': 0.9987}\u001b[0m\n",
      "[ 2025-09-30 00:29:22,634 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müåü Top XGBoost features:\n",
      "                   feature  importance\n",
      "73          sales_velocity    0.385631\n",
      "78         zero_sales_flag    0.233212\n",
      "84        sales_roll_3_std    0.130486\n",
      "75   sales_ratio_to_7d_avg    0.068551\n",
      "76  sales_ratio_to_28d_avg    0.056559\n",
      "86        sales_roll_3_max    0.055788\n",
      "69             sales_ewm_7    0.052685\n",
      "65      sales_roll_28_mean    0.004663\n",
      "26                 revenue    0.004244\n",
      "70            sales_ewm_14    0.003047\n",
      "51             sales_lag_1    0.000935\n",
      "33             price_lag_7    0.000438\n",
      "83       sales_roll_3_mean    0.000403\n",
      "37            price_lag_14    0.000346\n",
      "68       sales_roll_56_std    0.000294\n",
      "61       sales_roll_7_mean    0.000291\n",
      "71            sales_ewm_28    0.000173\n",
      "25              sell_price    0.000159\n",
      "52             sales_lag_2    0.000141\n",
      "72              time_index    0.000134\u001b[0m\n",
      "[ 2025-09-30 00:29:22,636 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ XGBoost training complete!\u001b[0m\n",
      "[ 2025-09-30 00:29:22,637 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müî• Training LightGBM model...\u001b[0m\n",
      "[ 2025-09-30 00:29:22,639 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚öôÔ∏è Starting LightGBM training...\u001b[0m\n",
      "[ 2025-09-30 00:29:22,640 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müì¶ Using static config parameters for LightGBM...\u001b[0m\n",
      "[ 2025-09-30 00:29:22,642 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Loaded config params: {'num_leaves': 31, 'learning_rate': 0.05, 'n_estimators': 100, 'objective': 'regression', 'random_state': 42}\u001b[0m\n",
      "[ 2025-09-30 00:29:22,644 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müõ†Ô∏è Training final LightGBM model...\u001b[0m\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's l2: 0.0580903\n",
      "[100]\tvalid_0's l2: 0.0087255\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's l2: 0.0087255\n",
      "[ 2025-09-30 00:29:26,825 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ LightGBM model trained and stored.\u001b[0m\n",
      "[ 2025-09-30 00:29:26,922 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müìä Calculating model evaluation metrics...\u001b[0m\n",
      "[ 2025-09-30 00:29:26,937 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Metrics calculated: {'rmse': 0.1335, 'mae': 0.0328, 'mape': 2.2987, 'r2': 0.9979}\u001b[0m\n",
      "[ 2025-09-30 00:29:26,948 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müåü Top LightGBM features:\n",
      "                   feature  importance\n",
      "86        sales_roll_3_max         706\n",
      "69             sales_ewm_7         335\n",
      "76  sales_ratio_to_28d_avg         262\n",
      "75   sales_ratio_to_7d_avg         254\n",
      "65      sales_roll_28_mean         244\n",
      "73          sales_velocity         180\n",
      "83       sales_roll_3_mean         162\n",
      "51             sales_lag_1         148\n",
      "74      sales_acceleration         143\n",
      "26                 revenue         116\n",
      "84        sales_roll_3_std         108\n",
      "52             sales_lag_2          74\n",
      "61       sales_roll_7_mean          44\n",
      "87     sales_roll_3_median          38\n",
      "63      sales_roll_14_mean          19\n",
      "66       sales_roll_28_std          17\n",
      "94      sales_roll_21_mean          13\n",
      "97       sales_roll_21_max          12\n",
      "41            price_lag_28          11\n",
      "25              sell_price          11\u001b[0m\n",
      "[ 2025-09-30 00:29:26,951 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ LightGBM training complete!\u001b[0m\n",
      "[ 2025-09-30 00:29:26,955 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müåü Training Prophet model...\u001b[0m\n",
      "[ 2025-09-30 00:29:26,960 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müìÖ Starting Prophet training...\u001b[0m\n",
      "[ 2025-09-30 00:29:28,172 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚ûï Added regressor to Prophet: time_index\u001b[0m\n",
      "[ 2025-09-30 00:29:28,202 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚ûï Added regressor to Prophet: days_since_first_sale\u001b[0m\n",
      "[ 2025-09-30 00:29:28,231 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚ûï Added regressor to Prophet: wm_yr_wk\u001b[0m\n",
      "[ 2025-09-30 00:29:28,261 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚ûï Added regressor to Prophet: consecutive_zero_days\u001b[0m\n",
      "[ 2025-09-30 00:29:28,290 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚ûï Added regressor to Prophet: week_of_year\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:30:06 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-09-30 00:30:06,818 ] cmdstanpy - \u001b[32mINFO\u001b[0m - \u001b[32mChain [1] start processing\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:33:59 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-09-30 00:33:59,557 ] cmdstanpy - \u001b[32mINFO\u001b[0m - \u001b[32mChain [1] done processing\u001b[0m\n",
      "[ 2025-09-30 00:34:02,445 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Prophet model trained successfully.\u001b[0m\n",
      "[ 2025-09-30 00:34:04,072 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müìä Prophet Validation Metrics ‚Äî RMSE: 2.5317 | MAE: 1.1956 | R¬≤: 0.0011\u001b[0m\n",
      "[ 2025-09-30 00:34:06,100 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müìä Calculating model evaluation metrics...\u001b[0m\n",
      "[ 2025-09-30 00:34:06,123 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Metrics calculated: {'rmse': 2.8765, 'mae': 1.3399, 'mape': 38.1474, 'r2': 0.0019}\u001b[0m\n",
      "[ 2025-09-30 00:34:06,127 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Prophet training complete!\u001b[0m\n",
      "[ 2025-09-30 00:34:06,135 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müß© Building stacking ensemble...\u001b[0m\n",
      "[ 2025-09-30 00:34:06,145 ] src.models.ensemble_model - \u001b[32mINFO\u001b[0m - \u001b[32mCustom weights provided. Normalized weights: {'xgboost': 0.3333333333333333, 'lightgbm': 0.3333333333333333, 'prophet': 0.3333333333333333}\u001b[0m\n",
      "[ 2025-09-30 00:34:06,148 ] src.models.ensemble_model - \u001b[32mINFO\u001b[0m - \u001b[32mInitialized EnsembleModel with models: ['xgboost', 'lightgbm', 'prophet']\u001b[0m\n",
      "[ 2025-09-30 00:34:06,150 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müìä Calculating model evaluation metrics...\u001b[0m\n",
      "[ 2025-09-30 00:34:06,159 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Metrics calculated: {'rmse': 0.9676, 'mae': 0.4501, 'mape': 12.911, 'r2': 0.8871}\u001b[0m\n",
      "[ 2025-09-30 00:34:06,160 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müèÜ Ensemble training complete!\u001b[0m\n",
      "[ 2025-09-30 00:34:06,164 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müéâ Successfully logged models and metrics: ['xgboost', 'lightgbm', 'prophet', 'ensemble']\u001b[0m\n",
      "[ 2025-09-30 00:34:06,168 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müîç Running diagnostics & visualizations...\u001b[0m\n",
      "[ 2025-09-30 00:34:06,185 ] src.models.digonistics - \u001b[32mINFO\u001b[0m - \u001b[32mChecking data quality...\u001b[0m\n",
      "[ 2025-09-30 00:34:06,244 ] src.models.digonistics - \u001b[32mINFO\u001b[0m - \u001b[32mChecking distribution shift...\u001b[0m\n",
      "[ 2025-09-30 00:34:06,264 ] src.models.digonistics - \u001b[32mINFO\u001b[0m - \u001b[32mAnalyzing predictions...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\anaconda3\\envs\\salesenv\\lib\\site-packages\\numpy\\lib\\function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\Administrator\\anaconda3\\envs\\salesenv\\lib\\site-packages\\numpy\\lib\\function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-09-30 00:34:07,708 ] src.models.digonistics - \u001b[32mINFO\u001b[0m - \u001b[32mDiagnosis saved to diagnostics\\diagnosis.json and diagnostics\\diagnosis_report.md\u001b[0m\n",
      "[ 2025-09-30 00:34:07,712 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müìã Diagnostic recommendations:\u001b[0m\n",
      "[ 2025-09-30 00:34:07,712 ] __main__ - \u001b[33mWARNING\u001b[0m - \u001b[33m‚ö†Ô∏è - Test set shows distribution shift (mean shift: 34.6%)\u001b[0m\n",
      "[ 2025-09-30 00:34:07,712 ] __main__ - \u001b[33mWARNING\u001b[0m - \u001b[33m‚ö†Ô∏è - High feature count (109). Consider feature selection.\u001b[0m\n",
      "[ 2025-09-30 00:34:07,722 ] __main__ - \u001b[33mWARNING\u001b[0m - \u001b[33m‚ö†Ô∏è - Many zero sales (160711 in training). Consider log transform or zero-inflated models.\u001b[0m\n",
      "[ 2025-09-30 00:34:07,722 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müìä Generating model comparison visualizations...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\anaconda3\\envs\\salesenv\\lib\\site-packages\\numpy\\lib\\function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\Administrator\\anaconda3\\envs\\salesenv\\lib\\site-packages\\numpy\\lib\\function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-09-30 00:34:07,928 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müé® Starting visualization generation...\u001b[0m\n",
      "[ 2025-09-30 00:34:08,077 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müìÅ Creating visualizations in temporary directory: C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\tmprotzv4dr\u001b[0m\n",
      "[ 2025-09-30 00:34:11,048 ] src.visualizations.model_visualizations - \u001b[32mINFO\u001b[0m - \u001b[32mSaved metrics comparison chart to C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\tmprotzv4dr\\metrics_comparison.png\u001b[0m\n",
      "[ 2025-09-30 00:34:12,772 ] src.visualizations.model_visualizations - \u001b[32mINFO\u001b[0m - \u001b[32mSaved predictions comparison chart to C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\tmprotzv4dr\\predictions_comparison.png\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\OneDrive\\Desktop\\SalesAI\\backend\\src\\visualizations\\model_visualizations.py:250: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  plt.tight_layout()\n",
      "c:\\Users\\Administrator\\OneDrive\\Desktop\\SalesAI\\backend\\src\\visualizations\\model_visualizations.py:253: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  plt.savefig(save_path, dpi=300, bbox_inches='tight')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-09-30 00:44:41,669 ] src.visualizations.model_visualizations - \u001b[32mINFO\u001b[0m - \u001b[32mSaved residuals analysis chart to C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\tmprotzv4dr\\residuals_analysis.png\u001b[0m\n",
      "[ 2025-09-30 00:44:47,800 ] src.visualizations.model_visualizations - \u001b[32mINFO\u001b[0m - \u001b[32mSaved error distribution chart to C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\tmprotzv4dr\\error_distribution.png\u001b[0m\n",
      "[ 2025-09-30 00:44:49,280 ] src.visualizations.model_visualizations - \u001b[32mINFO\u001b[0m - \u001b[32mSaved feature importance chart to C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\tmprotzv4dr\\feature_importance.png\u001b[0m\n",
      "[ 2025-09-30 00:44:51,118 ] src.visualizations.model_visualizations - \u001b[32mINFO\u001b[0m - \u001b[32mGenerated 6 visualization files in C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\tmprotzv4dr\u001b[0m\n",
      "[ 2025-09-30 00:44:51,123 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Generated 6 visualization files\u001b[0m\n",
      "[ 2025-09-30 00:44:52,367 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müì§ Logged visualization: metrics_comparison\u001b[0m\n",
      "[ 2025-09-30 00:44:52,392 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müì§ Logged visualization: predictions_comparison\u001b[0m\n",
      "[ 2025-09-30 00:44:52,432 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müì§ Logged visualization: residuals_analysis\u001b[0m\n",
      "[ 2025-09-30 00:44:52,465 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müì§ Logged visualization: error_distribution\u001b[0m\n",
      "[ 2025-09-30 00:44:52,500 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müì§ Logged visualization: feature_importance\u001b[0m\n",
      "[ 2025-09-30 00:44:52,537 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müì§ Logged visualization: summary\u001b[0m\n",
      "[ 2025-09-30 00:44:52,579 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müìÑ Logged metrics summary JSON\u001b[0m\n",
      "[ 2025-09-30 00:44:52,585 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müìù Creating combined HTML report for visualizations...\u001b[0m\n",
      "[ 2025-09-30 00:44:52,590 ] __main__ - \u001b[31mERROR\u001b[0m - \u001b[31müí• Failed to create combined HTML report: '\\n                        font-family'\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_19388\\1762574637.py\", line 939, in _create_combined_html_report\n",
      "    html_content = html_content.format(timestamp=datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
      "KeyError: '\\n                        font-family'\n",
      "[ 2025-09-30 00:44:52,638 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müíæ Saving artifacts...\u001b[0m\n",
      "[ 2025-09-30 00:44:52,648 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müìÅ Created artifact directory: /tmp/artifacts/20250930_004452\u001b[0m\n",
      "[ 2025-09-30 00:44:52,740 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müíæ Saved scalers, encoders, and feature columns.\u001b[0m\n",
      "[ 2025-09-30 00:44:52,820 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müõ†Ô∏è Saved model: xgboost -> /tmp/artifacts/20250930_004452\\models/xgboost\u001b[0m\n",
      "[ 2025-09-30 00:44:52,865 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müõ†Ô∏è Saved model: lightgbm -> /tmp/artifacts/20250930_004452\\models/lightgbm\u001b[0m\n",
      "[ 2025-09-30 00:44:54,593 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müõ†Ô∏è Saved model: ensemble -> /tmp/artifacts/20250930_004452\\models/ensemble\u001b[0m\n",
      "[ 2025-09-30 00:44:54,599 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müìú Saved metadata.\u001b[0m\n",
      "[ 2025-09-30 00:44:54,601 ] __main__ - \u001b[33mWARNING\u001b[0m - \u001b[33m‚ö†Ô∏è mlflow_manager not found or None. Skipping MLflow logging.\u001b[0m\n",
      "[ 2025-09-30 00:44:54,605 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32m‚úÖ Artifacts saved successfully in /tmp/artifacts/20250930_004452\u001b[0m\n",
      "[ 2025-09-30 00:44:54,606 ] __main__ - \u001b[32mINFO\u001b[0m - \u001b[32müèÅ MLflow run ended.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "results = trainer.train_all_models(train_df,value_df,test_df,target_col='sales',use_optuna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2175b70f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "salesenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
