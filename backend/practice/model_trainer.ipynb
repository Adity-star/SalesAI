{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e0015a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2025-09-29 22:29:39,332 ] src.utils.config_loader - \u001b[32mINFO\u001b[0m - \u001b[32mLoaded configuration from: C:\\Users\\Administrator\\OneDrive\\Desktop\\SalesAI\\backend\\configs\\ml_config.yaml\u001b[0m\n",
      "[ 2025-09-29 22:29:39,350 ] src.features.feature_pipeline - \u001b[32mINFO\u001b[0m - \u001b[32m✅ Config loaded successfully using config_loader\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import joblib\n",
    "from src.logger import logger\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from prophet import Prophet\n",
    "import optuna \n",
    "import mlflow\n",
    "import time\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from src.utils.mlflow_utils import MLflowManager\n",
    "from src.features.feature_pipeline import FeaturePipeline\n",
    "from src.data_pipelines.validators import DataValidator\n",
    "from src.models.advanced_ensemble import AdvancedEnsemble\n",
    "from src.models.digonistics import diagnose_model_performance\n",
    "from src.models.ensemble_model import EnsembleModel\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, MinMaxScaler, RobustScaler\n",
    "from src.utils.config_loader import ConfigLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b36fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import logging\n",
    "from src.logger import logger\n",
    "from typing import Dict, Any, List, Optional, Union\n",
    "from pathlib import Path\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, config_path: Optional[Union[str, Path]] = None):\n",
    "        self.config_loader = ConfigLoader()\n",
    "\n",
    "        self.config = self.config_loader.load_yaml(file_path=\"ml_config.yaml\")\n",
    "\n",
    "        self.model_config: Dict[str, Any] = self.config.get('models', {})\n",
    "        self.model_config: Dict[str, Any] = self.config.get('models', {})\n",
    "\n",
    "        self.mlflow_manager = MLflowManager(config_path)\n",
    "        self.feature_engineer = None\n",
    "\n",
    "        self.data_validator = DataValidator(config_path)\n",
    "\n",
    "        self.models: Dict[str, Any] = {}\n",
    "        self.scalers: Dict[str, Any] = {}\n",
    "        self.encoders: Dict[str, Any] = {}\n",
    "        self.feature_cols: List[str] = []\n",
    "\n",
    "    def prepare_data(\n",
    "        self, df: pd.DataFrame, target_col: str = \"sales\",\n",
    "        date_col: str = \"date\", group_cols: Optional[List[str]] = None,\n",
    "        categorical_cols: Optional[List[str]] = None\n",
    "    ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "\n",
    "        logger.info(\"Preparing data for training\")\n",
    "\n",
    "        required_cols = [date_col, target_col]\n",
    "        if group_cols:\n",
    "            required_cols.extend(group_cols)\n",
    "\n",
    "        missing_cols = set(required_cols) - set(df.columns)\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing required columns for training: {missing_cols}\")\n",
    "\n",
    "        \n",
    "        pipeline = FeaturePipeline(df, target_col=target_col, group_cols=group_cols)\n",
    "        df_features = pipeline.run()\n",
    "\n",
    "        if categorical_cols:\n",
    "            df_features = pipeline.create_target_encoding(df_features, target_col, categorical_cols)\n",
    "            logger.info(\"Applied target encoding to categorical columns.\")\n",
    "            \n",
    "         # Chronological split\n",
    "        df_sorted = df_features.sort_values(date_col)\n",
    "        train_size = int(len(df_sorted) * (1 - self.training_config[\"test_size\"] - self.training_config[\"validation_size\"]))\n",
    "        val_size = int(len(df_sorted) * self.training_config[\"validation_size\"])\n",
    "\n",
    "        train_df = df_sorted[:train_size]\n",
    "        val_df = df_sorted[train_size:train_size + val_size]\n",
    "        test_df = df_sorted[train_size + val_size:]\n",
    "\n",
    "        # Drop rows with missing target\n",
    "        train_df = train_df.dropna(subset=[target_col])\n",
    "        val_df = val_df.dropna(subset=[target_col])\n",
    "        test_df = test_df.dropna(subset=[target_col])\n",
    "\n",
    "        logger.info(f\"Data split → Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "        return train_df, val_df, test_df\n",
    "    \n",
    "\n",
    "    def preprocess_features(\n",
    "        self, train_df: pd.DataFrame, val_df: pd.DataFrame, test_df: pd.DataFrame,\n",
    "        target_col: str, exclude_cols: List[str] = [\"date\"]\n",
    "    ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, np.ndarray, np.ndarray, np.ndarray]:\n",
    "\n",
    "        feature_cols = [col for col in train_df.columns if col not in exclude_cols + [target_col]]\n",
    "        self.feature_cols = feature_cols\n",
    "\n",
    "        X_train, X_val, X_test = train_df[feature_cols].copy(), val_df[feature_cols].copy(), test_df[feature_cols].copy()\n",
    "        y_train, y_val, y_test = train_df[target_col].values, val_df[target_col].values, test_df[target_col].values\n",
    "\n",
    "        # Handle categorical encoding\n",
    "        categorical_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "        for col in categorical_cols:\n",
    "            if self.training_config.get(\"encoder\", \"label\") == \"label\":\n",
    "                if col not in self.encoders:\n",
    "                    self.encoders[col] = LabelEncoder()\n",
    "                    X_train.loc[:, col] = self.encoders[col].fit_transform(X_train[col].astype(str))\n",
    "                else:\n",
    "                    X_train.loc[:, col] = self.encoders[col].transform(X_train[col].astype(str))\n",
    "                X_val.loc[:, col] = self.encoders[col].transform(X_val[col].astype(str))\n",
    "                X_test.loc[:, col] = self.encoders[col].transform(X_test[col].astype(str))\n",
    "            elif self.training_config[\"encoder\"] == \"onehot\":\n",
    "                ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "                encoded = ohe.fit_transform(X_train[[col]])\n",
    "                encoded_cols = [f\"{col}_{cat}\" for cat in ohe.categories_[0]]\n",
    "                X_train = X_train.drop(col, axis=1).join(pd.DataFrame(encoded, columns=encoded_cols, index=X_train.index))\n",
    "                X_val = X_val.drop(col, axis=1).join(pd.DataFrame(ohe.transform(X_val[[col]]), columns=encoded_cols, index=X_val.index))\n",
    "                X_test = X_test.drop(col, axis=1).join(pd.DataFrame(ohe.transform(X_test[[col]]), columns=encoded_cols, index=X_test.index))\n",
    "                self.encoders[col] = ohe\n",
    "\n",
    "        # Scale numeric features\n",
    "        scaler_type = self.training_config.get(\"scaler\", \"standard\")\n",
    "        if scaler_type == \"standard\":\n",
    "            scaler = StandardScaler()\n",
    "        elif scaler_type == \"minmax\":\n",
    "            scaler = MinMaxScaler()\n",
    "        elif scaler_type == \"robust\":\n",
    "            scaler = RobustScaler()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported scaler type: {scaler_type}\")\n",
    "\n",
    "        X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "        X_val_scaled = pd.DataFrame(scaler.transform(X_val), columns=X_val.columns, index=X_val.index)\n",
    "        X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "        self.scalers[\"scaler\"] = scaler\n",
    "        logger.info(f\"Preprocessing complete. Features: {len(self.feature_cols)}\")\n",
    "\n",
    "        return X_train_scaled, X_val_scaled, X_test_scaled, y_train, y_val, y_test\n",
    "    \n",
    "\n",
    "    def calculate_metrics(self, y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n",
    "        metrics = {\n",
    "            'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "            'mae': mean_absolute_error(y_true, y_pred),\n",
    "            'mape': np.mean(np.abs((y_true - y_pred) / y_true)) * 100,\n",
    "            'r2': r2_score(y_true, y_pred)\n",
    "        }\n",
    "        return metrics\n",
    "    \n",
    "\n",
    "    def train_xgboost(self, \n",
    "                  X_train: np.ndarray, y_train: np.ndarray,\n",
    "                  X_val: np.ndarray, y_val: np.ndarray,\n",
    "                  use_optuna: bool = True) -> xgb.XGBRegressor:\n",
    "        \"\"\"\n",
    "        Train an XGBoost regressor with optional Optuna optimization.\n",
    "        \n",
    "        Args:\n",
    "            X_train, y_train: Training data\n",
    "            X_val, y_val: Validation data\n",
    "            use_optuna: Whether to run Optuna hyperparameter search\n",
    "        \n",
    "        Returns:\n",
    "            Trained XGBRegressor model\n",
    "        \"\"\"\n",
    "        logger.info(\"Training XGBoost model\")\n",
    "\n",
    "         # Input validation\n",
    "        if len(X_train) == 0 or len(X_val) == 0:\n",
    "           raise ValueError(\"Training or validation data is empty\")\n",
    "\n",
    "            # Detect GPU\n",
    "        try:\n",
    "            import xgboost as xgb\n",
    "            if xgb.get_config().get('use_gpu', False):\n",
    "                tree_method = \"gpu_hist\"\n",
    "            else:\n",
    "                tree_method = \"hist\"\n",
    "        except Exception:\n",
    "            tree_method = \"hist\"\n",
    "\n",
    "        best_params = None\n",
    "\n",
    "        if use_optuna:\n",
    "            logger.info(\"Starting Optuna hyperparameter search for XGBoost\")\n",
    "\n",
    "            def objective(trial):\n",
    "                params = {\n",
    "                    'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "                    'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "                    'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.3, log=True),\n",
    "                    'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "                    'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "                    'gamma': trial.suggest_float('gamma', 0, 1.0),\n",
    "                    'reg_alpha': trial.suggest_float('reg_alpha', 0, 5.0),\n",
    "                    'reg_lambda': trial.suggest_float('reg_lambda', 0, 5.0),\n",
    "                    'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "                    'random_state': 42,\n",
    "                    'tree_method': 'hist'  # faster training, GPU if available\n",
    "                }\n",
    "\n",
    "\n",
    "                model = xgb.XGBRegressor(**params, early_stopping_rounds=50)\n",
    "                model.fit(\n",
    "                    X_train, y_train,\n",
    "                    eval_set=[(X_val, y_val)],\n",
    "                    verbose=False,\n",
    "                )\n",
    "                y_pred = model.predict(X_val)\n",
    "                rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "                return rmse\n",
    "\n",
    "            study = optuna.create_study(\n",
    "                direction=\"minimize\",\n",
    "                sampler=optuna.samplers.TPESampler(seed=42),\n",
    "                pruner=optuna.pruners.MedianPruner()\n",
    "            )\n",
    "            study.optimize(objective, n_trials=self.training_config.get('optuna_trials', 50))\n",
    "\n",
    "            best_params = study.best_params\n",
    "            logger.info(f\"Optuna best params: {best_params}\")\n",
    "        else:\n",
    "            best_params = self.model_config['xgboost']['params']\n",
    "            logger.info(\"Using config params for XGBoost\")\n",
    "\n",
    "        # Final model training with best params\n",
    "        best_params.update({\n",
    "            \"random_state\": 42,\n",
    "            \"tree_method\": \"hist\" \n",
    "        })\n",
    "\n",
    "        model = xgb.XGBRegressor(**best_params, early_stopping_rounds=50)\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "        # Save trained model\n",
    "        self.models['xgboost'] = model\n",
    "        logger.info(f\"Best iteration: {model.best_iteration}\")\n",
    "        logger.info(f\"XXXXXXXXXXXXXXXXXXXXXXXXXXx trained XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXx\")\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def train_lightgbm(self, \n",
    "                   X_train: np.ndarray, y_train: np.ndarray,\n",
    "                   X_val: np.ndarray, y_val: np.ndarray,\n",
    "                   use_optuna: bool = True) -> lgb.LGBMRegressor:\n",
    "        \"\"\"\n",
    "        Train a LightGBM regressor with optional Optuna optimization.\n",
    "        \n",
    "        Args:\n",
    "            X_train, y_train: Training data\n",
    "            X_val, y_val: Validation data\n",
    "            use_optuna: Whether to run Optuna hyperparameter search\n",
    "        \n",
    "        Returns:\n",
    "            Trained LGBMRegressor model\n",
    "        \"\"\"\n",
    "        logger.info(\"Training LightGBM model\")\n",
    "\n",
    "        best_params = None\n",
    "\n",
    "        if use_optuna:\n",
    "            logger.info(\"Starting Optuna hyperparameter search for LightGBM\")\n",
    "\n",
    "            def objective(trial):\n",
    "                params = {\n",
    "                    'num_leaves': trial.suggest_int('num_leaves', 31, 256),\n",
    "                    'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.3, log=True),\n",
    "                    'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "                    'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "                    'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "                    'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "                    'reg_alpha': trial.suggest_float('reg_alpha', 0, 5.0),\n",
    "                    'reg_lambda': trial.suggest_float('reg_lambda', 0, 5.0),\n",
    "                    'min_split_gain': trial.suggest_float('min_split_gain', 0, 1.0),\n",
    "                    'random_state': 42,\n",
    "                    'verbosity': -1,\n",
    "                    'objective': 'regression',\n",
    "                    'metric': 'rmse',\n",
    "                    'boosting_type': 'gbdt'\n",
    "                }\n",
    "\n",
    "                model = lgb.LGBMRegressor(**params)\n",
    "                model.fit(\n",
    "                    X_train, y_train,\n",
    "                    eval_set=[(X_val, y_val)],\n",
    "                    callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]\n",
    "                )\n",
    "\n",
    "                y_pred = model.predict(X_val)\n",
    "                rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "                return rmse\n",
    "\n",
    "            study = optuna.create_study(\n",
    "                direction=\"minimize\",\n",
    "                sampler=optuna.samplers.TPESampler(seed=42),\n",
    "                pruner=optuna.pruners.MedianPruner()\n",
    "            )\n",
    "            study.optimize(objective, n_trials=self.training_config.get('optuna_trials', 50))\n",
    "\n",
    "            best_params = study.best_params\n",
    "            logger.info(f\"Optuna best params: {best_params}\")\n",
    "\n",
    "            best_params.update({\n",
    "                'random_state': 42,\n",
    "                'verbosity': -1,\n",
    "                'objective': 'regression',\n",
    "                'metric': 'rmse',\n",
    "                'boosting_type': 'gbdt'\n",
    "            })\n",
    "        else:\n",
    "            best_params = self.model_config['lightgbm']['params']\n",
    "            logger.info(\"Using config params for LightGBM\")\n",
    "\n",
    "        # Final model training\n",
    "        model = lgb.LGBMRegressor(**best_params)\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            callbacks=[lgb.early_stopping(50), lgb.log_evaluation(50)]\n",
    "        )\n",
    "\n",
    "        self.models['lightgbm'] = model\n",
    "\n",
    "        return model\n",
    "    \n",
    "\n",
    "    def train_prophet(self, \n",
    "                  train_df: pd.DataFrame, \n",
    "                  val_df: pd.DataFrame,\n",
    "                  date_col: str = 'date', \n",
    "                  target_col: str = 'sales') -> Prophet:\n",
    "        \"\"\"\n",
    "        Train a Prophet model with optional regressors and log results to MLflow.\n",
    "        \n",
    "        Args:\n",
    "            train_df: Training DataFrame\n",
    "            val_df: Validation DataFrame\n",
    "            date_col: Column name for dates\n",
    "            target_col: Column name for target variable\n",
    "        \n",
    "        Returns:\n",
    "            Trained Prophet model\n",
    "        \"\"\"\n",
    "        logger.info(\"Training Prophet model\")\n",
    "\n",
    "        # Prepare training data\n",
    "        prophet_train = train_df[[date_col, target_col]].rename(\n",
    "            columns={date_col: 'ds', target_col: 'y'}\n",
    "        ).dropna().sort_values('ds')\n",
    "\n",
    "        # Load params\n",
    "        prophet_params = self.model_config['prophet']['params'].copy()\n",
    "\n",
    "        # Override defaults for stability\n",
    "        prophet_params.update({\n",
    "            'stan_backend': 'CMDSTANPY',\n",
    "            'mcmc_samples': 0,         # faster, no Bayesian sampling\n",
    "            'uncertainty_samples': 100 # smaller uncertainty for speed\n",
    "        })\n",
    "\n",
    "        try:\n",
    "            model = Prophet(**prophet_params)\n",
    "\n",
    "            # --- Regressor selection ---\n",
    "            numeric_cols = train_df.select_dtypes(include=[np.number]).columns\n",
    "            regressor_cols = [c for c in numeric_cols if c not in [target_col, 'year', 'month', 'day', 'week', 'quarter']]\n",
    "\n",
    "            if len(regressor_cols) > 5:\n",
    "                variances = {c: train_df[c].var() for c in regressor_cols}\n",
    "                regressor_cols = sorted(variances, key=variances.get, reverse=True)[:5]\n",
    "\n",
    "            for col in regressor_cols:\n",
    "                if train_df[col].std() > 0:\n",
    "                    model.add_regressor(col)\n",
    "                    prophet_train[col] = train_df[col]\n",
    "\n",
    "            # --- Fit model ---\n",
    "            model.fit(prophet_train)\n",
    "            self.models['prophet'] = model\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Prophet training failed: {e}\")\n",
    "            logger.info(\"Retrying Prophet with minimal config...\")\n",
    "\n",
    "            model = Prophet(\n",
    "                yearly_seasonality=True,\n",
    "                weekly_seasonality=True,\n",
    "                daily_seasonality=False,\n",
    "                changepoint_prior_scale=0.05,\n",
    "                seasonality_prior_scale=10.0,\n",
    "                uncertainty_samples=50,\n",
    "                mcmc_samples=0\n",
    "            )\n",
    "            model.fit(prophet_train[['ds', 'y']])\n",
    "            self.models['prophet'] = model\n",
    "\n",
    "        # --- Validation evaluation ---\n",
    "        prophet_val = val_df[[date_col, target_col]].rename(\n",
    "            columns={date_col: 'ds', target_col: 'y'}\n",
    "        ).dropna().sort_values('ds')\n",
    "\n",
    "        # Add regressors if available\n",
    "        for col in regressor_cols if 'regressor_cols' in locals() else []:\n",
    "            if col in val_df:\n",
    "                prophet_val[col] = val_df[col]\n",
    "\n",
    "        forecast = model.predict(prophet_val)\n",
    "        y_true = prophet_val['y'].values\n",
    "        y_pred = forecast['yhat'].values\n",
    "\n",
    "        val_rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        val_mae = mean_absolute_error(y_true, y_pred)\n",
    "        val_r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "        logger.info(f\"Prophet Val RMSE: {val_rmse:.4f}, MAE: {val_mae:.4f}, R2: {val_r2:.4f}\")\n",
    "        return model\n",
    "    \n",
    "    def train_all_models(self, train_df: pd.DataFrame, val_df: pd.DataFrame,\n",
    "                        test_df: pd.DataFrame, target_col: str = 'sales',\n",
    "                        use_optuna: bool = True) -> Dict[str, Dict[str, Any]]:\n",
    "        \n",
    "        \"\"\"\n",
    "        Train all models (XGBoost, LightGBM, Prophet) and build ensemble.\n",
    "        Improvements:\n",
    "            - Parallel training for speed\n",
    "            - Config-driven model selection\n",
    "            - Stacking ensemble (meta-learner)\n",
    "            - Robust error handling\n",
    "            - Runtime performance logging\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # Start MLflow run\n",
    "        run_id = self.mlflow_manager.start_run(\n",
    "            run_name=f\"sales_forecast_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "            tags={\"model_type\": \"ensemble\", \"use_optuna\": str(use_optuna)}\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # ------------------------\n",
    "            # Preprocess Data\n",
    "            # ------------------------\n",
    "            X_train, X_val, X_test, y_train, y_val, y_test = self.preprocess_features(\n",
    "                train_df, val_df, test_df, target_col\n",
    "            )\n",
    "\n",
    "            self.mlflow_manager.log_params({\n",
    "                \"train_size\": len(train_df),\n",
    "                \"val_size\": len(val_df),\n",
    "                \"test_size\": len(test_df),\n",
    "                \"n_features\": X_train.shape[1]\n",
    "            })\n",
    "\n",
    "            # ------------\n",
    "            # Train Models\n",
    "            # ------------\n",
    "            \n",
    "            # Train XGBoost\n",
    "            xgb_model = self.train_xgboost(X_train, y_train, X_val, y_val, use_optuna)\n",
    "            xgb_pred = xgb_model.predict(X_test)\n",
    "            xgb_metrics = self.calculate_metrics(y_test, xgb_pred)\n",
    "            \n",
    "            self.mlflow_manager.log_metrics({f\"xgboost_{k}\": v for k, v in xgb_metrics.items()})\n",
    "            self.mlflow_manager.log_model(xgb_model, \"xgboost\", \n",
    "                                         input_example=X_train.iloc[:5])\n",
    "            \n",
    "            # Log feature importance\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'feature': self.feature_cols,\n",
    "                'importance': xgb_model.feature_importances_\n",
    "            }).sort_values('importance', ascending=False).head(20)\n",
    "            \n",
    "            logger.info(f\"Top XGBoost features:\\n{feature_importance.to_string()}\")\n",
    "            self.mlflow_manager.log_params({f\"xgb_top_feature_{i}\": f\"{row['feature']} ({row['importance']:.4f})\" \n",
    "                                           for i, (_, row) in enumerate(feature_importance.iterrows())})\n",
    "            \n",
    "            results['xgboost'] = {\n",
    "                'model': xgb_model,\n",
    "                'metrics': xgb_metrics,\n",
    "                'predictions': xgb_pred,\n",
    "                'actual': y_test\n",
    "            }\n",
    "            \n",
    "            # Train LightGBM\n",
    "            lgb_model = self.train_lightgbm(X_train, y_train, X_val, y_val, use_optuna)\n",
    "            lgb_pred = lgb_model.predict(X_test)\n",
    "            lgb_metrics = self.calculate_metrics(y_test, lgb_pred)\n",
    "            \n",
    "            self.mlflow_manager.log_metrics({f\"lightgbm_{k}\": v for k, v in lgb_metrics.items()})\n",
    "            self.mlflow_manager.log_model(lgb_model, \"lightgbm\",\n",
    "                                         input_example=X_train.iloc[:5])\n",
    "            \n",
    "            # Log feature importance for LightGBM\n",
    "            lgb_importance = pd.DataFrame({\n",
    "                'feature': self.feature_cols,\n",
    "                'importance': lgb_model.feature_importances_\n",
    "            }).sort_values('importance', ascending=False).head(20)\n",
    "            \n",
    "            logger.info(f\"Top LightGBM features:\\n{lgb_importance.to_string()}\")\n",
    "            \n",
    "            results['lightgbm'] = {\n",
    "                'model': lgb_model,\n",
    "                'metrics': lgb_metrics,\n",
    "                'predictions': lgb_pred,\n",
    "                'actual': y_test\n",
    "            }\n",
    "\n",
    "            # ------------------------\n",
    "            # Train Prophet (Sequential - slower but unavoidable)\n",
    "            # ------------------------\n",
    "            \n",
    "            # Train Prophet if enabled\n",
    "            prophet_enabled = self.model_config.get('prophet', {}).get('enabled', True)\n",
    "            \n",
    "            if prophet_enabled:\n",
    "                try:\n",
    "                    prophet_model = self.train_prophet(train_df, val_df)\n",
    "                    \n",
    "                    # Create future dataframe for Prophet predictions\n",
    "                    future = test_df[['date']].rename(columns={'date': 'ds'})\n",
    "                    \n",
    "                    # Add regressors if they exist\n",
    "                    if hasattr(prophet_model, 'extra_regressors') and prophet_model.extra_regressors:\n",
    "                        regressor_cols = [col for col in prophet_model.extra_regressors.keys()]\n",
    "                        for col in regressor_cols:\n",
    "                            if col in test_df.columns:\n",
    "                                future[col] = test_df[col]\n",
    "                    \n",
    "                    prophet_pred = prophet_model.predict(future)['yhat'].values\n",
    "                    prophet_metrics = self.calculate_metrics(y_test, prophet_pred)\n",
    "                    \n",
    "                    self.mlflow_manager.log_metrics({f\"prophet_{k}\": v for k, v in prophet_metrics.items()})\n",
    "                    \n",
    "                    results['prophet'] = {\n",
    "                        'model': prophet_model,\n",
    "                        'metrics': prophet_metrics,\n",
    "                        'predictions': prophet_pred,\n",
    "                        'actual':y_test\n",
    "                    }\n",
    "                    \n",
    "                    # Ensemble predictions with all three models\n",
    "                    ensemble_pred = (xgb_pred + lgb_pred + prophet_pred) / 3\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Prophet training failed: {e}. Using weighted ensemble of XGBoost and LightGBM.\")\n",
    "                    prophet_enabled = False\n",
    "            \n",
    "             # ------------------------\n",
    "            # Build Stacking Ensemble (Meta-Model)\n",
    "            # ------------------------\n",
    "            logger.info(\"Building stacking ensemble...\")\n",
    "\n",
    "            if not prophet_enabled:\n",
    "                # Weighted ensemble based on individual model performance (using validation R2)\n",
    "                xgb_val_pred = xgb_model.predict(X_val)\n",
    "                lgb_val_pred = lgb_model.predict(X_val)\n",
    "                \n",
    "                xgb_val_r2 = r2_score(y_val, xgb_val_pred)\n",
    "                lgb_val_r2 = r2_score(y_val, lgb_val_pred)\n",
    "                \n",
    "                # Calculate weights based on R2 scores with minimum weight constraint\n",
    "                # This prevents a poorly performing model from being completely ignored\n",
    "                min_weight = 0.2\n",
    "                xgb_weight = max(min_weight, xgb_val_r2 / (xgb_val_r2 + lgb_val_r2))\n",
    "                lgb_weight = max(min_weight, lgb_val_r2 / (xgb_val_r2 + lgb_val_r2))\n",
    "                \n",
    "                # Normalize weights\n",
    "                total_weight = xgb_weight + lgb_weight\n",
    "                xgb_weight = xgb_weight / total_weight\n",
    "                lgb_weight = lgb_weight / total_weight\n",
    "                \n",
    "                logger.info(f\"Ensemble weights - XGBoost: {xgb_weight:.3f}, LightGBM: {lgb_weight:.3f}\")\n",
    "                \n",
    "                # Create ensemble model\n",
    "                ensemble_weights = {\n",
    "                    'xgboost': xgb_weight,\n",
    "                    'lightgbm': lgb_weight\n",
    "                }\n",
    "                \n",
    "                # Use simple weighted ensemble based on validation performance\n",
    "                ensemble_pred = xgb_weight * xgb_pred + lgb_weight * lgb_pred\n",
    "            \n",
    "            # Create the ensemble model object\n",
    "            ensemble_models = {\n",
    "                'xgboost': xgb_model,\n",
    "                'lightgbm': lgb_model\n",
    "            }\n",
    "            \n",
    "            if 'prophet' in results:\n",
    "                ensemble_models['prophet'] = results['prophet']['model']\n",
    "                ensemble_weights = {\n",
    "                    'xgboost': 1/3,\n",
    "                    'lightgbm': 1/3,\n",
    "                    'prophet': 1/3\n",
    "                }\n",
    "            \n",
    "            ensemble_model = EnsembleModel(ensemble_models, ensemble_weights)\n",
    "            \n",
    "            # Save ensemble model\n",
    "            self.models['ensemble'] = ensemble_model\n",
    "            \n",
    "            ensemble_metrics = self.calculate_metrics(y_test, ensemble_pred)\n",
    "            \n",
    "            self.mlflow_manager.log_metrics({f\"ensemble_{k}\": v for k, v in ensemble_metrics.items()})\n",
    "            self.mlflow_manager.log_model(ensemble_model, \"ensemble\", \n",
    "                                         input_example=X_train.iloc[:5])\n",
    "            \n",
    "            results['ensemble'] = {\n",
    "                'model': ensemble_model,\n",
    "                'metrics': ensemble_metrics,\n",
    "                'predictions': ensemble_pred,\n",
    "                'actual':y_test\n",
    "            }\n",
    "\n",
    "            logger.info(f\"Sucessfully logged models with : {results}\")\n",
    "\n",
    "\n",
    "            logger.info(\"Running diagnostics & visualizations...\")\n",
    "        \n",
    "            # Run diagnostics\n",
    "            logger.info(\"Running model diagnostics...\")\n",
    "            test_predictions = {\n",
    "                'xgboost': xgb_pred if 'xgboost' in results else None,\n",
    "                'lightgbm': lgb_pred if 'lightgbm' in results else None,\n",
    "                'ensemble': ensemble_pred\n",
    "            }\n",
    "            \n",
    "            diagnosis = diagnose_model_performance(\n",
    "                train_df, val_df, test_df, test_predictions, target_col\n",
    "            )\n",
    "            \n",
    "            logger.info(\"Diagnostic recommendations:\")\n",
    "            for rec in diagnosis['recommendations']:\n",
    "                logger.warning(f\"- {rec}\")\n",
    "            \n",
    "            # Generate visualizations\n",
    "            logger.info(\"Generating model comparison visualizations...\")\n",
    "            try:\n",
    "                self._generate_and_log_visualizations(results, test_df, target_col)\n",
    "            except Exception as viz_error:\n",
    "                logger.error(f\"Visualization generation failed: {viz_error}\", exc_info=True)\n",
    "            \n",
    "            # Save artifacts\n",
    "            self.save_artifacts()\n",
    "            \n",
    "            # Get current run ID for verification\n",
    "            current_run_id = mlflow.active_run().info.run_id\n",
    "            \n",
    "            self.mlflow_manager.end_run()\n",
    "            \n",
    "            # Sync artifacts to S3\n",
    "            from src.utils.mlflow_s3_utils import MLflowS3Manager\n",
    "            \n",
    "            logger.info(\"Syncing artifacts to S3...\")\n",
    "            try:\n",
    "                s3_manager = MLflowS3Manager()\n",
    "                s3_manager.sync_mlflow_artifacts_to_s3(current_run_id)\n",
    "                logger.info(\"✓ Successfully synced artifacts to S3\")\n",
    "                \n",
    "                # Verify S3 artifacts after sync\n",
    "                from utils.s3_verification import verify_s3_artifacts, log_s3_verification_results\n",
    "                \n",
    "                logger.info(\"Verifying S3 artifact storage...\")\n",
    "                verification_results = verify_s3_artifacts(\n",
    "                    run_id=current_run_id,\n",
    "                    expected_artifacts=[\n",
    "                        'models/', \n",
    "                        'scalers.pkl', \n",
    "                        'encoders.pkl', \n",
    "                        'feature_cols.pkl',\n",
    "                        'visualizations/',\n",
    "                        'reports/'\n",
    "                    ]\n",
    "                )\n",
    "                log_s3_verification_results(verification_results)\n",
    "                \n",
    "                if not verification_results[\"success\"]:\n",
    "                    logger.warning(\"S3 artifact verification failed after sync\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to sync artifacts to S3: {e}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.mlflow_manager.end_run(status=\"FAILED\")\n",
    "            raise e\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def _generate_and_log_visualizations(self, results: Dict[str, Any], \n",
    "                                     test_df: pd.DataFrame, \n",
    "                                     target_col: str = 'sales') -> None:\n",
    "        \"\"\"Generate and log model comparison visualizations to MLflow\"\"\"\n",
    "        try:\n",
    "            from src.visualizations.model_visualizations import ModelVisualizer\n",
    "            import tempfile, os, json, mlflow\n",
    "            \n",
    "            logger.info(\"Starting visualization generation...\")\n",
    "            visualizer = ModelVisualizer()\n",
    "            \n",
    "            # Ensure date column exists\n",
    "            if 'date' not in test_df.columns:\n",
    "                test_df = test_df.reset_index().rename(columns={'index': 'date'})\n",
    "                logger.warning(\"No 'date' column found in test data. Using index as date.\")\n",
    "            \n",
    "            # Extract metrics\n",
    "            metrics_dict = {\n",
    "                model_name: model_results['metrics']\n",
    "                for model_name, model_results in results.items()\n",
    "                if 'metrics' in model_results\n",
    "            }\n",
    "            logger.debug(f\"Collected metrics: {metrics_dict}\")\n",
    "            \n",
    "            # Prepare predictions\n",
    "            predictions_dict = {}\n",
    "            for model_name, model_results in results.items():\n",
    "                if 'predictions' in model_results and model_results['predictions'] is not None:\n",
    "                    pred_df = test_df[['date']].copy()\n",
    "                    pred_df['prediction'] = model_results['predictions']\n",
    "                    predictions_dict[model_name] = pred_df\n",
    "            logger.debug(f\"Prepared predictions for models: {list(predictions_dict.keys())}\")\n",
    "            \n",
    "            # Feature importances\n",
    "            feature_importance_dict = {}\n",
    "            for model_name, model_results in results.items():\n",
    "                model = model_results.get('model')\n",
    "                if model and hasattr(model, 'feature_importances_'):\n",
    "                    importance_df = pd.DataFrame({\n",
    "                        'feature': self.feature_cols,\n",
    "                        'importance': model.feature_importances_\n",
    "                    }).sort_values('importance', ascending=False)\n",
    "                    feature_importance_dict[model_name] = importance_df\n",
    "            logger.debug(\"Extracted feature importances\")\n",
    "            \n",
    "            # Generate and log\n",
    "            with tempfile.TemporaryDirectory() as temp_dir:\n",
    "                logger.info(f\"Creating visualizations in {temp_dir}\")\n",
    "                \n",
    "                saved_files = visualizer.create_comprehensive_report(\n",
    "                    metrics_dict=metrics_dict,\n",
    "                    predictions_dict=predictions_dict,\n",
    "                    actual_data=test_df,\n",
    "                    feature_importance_dict=feature_importance_dict if feature_importance_dict else None,\n",
    "                    save_dir=temp_dir\n",
    "                    )\n",
    "                                    \n",
    "                \n",
    "                # Validate output\n",
    "                if not isinstance(saved_files, dict):\n",
    "                    logger.error(f\"Expected dict from visualizer.create_comprehensive_report, got {type(saved_files)}: {saved_files}\")\n",
    "                    raise TypeError(\"Invalid return type from create_comprehensive_report\")\n",
    "\n",
    "                logger.info(f\"Generated {len(saved_files)} visualization files\")\n",
    "\n",
    "\n",
    "                # Log files\n",
    "                for viz_name, file_path in saved_files.items():\n",
    "                    if os.path.exists(file_path):\n",
    "                        mlflow.log_artifact(file_path, \"visualizations\")\n",
    "                        logger.info(f\"Logged visualization: {viz_name}\")\n",
    "                    else:\n",
    "                        logger.warning(f\"File not found: {file_path} for {viz_name}\")\n",
    "                \n",
    "                # Save metrics JSON\n",
    "                summary_file = os.path.join(temp_dir, \"metrics_summary.json\")\n",
    "                with open(summary_file, \"w\") as f:\n",
    "                    json.dump(metrics_dict, f, indent=4)\n",
    "                mlflow.log_artifact(summary_file, \"reports\")\n",
    "                \n",
    "\n",
    "                # Combined HTML report\n",
    "                self._create_combined_html_report(results, temp_dir)\n",
    "                combined_report = os.path.join(temp_dir, 'model_comparison_report.html')\n",
    "                if os.path.exists(combined_report):\n",
    "                    mlflow.log_artifact(combined_report, \"reports\")\n",
    "                    logger.info(\"Logged combined HTML report\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to generate visualizations: {e}\", exc_info=True)\n",
    "\n",
    "\n",
    "    def _create_combined_html_report(self, saved_files: Dict[str, str], save_dir: str) -> None:\n",
    "        \"\"\"Create a combined HTML report with all visualizations\"\"\"\n",
    "        import os\n",
    "        from datetime import datetime\n",
    "        \n",
    "        html_content = \"\"\"\n",
    "        <!DOCTYPE html>\n",
    "        <html>\n",
    "        <head>\n",
    "            <title>Model Comparison Report</title>\n",
    "            <style>\n",
    "                body {\n",
    "                    font-family: Arial, sans-serif;\n",
    "                    margin: 20px;\n",
    "                    background-color: #f5f5f5;\n",
    "                }\n",
    "                h1, h2 {\n",
    "                    color: #333;\n",
    "                }\n",
    "                .section {\n",
    "                    background-color: white;\n",
    "                    padding: 20px;\n",
    "                    margin-bottom: 20px;\n",
    "                    border-radius: 8px;\n",
    "                    box-shadow: 0 2px 4px rgba(0,0,0,0.1);\n",
    "                }\n",
    "                .timestamp {\n",
    "                    color: #666;\n",
    "                    font-size: 14px;\n",
    "                }\n",
    "                iframe {\n",
    "                    width: 100%;\n",
    "                    height: 800px;\n",
    "                    border: 1px solid #ddd;\n",
    "                    border-radius: 4px;\n",
    "                    margin-top: 10px;\n",
    "                }\n",
    "                img {\n",
    "                    max-width: 100%;\n",
    "                    height: auto;\n",
    "                    border-radius: 4px;\n",
    "                    margin-top: 10px;\n",
    "                }\n",
    "            </style>\n",
    "        </head>\n",
    "        <body>\n",
    "            <h1>Sales Forecast Model Comparison Report</h1>\n",
    "            <p class=\"timestamp\">Generated on: {timestamp}</p>\n",
    "        \"\"\"\n",
    "        \n",
    "        html_content = html_content.format(timestamp=datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "        \n",
    "        # Add each visualization section\n",
    "        sections = [\n",
    "            ('metrics_comparison', 'Model Performance Metrics'),\n",
    "            ('predictions_comparison', 'Predictions Comparison'),\n",
    "            ('residuals_analysis', 'Residuals Analysis'),\n",
    "            ('error_distribution', 'Error Distribution'),\n",
    "            ('feature_importance', 'Feature Importance'),\n",
    "            ('summary', 'Summary Statistics')\n",
    "        ]\n",
    "        \n",
    "        for key, title in sections:\n",
    "            if key in saved_files:\n",
    "                html_content += f'<div class=\"section\"><h2>{title}</h2>'\n",
    "                \n",
    "                # All files are now PNG - base64 encode them\n",
    "                import base64\n",
    "                with open(saved_files[key], 'rb') as f:\n",
    "                    img_data = base64.b64encode(f.read()).decode()\n",
    "                html_content += f'<img src=\"data:image/png;base64,{img_data}\" alt=\"{title}\">'\n",
    "                \n",
    "                html_content += '</div>'\n",
    "        \n",
    "        html_content += \"\"\"\n",
    "        </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Save the combined report\n",
    "        with open(os.path.join(save_dir, 'model_comparison_report.html'), 'w') as f:\n",
    "            f.write(html_content)\n",
    "\n",
    "    def save_artifacts(self, version: str = None):\n",
    "        \"\"\"\n",
    "        Save scalers, encoders, feature columns, and trained models.\n",
    "        Also logs everything to MLflow for version tracking.\n",
    "        \"\"\"\n",
    "        import os\n",
    "        import joblib\n",
    "        from datetime import datetime\n",
    "\n",
    "        # Create a versioned folder\n",
    "        version = version or datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        base_dir = f'/tmp/artifacts/{version}'\n",
    "        try:\n",
    "            os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "            # Save preprocessing objects\n",
    "            joblib.dump(self.scalers, os.path.join(base_dir, 'scalers.pkl'))\n",
    "            joblib.dump(self.encoders, os.path.join(base_dir, 'encoders.pkl'))\n",
    "            joblib.dump(self.feature_cols, os.path.join(base_dir, 'feature_cols.pkl'))\n",
    "\n",
    "            # Save model directories\n",
    "            model_dirs = {\n",
    "                'xgboost': os.path.join(base_dir, 'models/xgboost'),\n",
    "                'lightgbm': os.path.join(base_dir, 'models/lightgbm'),\n",
    "                'ensemble': os.path.join(base_dir, 'models/ensemble')\n",
    "            }\n",
    "\n",
    "            for mname, mdir in model_dirs.items():\n",
    "                os.makedirs(mdir, exist_ok=True)\n",
    "                if mname in self.models:\n",
    "                    model = self.models[mname]\n",
    "                    joblib.dump(model, os.path.join(mdir, f\"{mname}_model.pkl\"))\n",
    "                    logger.info(f\"Saved model: {mname} -> {mdir}\")\n",
    "\n",
    "            # Save metadata for reproducibility\n",
    "            metadata = {\n",
    "                \"version\": version,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"models_saved\": list(self.models.keys()),\n",
    "                \"feature_count\": len(self.feature_cols),\n",
    "            }\n",
    "            joblib.dump(metadata, os.path.join(base_dir, 'metadata.pkl'))\n",
    "            logger.info(\"Saved metadata.\")\n",
    "\n",
    "            # Log artifacts to MLflow\n",
    "            if hasattr(self, \"mlflow_manager\"):\n",
    "                self.mlflow_manager.log_artifacts(base_dir)\n",
    "                logger.info(\"Artifacts logged to MLflow.\")\n",
    "            else:\n",
    "                logger.warning(\"mlflow_manager not found. Skipping MLflow logging.\")\n",
    "\n",
    "            logger.info(f\"✅ Artifacts saved successfully in {base_dir}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Failed to save artifacts: {e}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Config file not found: C:\\Users\\Administrator\\OneDrive\\Desktop\\SalesAI\\backend\\configs\\configs\\ml_configs.yaml",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mModelTrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 13\u001b[0m, in \u001b[0;36mModelTrainer.__init__\u001b[1;34m(self, config_path)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config_path: Optional[Union[\u001b[38;5;28mstr\u001b[39m, Path]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_loader \u001b[38;5;241m=\u001b[39m ConfigLoader()\n\u001b[1;32m---> 13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_yaml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfigs/ml_configs.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m'\u001b[39m, {})\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m'\u001b[39m, {})\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\OneDrive\\Desktop\\SalesAI\\backend\\src\\utils\\config_loader.py:49\u001b[0m, in \u001b[0;36mConfigLoader.load_yaml\u001b[1;34m(self, file_path)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_yaml\u001b[39m(\u001b[38;5;28mself\u001b[39m, file_path: Union[\u001b[38;5;28mstr\u001b[39m, Path]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m---> 49\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_full_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     cache_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(path\u001b[38;5;241m.\u001b[39mresolve())\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cache_key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_cache:\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\OneDrive\\Desktop\\SalesAI\\backend\\src\\utils\\config_loader.py:45\u001b[0m, in \u001b[0;36mConfigLoader._get_full_path\u001b[1;34m(self, file_path)\u001b[0m\n\u001b[0;32m     43\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_dir \u001b[38;5;241m/\u001b[39m p\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m p\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m---> 45\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfig file not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m p\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Config file not found: C:\\Users\\Administrator\\OneDrive\\Desktop\\SalesAI\\backend\\configs\\configs\\ml_configs.yaml"
     ]
    }
   ],
   "source": [
    "trainer = ModelTrainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4508dc9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143aafd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96979e6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003ffa56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "salesenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
